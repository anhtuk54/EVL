diff --git a/Documentation/dovetail.rst b/Documentation/dovetail.rst
new file mode 100644
index 000000000000..fd8af6ce28d3
--- /dev/null
+++ b/Documentation/dovetail.rst
@@ -0,0 +1,342 @@
+========================
+Introduction to Dovetail
+========================
+
+:Copyright: |copy| 2016-2018: Philippe Gerum
+
+Using Linux as a host for lightweight software cores specialized in
+delivering very short and bounded response times has been a popular
+way of supporting real-time applications in the embedded space over
+the years.
+
+This design - known as the *dual kernel* approach - introduces a small
+real-time infrastructure which schedules time-critical activities
+independently from the main kernel. Application threads co-managed by
+this infrastructure still benefit from the ancillary kernel services
+such as virtual memory management, and can also leverage the rich GPOS
+feature set Linux provides such as networking, data storage or GUIs.
+
+There are significant upsides to keeping the real-time core separate
+from the GPOS infrastructure:
+
+- because the two kernels are independent, real-time activities are
+  not serialized with GPOS operations internally, removing potential
+  delays which might be induced by the non time-critical
+  work. Likewise, there is no requirement for keeping the GPOS
+  operations fine-grained and highly preemptible at any time, which
+  would otherwise induce noticeable overhead on low-end hardware, due
+  to the need for pervasive task priority inheritance and IRQ
+  threading.
+
+- when debugging a real-time issue, the functional isolation of the
+  real-time infrastructure from the rest of the kernel code restricts
+  bug hunting to the scope of the small co-kernel, excluding most
+  interactions with the very large GPOS kernel base.
+
+- with a dedicated infrastructure providing a specific, well-defined
+  set of real-time services, applications can unambiguously figure out
+  which API calls are available for supporting time-critical work,
+  excluding all the rest as being potentially non-deterministic with
+  respect to response time.
+
+To support such a *dual kernel system*, we need the kernel to exhibit
+a high-priority execution context, for running out-of-band real-time
+duties concurrently to the regular operations.
+
+.. NOTE:: Dovetail only introduces the basic mechanisms for hosting
+such a real-time core, enabling the common programming model for its
+applications in user-space. It does *not* implement the real-time core
+per se, which should be provided by a separate kernel component.
+
+Interrupt pipelining
+====================
+
+.. _pipeline
+The real-time core has to act upon device interrupts with no delay,
+regardless of the regular kernel operations which may be ongoing when
+the interrupt is received by the CPU. Therefore, there is a basic
+requirement for prioritizing interrupt masking and delivery between
+the real-time core and GPOS operations, while maintaining consistent
+internal serialization for the kernel.
+
+To this end, Dovetail leverages a mechanism called *interrupt
+pipelining*, which is described in the
+:ref:`Documentation/irq_pipeline.rst <Interrupt Pipeline>`)
+document. Understanding the concepts and mechanisms described in the
+later document is required for a full comprehension of the Dovetail
+basics.
+
+Alternate scheduling
+====================
+
+Dovetail promotes the idea that a *dual kernel* system should keep the
+functional overlap between the kernel and the real-time core
+minimal. To this end, a real-time thread should be merely seen as a
+regular task with additional scheduling capabilities guaranteeing very
+low response times.
+
+To support such idea, Dovetail enables kthreads and regular user tasks
+to run alternatively in the out-of-band execution context introduced
+by the interrupt pipeline_ (aka *oob* stage), or the common in-band
+kernel context for GPOS operations (aka *in-band* stage).
+
+As a result, real-time core applications in user-space benefit from
+the common Linux programming model - including virtual memory
+protection -, and still have access to the regular Linux services when
+carrying out non time-critical work.
+
+Task migration to the oob stage
+-------------------------------
+
+Low latency response time to events can be achieved when Linux tasks
+wait for them from the out-of-band execution context. The real-time
+core is responsible for switching a task to such a context as part of
+its task management rules; Dovetail facilitates this migration with
+dedicated services.
+
+The migration process of a task from the GPOS/in-band context to the
+high-priority, out-of-band context is as follows:
+
+1. :c:func:`dovetail_leave_inband` is invoked from the migrating task
+   context, with the same prerequisites than for calling
+   :c:func:`schedule` (preemption enabled, interrupts on).
+
+.. _`in-band sleep operation`:
+2. the caller is put to interruptible sleep state (S).
+
+3. the real-time core's implementation of :c:func:`resume_oob_task` is
+   passed a pointer to the task_struct descriptor of the migrating
+   task. This routine is expected to perform the necessary steps for
+   taking control over the task on behalf of the real-time core,
+   re-scheduling its code appropriately over the oob stage. This
+   typically involves resuming it from the `out-of-band suspended
+   state`_ applied during the converse migration path.
+
+4. at some point later, when the migrated task is picked by the
+   real-time scheduler, it resumes execution on the oob stage with the
+   register file previously saved by the kernel scheduler in
+   :c:func:`switch_to` at step 1.
+
+Task migration to the in-band stage
+-----------------------------------
+
+Sometimes, a real-time thread may want to leave the out-of-band
+context, continuing execution from the in-band context instead, so as
+to:
+
+- run non time-critical (in-band) work involving regular system calls
+  handled by the kernel,
+
+- recover from CPU exceptions, such as handling major memory access
+  faults, for which there is no point in caring for response time, and
+  therefore makes no sense to duplicate in the real-time core anyway.
+
+.. NOTE: The discussion about exception_ handling covers the last
+   point in details.
+
+The migration process of a task from the high-priority, out-of-band
+context to the GPOS/in-band context is as follows::
+
+1. the real-time core schedules an in-band handler for execution which
+   should call :c:func:`wake_up_process` to unblock the migrating task
+   from the standpoint of the kernel scheduler. This is the
+   counterpart of the :ref:`in-band sleep operation <in-band sleep
+   operation>` from the converse migration path. The
+   :ref:`Documentation/irq_pipeline.rst` <irq_work> mechanism can be
+   used for scheduling such event from the out-of-band context.
+
+.. _`out-of-band suspended state`:
+2. the real-time core suspends execution of the current task from its
+   own standpoint, calling :c:func:`dovetail_leave_oob` right before
+   scheduling out the task. The real-time scheduler is assumed to be
+   using the common :c:func:`switch_to` routine for switching task
+   contexts.
+
+3. at some point later, the out-of-band context is exited by the
+   current CPU when no more high-priority work is left, causing the
+   preempted in-band kernel code to resume execution on the in-band
+   stage. The handler scheduled at step 1 eventually runs, waking up
+   the migrating task from the standpoint of the kernel.
+
+4. the migrating task resumes from the tail scheduling code of the
+   real-time scheduler, where it suspended in step 2. Noticing the
+   migration, the real-time core eventually calls
+   :c:func:`dovetail_resume_inband` for finalizing the transition of
+   the incoming task to the in-band stage.
+
+Binding to the real-time core
+-----------------------------
+
+.. _binding:
+Dovetail facilitates fine-grained per-thread management from the
+real-time core, as opposed to per-process. For this reason, the
+real-time core should at least implement a mechanism for turning a
+regular task into a real-time thread with extended capabilities,
+binding it to the core.
+
+The real-time core should inform the kernel about its intent to share
+control over a task, by calling :c:func::`dovetail_start_altsched` on
+behalf of that task, i.e. when such task is current.
+
+For this reason, the binding operation is usually carried out by a
+dedicated system call exposed by the real-time core, which a regular
+task would invoke.
+
+Once :c:func::`dovetail_start_altsched` has returned, Dovetail
+notifications are enabled for the current task (see below).
+
+.. NOTE:: Whether there should be distinct procedures for binding
+	  processes *and* threads to the real-time core, or only a
+	  thread binding procedure is up to the real-time core
+	  implementation.
+
+Notifications
+-------------
+
+Exception handling
+~~~~~~~~~~~~~~~~~~
+
+.. _exception
+If a processor exception is raised while the CPU is busy running a
+real-time thread in the out-of-band context (e.g. due to some invalid
+memory access, bad instruction, FPU or alignment error etc), the task
+may have to leave such context immediately if the fault handler is not
+protected against out-of-band interrupts, and therefore cannot be
+properly serialized with out-of-band code.
+
+Dovetail notifies the real-time core about incoming exceptions early
+from the low-level fault handlers, but only when some out-of-band code
+was running when the exception was taken. The real-time core may then
+take action, such as reconciling the current task's execution context
+with the kernel's expectations before the task may traverse the
+regular fault handling code.
+
+.. HINT:: Enabling debuggers to trace real-time thread involves
+          dealing with debug traps the former may poke into the
+          debuggee's code for breakpointing duties.
+
+The notification is issued by a call to :c:func:`oob_trap_notify`
+which in turn invokes the :c:func:`handle_oob_trap` routine the
+real-time core should override for receiving those events (*__weak*
+binding). Interrupts are **disabled** in the CPU when
+:c:func:`handle_oob_trap` is called.::
+
+     /* out-of-band code running */
+     *bad_pointer = 42;
+        [ACCESS EXCEPTION]
+	   /* low-level fault handler in arch/<arch>/mm */
+           -> do_page_fault()
+	      -> oob_trap_notify(...)
+	         /* real-time core */
+	         -> handle_oob_trap(...)
+		    -> forced task migration to in-band stage
+	   ...
+           -> handle_mm_fault()
+
+.. NOTE:: handling minor memory access faults only requiring quick PTE
+          fixups should not involve switching the current task to the
+          in-band context though. Instead, the fixup code should be
+          made :ref:`Documentation/irq_pipeline.rst` strictly <atomic>
+          for serializing accesses from any context.
+
+System calls
+~~~~~~~~~~~~
+
+A real-time core interfaced with the kernel via Dovetail may introduce
+its own set of system calls. From the standpoint of the kernel, this
+is a foreign set of calls, which can be distinguished unambiguously
+from regular ones based on an arch-specific marker.
+
+.. HINT:: Syscall numbers from this set might have a different base,
+	  and/or some high-order bit set which regular syscall numbers
+	  would not have.
+
+If a task bound to the real-time core issues any system call,
+regardless of which of the kernel or real-time core should handle it,
+the latter must be given the opportunity to:
+
+- perform the service directly, possibly switching the caller to
+  out-of-band context first would the request require it.
+
+- pass the request downward to the normal system call path on the
+  in-band stage, possibly switching the caller to in-band context if
+  needed.
+
+If a regular task (i.e. *not* known from the real-time core [yet])
+issues any foreign system call, the real-time core is given a chance
+to handle it. This way, a foreign system call which would initially
+bind a regular task to the real-time core would be delivered to the
+real-time core as expected (see binding_).
+
+Dovetail intercepts system calls early in the kernel entry code,
+delivering them to the proper handler according to the following
+logic::
+
+     is_foreign(syscall_nr)?
+	    Y: is_bound(task)
+	           Y: -> handle_oob_syscall()
+		   N: -> handle_pipelined_syscall()
+            N: is_bound(task)
+	           Y: -> handle_pipelined_syscall()
+		   N: -> normal syscall handling
+
+:c:func:`handle_oob_syscall` is the fast path for handling foreign
+system calls from tasks already running in out-of-band context.
+
+:c:func:`handle_pipelined_syscall` is a slower path for handling requests
+which might require the caller to switch to the out-of-band context
+first before proceeding.
+
+In-band kernel events
+~~~~~~~~~~~~~~~~~~~~~
+
+The last set of notifications involves pure in-band events which the
+real-time core may need to know about, as they may affect its own task
+management. Except for INBAND_PROCESS_CLEANUP which is called for
+*any* exiting user-space task, all other notifications are only issued
+for tasks bound to the real-time core (which may involve kthreads).
+
+The notification is issued by a call to :c:func:`inband_event_notify`
+which in turn invokes the :c:func:`handle_inband_event` routine the
+real-time core should override for receiving those events (*__weak*
+binding). Interrupts are **enabled** in the CPU when
+:c:func:`handle_inband_event` is called.
+
+The notification hook is given the event type code, and a single
+pointer argument which relates to the event type.
+
+The following events are defined (include/linux/dovetail.h):
+
+- INBAND_TASK_SCHEDULE(struct task_struct *next)
+
+  sent in preparation of a context switch, right before the memory
+  context is switched to *next*.
+
+- INBAND_TASK_SIGNAL(struct task_struct *target)
+
+  sent when *target* is about to receive a signal. The real-time core
+  may decide to schedule a transition of the recipient to the in-band
+  stage in order to have it handle that signal asap, which is required
+  for keeping the kernel sane. This notification is always sent from
+  the context of the issuer.
+
+- INBAND_TASK_MIGRATION(struct dovetail_migration_data *p)
+
+  sent when p->task is about to move to CPU p->dest_cpu.
+
+- INBAND_TASK_EXIT(struct task_struct *current)
+
+  sent from :c:func:`do_exit` before the current task has dropped the
+  files and mappings it owns.
+
+- INBAND_PROCESS_CLEANUP(struct mm_struct *mm)
+
+  sent before *mm* is entirely dropped, before the mappings are
+  exited. Per-process resources which might be maintained by the
+  real-time core could be released there, as all threads have exited.
+
+Terminology
+===========
+
+See the :ref:`Documentation/irq_pipeline.rst` <Interrupt Pipeline
+terminology>.
diff --git a/Documentation/irq_pipeline.rst b/Documentation/irq_pipeline.rst
new file mode 100644
index 000000000000..4f84c28e31f7
--- /dev/null
+++ b/Documentation/irq_pipeline.rst
@@ -0,0 +1,693 @@
+.. include:: <isonum.txt>
+
+====================
+Interrupt pipelining
+====================
+
+:Copyright: |copy| 2016-2018: Philippe Gerum
+
+Purpose
+=======
+
+To protect from deadlocks and maintain data integrity, Linux hard
+disables interrupts around any critical section of code which must not
+be preempted by interrupt handlers on the same CPU, enforcing a
+strictly serialized execution among those contexts.
+
+The unpredictable delay this may cause before external events can be
+handled is a major roadblock for kernel components requiring
+predictable and very short response times to external events, in the
+range of a few microseconds.
+
+To address this issue, a mechanism called *interrupt pipelining* turns
+all device IRQs into pseudo-NMIs, only to run NMI-safe interrupt
+handlers from the perspective of the regular kernel activities.
+
+Two-stage IRQ pipeline
+======================
+
+Interrupt pipelining is a lightweight approach based on the
+introduction of a separate, high-priority execution stage for running
+out-of-band interrupt handlers immediately upon IRQ receipt, which
+cannot be delayed by the in-band, regular kernel work even if the
+latter serializes the execution by - seemingly - disabling interrupts.
+
+IRQs which have no handlers in the high priority stage may be deferred
+on the receiving CPU until the out-of-band activity has quiesced on
+that CPU. Eventually, the preempted in-band code can resume normally,
+which may involve handling the deferred interrupts.
+
+In other words, interrupts are flowing down from the out-of-band to
+the in-band interrupt stages, which form a two-stage pipeline for
+prioritizing interrupt delivery.
+
+The runtime context of the out-of-band interrupt handlers is known as
+the *oob stage* of the pipeline, as opposed to the in-band kernel
+activities sitting on the *inband stage*::
+
+                    Out-of-band                 In-band
+                    IRQ handlers()            IRQ handlers()
+               __________   _______________________   ______
+                  .     /  /  .             .     /  /  .
+                  .    /  /   .             .    /  /   .
+                  .   /  /    .             .   /  /    .
+                  ___/  /______________________/  /     .
+     [IRQ] -----> _______________________________/      .
+                  .           .             .           .
+                  .   OOB     .             .   In-band .
+                  .   Stage   .             .   Stage   .
+               _____________________________________________
+
+
+A software core can base its own activities on the oob stage,
+interposing on specific IRQ events, for delivering real-time
+capabilities to a particular set of applications. Meanwhile, the
+regular kernel operations keep going over the in-band stage
+unaffected, only delayed by short preemption times for running the
+out-of-band work.  A generic interface for coupling such a real-time
+core to the kernel is described in the
+:ref:`Documentation/dovetail.rst <Dovetail>`) document.
+
+.. NOTE:: Interrupt pipelining is a partial implementation of [#f2]_,
+          in which an interrupt *stage* is a limited form of an
+          operating system *domain*.
+
+Virtual interrupt flag
+----------------------
+
+.. _flag:
+As hinted earlier, predictable response time of out-of-band handlers
+to IRQ receipts requires the in-band kernel work not to be allowed to
+delay them by masking interrupts in the CPU.
+
+However, critical sections delimited this way by the in-band code must
+still be enforced for the *in-band stage*, so that system integrity is
+not at risk. This means that although out-of-band IRQ handlers may run
+at any time while the *oob stage* is accepting interrupts, in-band IRQ
+handlers should be allowed to run only when the in-band stage is
+accepting interrupts too.
+
+So we need to decouple the interrupt masking and delivery logic which
+applies to the oob stage from the one in effect on the in-band stage,
+by implementing a dual interrupt control.
+
+To this end, a software logic managing a virtual interrupt disable
+flag is introduced by the interrupt pipeline between the hardware and
+the generic IRQ management layer. This logic can mask IRQs from the
+perspective of the regular kernel work when :c:func:`local_irq_save`,
+:c:func:`local_irq_disable` or any lock-controlled masking operations
+like :c:func:`spin_lock_irqsave` is called, while still accepting IRQs
+from the CPU for immediate delivery to out-of-band handlers.
+
+The oob stage protects from interrupts by disabling them in the CPU's
+status register, while the in-band stage disables interrupts only
+virtually. A stage for which interrupts are disabled is said to be
+*stalled*. Conversely, *unstalling* a stage means re-enabling
+interrupts for it.
+
+Obviously, stalling the oob stage implicitly means disabling
+further IRQ receipts for the in-band stage too.
+
+Interrupt deferral for the *in-band stage*
+---------------------------------------
+
+.. _deferral:
+.. _deferred:
+When the in-band stage is stalled because the virtual interrupt disable
+flag is set, any IRQ event which was not immediately delivered to the
+*oob stage* is recorded into a per-CPU log, postponing delivery to
+the regular kernel handler.
+
+Such delivery is deferred until the in-band kernel code clears the
+virtual interrupt disable flag by calling :c:func:`local_irq_enable`
+or any of its variants, which unstalls the in-band stage. When this
+happens, the interrupt state is resynchronized by playing the log,
+firing the in-band handlers for which an IRQ event is pending.
+
+::
+   /* Both stages unstalled on entry */
+   local_irq_save(flags);
+   <IRQx received: no out-of-band handler>
+       (pipeline logs IRQx event)
+   ...
+   local_irq_restore(flags);
+       (pipeline plays IRQx event)
+            handle_IRQx_interrupt();
+
+If the in-band stage is unstalled at the time of the IRQ receipt, the
+in-band handler is immediately invoked, just like with the
+non-pipelined IRQ model.
+
+.. NOTE:: The principle of deferring interrupt delivery based on a
+          software flag coupled to an event log has been originally
+          described as "Optimistic interrupt protection" in [#f1]_.
+
+Device interrupts virtually turned into NMIs
+--------------------------------------------
+
+From the standpoint of the in-band kernel code (i.e. the one running
+over the *in-band* interrupt stage) , the interrupt pipelining logic
+virtually turns all device IRQs into NMIs, for running out-of-band
+handlers.
+
+.. _re-entry:
+For this reason, out-of-band code may generally **NOT** re-enter
+in-band code, for preventing creepy situations like this one::
+
+   /* in-band context */
+   spin_lock_irqsave(&lock, flags);
+      <IRQx received: out-of-band handler installed>
+         handle_oob_event();
+            /* attempted re-entry to in-band from out-of-band. */
+            in_band_routine();
+               spin_lock_irqsave(&lock, flags);
+               <DEADLOCK>
+               ...
+            ...
+         ...
+   ...
+   spin_unlock irqrestore(&lock, flags);
+
+Even in absence of an attempt to get a spinlock recursively, the outer
+in-band code in the example above is entitled to assume that no access
+race can occur on the current CPU while interrupts are
+masked. Re-entering in-band code from an out-of-band handler would
+invalidate this assumption.
+
+In rare cases, we may need to fix up the in-band kernel routines in
+order to allow out-of-band handlers to call them. Typically, atomic_
+helpers are such routines, which serialize in-band and out-of-band
+callers.
+
+Synthetic interrupt vectors
+---------------------------
+
+.. _synthetic:
+The pipeline introduces an additional type of interrupts, which are
+purely software-originated, with no hardware involvement. These IRQs
+can be triggered by any kernel code. Synthetic IRQs are inherently
+per-CPU events.
+
+Because the common pipeline flow_ applies to synthetic interrupts, it
+is possible to attach them to out-of-band and/or in-band handlers,
+just like device interrupts.
+
+.. NOTE:: synthetic interrupts and regular softirqs differ in essence:
+          the latter only exist in the in-band context, and therefore
+          cannot trigger out-of-band activities.
+
+Synthetic interrupt vectors are allocated from the
+*synthetic_irq_domain*, using the :c:func:`irq_create_direct_mapping`
+routine.
+
+For instance, a synthetic interrupt can be used for triggering an
+in-band activity on the in-band stage from the oob stage as follows::
+
+  #include <linux/irq_pipeline.h>
+
+  static irqreturn_t sirq_handler(int sirq, void *dev_id)
+  {
+        do_in_band_work();
+
+        return IRQ_HANDLED;
+  }
+
+  static struct irqaction sirq_action = {
+        .handler = sirq_handler,
+        .name = "In-band synthetic interrupt",
+        .flags = IRQF_NO_THREAD,
+  };
+
+  unsigned int alloc_sirq(void)
+  {
+	unsigned int sirq;
+
+	sirq = irq_create_direct_mapping(synthetic_irq_domain);
+	if (!sirq)
+		return 0;
+
+	setup_percpu_irq(sirq, &sirq_action);
+
+	return sirq;
+  }
+
+Code can schedule the execution of :c:func:`sirq_handler` like this::
+
+  irq_inject_pipeline(sirq);
+
+or, via a lightweight injection method requiring hard IRQs to be
+disabled::
+
+  unsigned long flags = hard_local_irqsave();
+  irq_post_inband(sirq);
+  hard_local_irqrestore(flags);
+
+Conversely, a synthetic interrupt can be handled from the out-of-band
+context::
+
+  static irqreturn_t sirq_oob_handler(int sirq, void *dev_id)
+  {
+        do_out_of_band_work();
+
+        return IRQ_HANDLED;
+  }
+
+  unsigned int alloc_sirq(void)
+  {
+	unsigned int sirq;
+
+	sirq  = irq_create_direct_mapping(synthetic_irq_domain);
+	if (!sirq)
+		return 0;
+
+	ret = __request_percpu_irq(sirq, sirq_oob_handler,
+                                   IRQF_OOB,
+                                   "Out-of-band synthetic interrupt",
+                                   dev_id);
+	if (ret) {
+		irq_dispose_mapping(sirq);
+		return 0;
+	}
+
+	return sirq;
+  }
+
+Code can trigger the immediate execution of :c:func:`sirq_oob_handler`
+on the oob stage as follows::
+
+  irq_inject_pipeline(sirq);
+
+Pipelined interrupt flow
+------------------------
+
+.. _flow:
+When interrupt pipelining is enabled, IRQs are first delivered to
+the pipeline entry point via a call to
+:c:func:`generic_pipeline_irq`::
+
+    asm_irq_entry
+       -> irqchip_handle_irq()
+          -> handle_domain_irq()
+             -> generic_pipeline_irq()
+                -> irq_flow_handler()
+                <IRQ delivery logic>
+
+Contrary to the non-pipelined model, the generic IRQ flow handler does
+*not* call the in-band interrupt handler immediately, but only runs
+the irqchip-specific handler for acknowledging the incoming IRQ event
+in the interrupt controller, before running out-of-band handlers for
+that event if any.
+
+In absence of out-of-band handler for the event, the device may keep
+asserting the interrupt signal until the cause has been lifted in its
+own registers. For this reason, the flow handlers as modified by the
+pipeline code may have to to mask the interrupt line until the in-band
+handler has run from the in-band stage, lifting the interrupt cause. This
+typically happens with level-triggered interrupts. This addresses the
+following scenario, which happens for a similar reason while an IRQ
+thread waits for being scheduled in, requiring the same kind of
+provision::
+
+    /* in-band stage stalled on entry */
+    asm_irq_entry
+       ...
+          -> generic_pipeline_irq()
+             ...
+                <IRQ logged, delivery deferred>
+    asm_irq_exit
+    /*
+     * CPU allowed to accept interrupts again with IRQ cause not
+     * acknowledged in device yet => **IRQ storm**.
+     */
+    asm_irq_entry
+       ...
+    asm_irq_exit
+    asm_irq_entry
+       ...
+    asm_irq_exit
+
+Since all of the IRQ handlers sharing an interrupt line are either
+in-band or out-of-band in a mutually exclusive way, such masking
+cannot delay out-of-band events.
+
+Prerequisites
+=============
+
+The interrupt pipeline requires the following features to be available
+from the target kernel:
+
+- Generic IRQ handling
+- IRQ domains
+- Clock event abstraction
+
+Implementation
+==============
+
+The following kernel areas are involved in interrupt pipelining:
+
+- Generic IRQ core
+
+  * IRQ descriptor management.
+
+    The driver API to the IRQ subsystem exposes the new interrupt type
+    flag `IRQF_OOB`, denoting an out-of-band handler with the
+    :c:func:`setup_irq`, :c:func:`request_irq`, and
+    :c:func:`__request_percpu_irq` routines.
+
+    Support for IRQ domains is a prerequisite for interrupt
+    pipelining. :c:func:`handle_domain_irq` from the IRQ domain
+    interface redirects the interrupt flow to the pipeline entry,
+    represented by the :c:func:`generic_pipeline_irq`
+    routine.
+
+  * IRQ flow handlers
+
+    Generic flow handlers acknowledge the incoming IRQ event in the
+    hardware as usual, by calling the appropriate irqchip routine
+    (e.g. :c:func:`irq_ack`, :c:func:`irq_eoi`). However, the flow_
+    handlers do not immediately invoke the in-band interrupt
+    handlers. Instead, they hand the event over to the pipeline core
+    by calling :c:func:`handle_oob_irq`.
+
+    If an out-of-band handler exists for the interrupt received,
+    :c:func:`handle_oob_irq` invokes it immediately, after switching
+    the execution context to the oob stage if not current yet.
+
+    Otherwise, if the execution context is currently over the in-band
+    stage and unstalled, the pipeline core delivers it immediately to
+    the in-band handler. In all other cases, the interrupt is
+    deferred, marked as pending into the current CPU's event log, then
+    the IRQ frame is left.
+
+  * IRQ work
+
+    .. _irq_work:
+    With interrupt pipelining, a code running over the oob stage
+    could have preempted the in-band stage in the middle of a critical
+    section. For this reason, it would be unsafe to call any
+    in-band routine from an out-of-band context.
+
+    Triggering in-band work handlers from out-of-band code can be done
+    by using :c:func:`irq_work_queue`. The work request issued from
+    the oob stage will be scheduled for running over the in-band
+    stage.
+
+    .. NOTE:: the interrupt pipeline forces the use of a synthetic_
+              IRQ as a notification signal for the IRQ work machinery,
+              instead of a hardware-specific interrupt vector.
+
+  * IRQ pipeline core
+
+- Arch-specific bits
+
+  * CPU interrupt mask handling
+
+    The architecture-specific code which manipulates the interrupt
+    flag in the CPU's state register
+    (i.e. arch/<arch>/include/asm/irqflags.h) is split between real
+    and virtual interrupt control:
+
+    + the *native_* level helpers affect the hardware state in the CPU.
+
+    + the *arch_* level helpers affect the virtual interrupt disable
+      flag_ implemented by the pipeline core for controlling the in-band
+      stage protection against interrupts.
+
+    This means that generic helpers from <linux/irqflags.h> such as
+    :c:func:`local_irq_disable` and :c:func:`local_irq_enable`
+    actually refer to the virtual protection scheme when interrupts
+    are pipelined, implementing interrupt deferral_ for the protected
+    in-band code running over the in-band stage.
+
+  * Assembly-level IRQ, exception paths
+
+    Since interrupts are only virtually masked for the in-band code,
+    IRQs can still be taken by the CPU although they should not be
+    visible from the in-band stage when they happen in the following
+    situations:
+
+    + when the virtual protection flag_ is raised, meaning the in-band
+      stage does not accept IRQs, in which case interrupt _deferral
+      happens.
+
+    + when the CPU runs out-of-band code, regardless of the state of
+      the virtual protection flag.
+
+    In both cases, the low-level assembly code handling incoming IRQs
+    takes a fast exit path unwinding the interrupt frame early,
+    instead of running the common in-band epilogue which checks for
+    task rescheduling opportunities and pending signals.
+
+    Likewise, the low-level fault/exception handling code also takes a
+    fast exit path under the same circumstances. Typically, an
+    out-of-band handler causing a minor page fault should benefit from
+    a lightweight PTE fixup performed by the high-level fault handler,
+    but is not allowed to traverse the rescheduling logic upon return
+    from exception.
+
+- Scheduler core
+
+  * CPUIDLE support
+
+    The logic of the CPUIDLE framework has to account for those
+    specific issues the interrupt pipelining introduces:
+
+    - the kernel might be idle in the sense that no in-band activity
+    is scheduled yet, and planning to shut down the timer device
+    suffering the C3STOP (mis)feature.  However, at the same time,
+    some out-of-band code might wait for a tick event already
+    programmed in the timer hardware they both share via the proxy_
+    clock event device.
+
+    - switching the CPU to a power saving state may incur a
+    significant latency, particularly for waking it up before it can
+    handle an incoming IRQ, which is at odds with the purpose of
+    interrupt pipelining.
+
+    Obviously, we don't want the CPUIDLE logic to turn off the
+    hardware timer when C3STOP is in effect for the timer device,
+    which would cause the pending out-of-band event to be
+    lost.
+
+    Likewise, the wake up latency induced by entering a sleep state on
+    a particular hardware may not always be acceptable.
+
+    Since the in-band kernel code does not know about the out-of-band
+    code plans by design, CPUIDLE calls :c:func:`irq_cpuidle_control`
+    to figure out whether the out-of-band system is fine with entering
+    the idle state as well.  This routine should be overriden by the
+    out-of-band code for receiving such notification (*__weak*
+    binding).
+
+    If this hook returns a boolean *true* value, CPUIDLE proceeds as
+    normally. Otherwise, the CPU is simply denied from entering the
+    idle state, leaving the timer hardware enabled.
+
+    ..CAUTION:: If some out-of-band code waiting for an external event
+    cannot bear with the latency that might be induced by the default
+    architecture-specific CPU idling code, then CPUIDLE is not usable
+    and should be disabled at build time.
+
+  * Kernel preemption control (PREEMPT)
+
+    :c:func:`preempt_schedule_irq` reconciles the virtual interrupt
+    state - which has not been touched by the assembly level code upon
+    kernel entry - with basic assumptions made by the scheduler core,
+    such as entering with interrupts disabled.
+
+- Timer management
+
+  * Proxy tick device
+
+.. _proxy:
+    The proxy tick device is a synthetic clock event device for
+    handing over control of the hardware tick device in use by the
+    kernel to an out-of-band timing logic.
+
+    With this proxy in place, the out-of-band code must carry out the
+    timing requests from the in-band timer core (i.e. hrtimers) in
+    addition to its own timing duties.
+
+    In other words, the proxy tick device shares the functionality of
+    the actual device between the in-band and out-of-band contexts,
+    with only the latter actually programming the hardware.
+
+- Generic locking & atomic
+
+  * Generic atomic ops
+
+.. _atomic:
+    The effect of virtualizing interrupt protection must be reversed
+    for atomic helpers in <asm-generic/{atomic|bitops/atomic}.h> and
+    <asm-generic/cmpxchg-local.h>, so that no interrupt can preempt
+    their execution, regardless of the stage their caller live
+    on.
+
+    This is required to keep those helpers usable on data which
+    might be accessed concurrently from both stages.
+
+    The usual way to revert such virtualization consists of delimiting
+    the protected section with :c:func:`hard_local_irq_save`,
+    :c:func:`hard_local_irq_restore` calls, in replacement for
+    :c:func:`local_irq_save`, :c:func:`local_irq_restore`
+    respectively.
+
+  * Mutable and hard spinlocks
+
+    .. _spinlocks:
+    The pipeline core introduces two spinlock types:
+
+    + *hard* spinlocks manipulate the CPU interrupt mask, and don't
+      affect the kernel preemption state in locking/unlocking
+      operations.
+
+      This type of spinlock is useful for implementing a critical
+      section to serialize concurrent accesses from both in-band and
+      out-of-band contexts, i.e. from in-band and oob stages. Obviously,
+      sleeping into a critical section protected by a hard spinlock
+      would be a very bad idea.
+
+      In other words, hard spinlocks are not subject to virtual
+      interrupt disabling, therefore can be used to serialize with
+      out-of-band activities, including from the in-band kernel
+      code. At any rate, those sections ought to be quite short, for
+      keeping latency low.
+
+   + Mutable spinlocks are used internally by the pipeline core to
+     protect access to IRQ descriptors (`struct irq_desc::lock`), so
+     that we can keep the original locking scheme of the generic IRQ
+     core unmodified for handling out-of-band interrupts.
+
+     Mutable spinlocks behave like *hard* spinlocks when traversed by
+     the low-level IRQ handling code on entry to the pipeline, or
+     common *raw* spinlocks otherwise, preserving the kernel
+     (virtualized) interrupt and preemption states as perceived by the
+     in-band context. This type of lock is not meant to be used in any
+     other situation.
+
+  * Lockdep
+
+    The lock validator automatically reconciles the real and virtual
+    interrupt states, so it can deliver proper diagnosis for locking
+    constructs defined in both in-band and out-of-band contexts.
+
+    This means that *hard* and *mutable* spinlocks_ are included in
+    the validation set when LOCKDEP is enabled.
+
+  .. CAUTION:: These two additional types are subject to LOCKDEP
+                analysis. However, be aware that latency figures are
+                likely to be really **bad** when LOCKDEP is enabled,
+                due to the large amount of work the lock validator may
+                have to do while critical sections are being enforced
+                by disabling interrupts in the CPU.
+
+- Drivers
+
+  * IRQ chip drivers
+
+    .. _irqchip:
+    `irqchip` drivers need to be specifically adapted for supporting the
+    pipelined interrupt model. The basic task is to ensure that the
+    following `struct irq_chip` handlers can be called from an
+    out-of-band context safely when defined for the interrupt
+    controller: :c:func:`irq_mask`, :c:func:`irq_ack`,
+    :c:func:`irq_mask_ack`, :c:func:`irq_eoi`, :c:func:`irq_unmask`.
+
+    Such handler is deemed safe to be called from out-of-band context
+    when it does not invoke **any** regular kernel service, which
+    might cause an invalid in-band context re-entry_.
+
+    The generic IRQ management core serializes calls to `irqchip`
+    handlers for a given IRQ by serializing access to its interrupt
+    descriptor, acquiring the per-descriptor `irq_desc::lock`
+    spinlock.  Holding `irq_desc::lock` when running a handler for any
+    IRQ shared between all CPUs ensures that a single CPU handles the
+    event.
+
+    In addition, there might be inner spinlocks defined by some
+    `irqchip` drivers for serializing handlers accessing a common
+    interrupt controller hardware for _distinct_ IRQs from multiple
+    CPUs concurrently.  Adapting the `irqchip` driver to support
+    interrupt pipelining may involve converting those spinlocks hard
+    spinlocks_.
+
+    .. CAUTION:: switching to hard spinlocks_ should involve a careful
+                 review of any section in the `irqchip` driver
+                 serializing execution with such spinlock. Any such
+                 section would then have the same requirement about
+                 not calling any regular kernel service, and be short
+                 enough to keep interrupt latency figures low.
+
+    Other section of code which were originally serialized by common
+    interrupt disabling may need to be made fully atomic_ for running
+    consistenly in pipelined interrupt mode. This can be done by
+    introducing hard masking with :c:func:`hard_local_irq_save()`,
+    :c:func:`hard_local_irq_restore()`.
+
+    Finally, `IRQCHIP_PIPELINE_SAFE` must be added to `struct
+    irqchip::flags` member of a pipeline-aware `irqchip` driver, in
+    order to notify the kernel that such controller can operate in
+    pipelined interrupt mode.
+
+    .. NOTE:: :c:func:`irq_set_chip` will complain loudly with a
+              kernel warning whenever the `irqchip` descriptor passed
+              does not bear the `IRQCHIP_PIPELINE_SAFE` flag and
+              CONFIG_IRQ_PIPELINE is enabled.
+
+  * Clock event devices
+
+    Clock chip devices which may be controlled by the proxy tick
+    device need their drivers to be specifically adapted for such use:
+
+    + :c:func:`clockevents_handle_event` must be used to invoke the
+      event handler from the interrupt handler, instead of
+      dereferencing `struct clock_event_device::event_handler`
+      directly.
+
+    + `struct clock_event_device::irq` must be properly set to the
+      actual IRQ number signaling an event from this device.
+
+    + `struct clock_event_device::features` must include
+      `CLOCK_EVT_FEAT_PIPELINE`.
+
+    + `__IRQF_TIMER` must be set for the action handler of the timer
+       device interrupt.
+
+    .. CAUTION:: only oneshot-capable clock event devices can be
+                 shared via the proxy tick device.
+
+- Misc
+
+  * :c:func:`printk`
+
+    :c:func:`printk` may be called by out-of-band code safely, without
+    encurring extra latency. The output is conveyed like
+    NMI-originated output, which involves some delay until the in-band
+    code resumes, and the console driver(s) can handle it.
+
+  * Tracing core
+
+    Tracepoints can be traversed by out-of-band code safely. Dynamic
+    tracing is available to a kernel running the pipelined interrupt
+    model too.
+
+Terminology
+===========
+
+.. _terminology:
+======================   =======================================================
+    Term                                       Definition
+======================   =======================================================
+OOB stage                high-priority execution context trigged by out-of-band IRQs
+In-band stage            regular execution context performing GPOS work
+Out-of-band code         code running over the oob stage
+In-band code             code running over the in-band stage
+
+
+Resources
+=========
+
+.. [#f1] Stodolsky, Chen & Bershad; "Fast Interrupt Priority Management in Operating System Kernels"
+    https://www.usenix.org/legacy/publications/library/proceedings/micro93/full_papers/stodolsky.txt
+.. [#f2] Yaghmour, Karim; "ADEOS - Adaptive Domain Environment for Operating Systems"
+    https://www.opersys.com/ftp/pub/Adeos/adeos.pdf
diff --git a/arch/Kconfig b/arch/Kconfig
index 5f8a5d84dbbe..1a4733f30cd4 100644
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -201,6 +201,9 @@ config HAVE_FUNCTION_ERROR_INJECTION
 config HAVE_NMI
 	bool
 
+config HAVE_PERCPU_PREEMPT_COUNT
+	bool
+
 #
 # An arch should select this if it provides all these things:
 #
diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 8a50efb559f3..e85af7e2fd91 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -580,6 +580,8 @@ config ARCH_MULTI_V7
 config ARCH_MULTI_V6_V7
 	bool
 	select MIGHT_HAVE_CACHE_L2X0
+	select HAVE_IRQ_PIPELINE
+	select HAVE_DOVETAIL if CPU_HAS_ASID
 
 config ARCH_MULTI_CPU_AUTO
 	def_bool !(ARCH_MULTI_V4 || ARCH_MULTI_V4T || ARCH_MULTI_V6_V7)
@@ -1238,6 +1240,8 @@ config SCHED_SMT
 	  MultiThreading at a cost of slightly increased overhead in some
 	  places. If unsure say N here.
 
+source "kernel/Kconfig.dovetail"
+
 config HAVE_ARM_SCU
 	bool
 	help
diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 99929122dad7..b6e57ed31382 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -107,6 +107,18 @@
 	.endm
 #endif
 
+	.macro  disable_irq_if_pipelined
+#ifdef CONFIG_IRQ_PIPELINE
+	disable_irq_notrace
+#endif
+	.endm
+
+	.macro  enable_irq_if_pipelined
+#ifdef CONFIG_IRQ_PIPELINE
+	enable_irq_notrace
+#endif
+	.endm
+
 	.macro asm_trace_hardirqs_off, save=1
 #if defined(CONFIG_TRACE_IRQFLAGS)
 	.if \save
diff --git a/arch/arm/include/asm/atomic.h b/arch/arm/include/asm/atomic.h
index 75bb2c543e59..57665cc84942 100644
--- a/arch/arm/include/asm/atomic.h
+++ b/arch/arm/include/asm/atomic.h
@@ -166,9 +166,9 @@ static inline void atomic_##op(int i, atomic_t *v)			\
 {									\
 	unsigned long flags;						\
 									\
-	raw_local_irq_save(flags);					\
+	flags = hard_local_irq_save();					\
 	v->counter c_op i;						\
-	raw_local_irq_restore(flags);					\
+	hard_local_irq_restore(flags);					\
 }									\
 
 #define ATOMIC_OP_RETURN(op, c_op, asm_op)				\
@@ -177,10 +177,10 @@ static inline int atomic_##op##_return(int i, atomic_t *v)		\
 	unsigned long flags;						\
 	int val;							\
 									\
-	raw_local_irq_save(flags);					\
+	flags = hard_local_irq_save();					\
 	v->counter c_op i;						\
 	val = v->counter;						\
-	raw_local_irq_restore(flags);					\
+	hard_local_irq_restore(flags);					\
 									\
 	return val;							\
 }
@@ -191,10 +191,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)			\
 	unsigned long flags;						\
 	int val;							\
 									\
-	raw_local_irq_save(flags);					\
+	flags = hard_local_irq_save();					\
 	val = v->counter;						\
 	v->counter c_op i;						\
-	raw_local_irq_restore(flags);					\
+	hard_local_irq_restore(flags);					\
 									\
 	return val;							\
 }
@@ -204,11 +204,11 @@ static inline int atomic_cmpxchg(atomic_t *v, int old, int new)
 	int ret;
 	unsigned long flags;
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 	ret = v->counter;
 	if (likely(ret == old))
 		v->counter = new;
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 
 	return ret;
 }
diff --git a/arch/arm/include/asm/bitops.h b/arch/arm/include/asm/bitops.h
index c92e42a5c8f7..9779f321b7dd 100644
--- a/arch/arm/include/asm/bitops.h
+++ b/arch/arm/include/asm/bitops.h
@@ -40,9 +40,9 @@ static inline void ____atomic_set_bit(unsigned int bit, volatile unsigned long *
 
 	p += BIT_WORD(bit);
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 	*p |= mask;
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static inline void ____atomic_clear_bit(unsigned int bit, volatile unsigned long *p)
@@ -52,9 +52,9 @@ static inline void ____atomic_clear_bit(unsigned int bit, volatile unsigned long
 
 	p += BIT_WORD(bit);
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 	*p &= ~mask;
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static inline void ____atomic_change_bit(unsigned int bit, volatile unsigned long *p)
@@ -64,9 +64,9 @@ static inline void ____atomic_change_bit(unsigned int bit, volatile unsigned lon
 
 	p += BIT_WORD(bit);
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 	*p ^= mask;
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static inline int
@@ -78,10 +78,10 @@ ____atomic_test_and_set_bit(unsigned int bit, volatile unsigned long *p)
 
 	p += BIT_WORD(bit);
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 	res = *p;
 	*p = res | mask;
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 
 	return (res & mask) != 0;
 }
@@ -95,10 +95,10 @@ ____atomic_test_and_clear_bit(unsigned int bit, volatile unsigned long *p)
 
 	p += BIT_WORD(bit);
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 	res = *p;
 	*p = res & ~mask;
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 
 	return (res & mask) != 0;
 }
@@ -112,10 +112,10 @@ ____atomic_test_and_change_bit(unsigned int bit, volatile unsigned long *p)
 
 	p += BIT_WORD(bit);
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 	res = *p;
 	*p = res ^ mask;
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 
 	return (res & mask) != 0;
 }
diff --git a/arch/arm/include/asm/clocksource.h b/arch/arm/include/asm/clocksource.h
index 0b350a7e26f3..ceb2048a2846 100644
--- a/arch/arm/include/asm/clocksource.h
+++ b/arch/arm/include/asm/clocksource.h
@@ -1,8 +1,16 @@
 #ifndef _ASM_CLOCKSOURCE_H
 #define _ASM_CLOCKSOURCE_H
 
+enum arch_clock_uaccess_type {
+	ARM_CLOCK_NONE = 0,
+	ARM_CLOCK_ARCH_TIMER,
+
+	ARM_CLOCK_USER_MMIO_BASE, /* Must remain last */
+};
+
 struct arch_clocksource_data {
 	bool vdso_direct;	/* Usable for direct VDSO access? */
+	enum arch_clock_uaccess_type clock_type;
 };
 
 #endif
diff --git a/arch/arm/include/asm/cmpxchg.h b/arch/arm/include/asm/cmpxchg.h
index 8b701f8e175c..60ccad3c936f 100644
--- a/arch/arm/include/asm/cmpxchg.h
+++ b/arch/arm/include/asm/cmpxchg.h
@@ -77,17 +77,17 @@ static inline unsigned long __xchg(unsigned long x, volatile void *ptr, int size
 #error SMP is not supported on this platform
 #endif
 	case 1:
-		raw_local_irq_save(flags);
+		flags = hard_local_irq_save();
 		ret = *(volatile unsigned char *)ptr;
 		*(volatile unsigned char *)ptr = x;
-		raw_local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 		break;
 
 	case 4:
-		raw_local_irq_save(flags);
+		flags = hard_local_irq_save();
 		ret = *(volatile unsigned long *)ptr;
 		*(volatile unsigned long *)ptr = x;
-		raw_local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 		break;
 #else
 	case 1:
diff --git a/arch/arm/include/asm/dovetail.h b/arch/arm/include/asm/dovetail.h
new file mode 100644
index 000000000000..6b975d0ca6d6
--- /dev/null
+++ b/arch/arm/include/asm/dovetail.h
@@ -0,0 +1,30 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016 Philippe Gerum.
+ */
+#ifndef _ASM_ARM_DOVETAIL_H
+#define _ASM_ARM_DOVETAIL_H
+
+/* ARM traps */
+#define ARM_TRAP_ACCESS		0	/* Data or instruction access exception */
+#define ARM_TRAP_SECTION	1	/* Section fault */
+#define ARM_TRAP_DABT		2	/* Generic data abort */
+#define ARM_TRAP_PABT		3	/* Prefetch abort */
+#define ARM_TRAP_BREAK		4	/* Instruction breakpoint */
+#define ARM_TRAP_FPU		5	/* Floating point exception */
+#define ARM_TRAP_VFP		6	/* VFP floating point exception */
+#define ARM_TRAP_UNDEFINSTR	7	/* Undefined instruction */
+#define ARM_TRAP_ALIGNMENT	8	/* Unaligned access exception */
+
+#ifndef __ASSEMBLY__
+
+static inline void arch_dovetail_switch_prepare(bool leave_inband)
+{ }
+
+static inline void arch_dovetail_switch_finish(bool enter_inband)
+{ }
+
+#endif
+
+#endif /* _ASM_ARM_DOVETAIL_H */
diff --git a/arch/arm/include/asm/efi.h b/arch/arm/include/asm/efi.h
index 7667826b93f1..40c33fd846f1 100644
--- a/arch/arm/include/asm/efi.h
+++ b/arch/arm/include/asm/efi.h
@@ -38,7 +38,11 @@ int efi_set_mapping_permissions(struct mm_struct *mm, efi_memory_desc_t *md);
 
 static inline void efi_set_pgd(struct mm_struct *mm)
 {
+	unsigned long flags;
+
+	protect_inband_mm(flags);
 	check_and_switch_context(mm, NULL);
+	unprotect_inband_mm(flags);
 }
 
 void efi_virtmap_load(void);
diff --git a/arch/arm/include/asm/irq_pipeline.h b/arch/arm/include/asm/irq_pipeline.h
new file mode 100644
index 000000000000..ae1173e73ebc
--- /dev/null
+++ b/arch/arm/include/asm/irq_pipeline.h
@@ -0,0 +1,126 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#ifndef _ASM_ARM_IRQ_PIPELINE_H
+#define _ASM_ARM_IRQ_PIPELINE_H
+
+#include <asm-generic/irq_pipeline.h>
+
+#ifdef CONFIG_IRQ_PIPELINE
+
+/*
+ * Out-of-band IPIs are directly mapped to SGI1-2, instead of
+ * multiplexed over SGI0 like regular in-band messages.
+ */
+#define INBAND_NR_IPI		(NR_IPI + 1) /* +IPI_CPU_BACKTRACE */
+#define OOB_IPI_BASE		2048
+#define OOB_NR_IPI		2
+#define TIMER_OOB_IPI		(OOB_IPI_BASE + INBAND_NR_IPI)
+#define RESCHEDULE_OOB_IPI	(OOB_IPI_BASE + INBAND_NR_IPI + 1)
+
+static inline notrace
+unsigned long arch_irqs_virtual_to_native_flags(int stalled)
+{
+	return (!!stalled) << IRQMASK_I_POS;
+}
+
+static inline notrace
+unsigned long arch_irqs_native_to_virtual_flags(unsigned long flags)
+{
+	return (!!hard_irqs_disabled_flags(flags)) << IRQMASK_i_POS;
+}
+
+static inline notrace unsigned long arch_local_irq_save(void)
+{
+	int stalled = inband_irq_save();
+	barrier();
+	return arch_irqs_virtual_to_native_flags(stalled);
+}
+
+static inline notrace void arch_local_irq_enable(void)
+{
+	barrier();
+	inband_irq_enable();
+}
+
+static inline notrace void arch_local_irq_disable(void)
+{
+	inband_irq_disable();
+	barrier();
+}
+
+static inline notrace unsigned long arch_local_save_flags(void)
+{
+	int stalled = inband_irqs_disabled();
+	barrier();
+	return arch_irqs_virtual_to_native_flags(stalled);
+}
+
+static inline int arch_irqs_disabled_flags(unsigned long flags)
+{
+	return native_irqs_disabled_flags(flags);
+}
+
+static inline notrace void arch_local_irq_restore(unsigned long flags)
+{
+	inband_irq_restore(arch_irqs_disabled_flags(flags));
+	barrier();
+}
+
+static inline
+void arch_save_timer_regs(struct pt_regs *dst,
+			  struct pt_regs *src, bool head_context)
+{
+	dst->ARM_cpsr = src->ARM_cpsr;
+	dst->ARM_pc = src->ARM_pc;
+	if (head_context)
+		dst->ARM_cpsr |= IRQMASK_I_BIT;
+}
+
+static inline bool arch_steal_pipelined_tick(struct pt_regs *regs)
+{
+	return !!(regs->ARM_cpsr & IRQMASK_I_BIT);
+}
+
+static inline int arch_enable_oob_stage(void)
+{
+	return 0;
+}
+
+#else /* !CONFIG_IRQ_PIPELINE */
+
+static inline unsigned long arch_local_irq_save(void)
+{
+	return native_irq_save();
+}
+
+static inline void arch_local_irq_enable(void)
+{
+	native_irq_enable();
+}
+
+static inline void arch_local_irq_disable(void)
+{
+	native_irq_disable();
+}
+
+static inline unsigned long arch_local_save_flags(void)
+{
+	return native_save_flags();
+}
+
+static inline void arch_local_irq_restore(unsigned long flags)
+{
+	native_irq_restore(flags);
+}
+
+static inline int arch_irqs_disabled_flags(unsigned long flags)
+{
+	return native_irqs_disabled_flags(flags);
+}
+
+#endif /* !CONFIG_IRQ_PIPELINE */
+
+#endif /* _ASM_ARM_IRQ_PIPELINE_H */
diff --git a/arch/arm/include/asm/irqflags.h b/arch/arm/include/asm/irqflags.h
index aeec7f24eb75..a51be113176c 100644
--- a/arch/arm/include/asm/irqflags.h
+++ b/arch/arm/include/asm/irqflags.h
@@ -13,41 +13,44 @@
 #define IRQMASK_REG_NAME_R "primask"
 #define IRQMASK_REG_NAME_W "primask"
 #define IRQMASK_I_BIT	1
+#define IRQMASK_I_POS	0
 #else
 #define IRQMASK_REG_NAME_R "cpsr"
 #define IRQMASK_REG_NAME_W "cpsr_c"
 #define IRQMASK_I_BIT	PSR_I_BIT
+#define IRQMASK_I_POS	7
 #endif
+#define IRQMASK_i_POS	31
 
 #if __LINUX_ARM_ARCH__ >= 6
 
 #define arch_local_irq_save arch_local_irq_save
-static inline unsigned long arch_local_irq_save(void)
+static inline unsigned long native_irq_save(void)
 {
 	unsigned long flags;
 
 	asm volatile(
-		"	mrs	%0, " IRQMASK_REG_NAME_R "	@ arch_local_irq_save\n"
+		"	mrs	%0, " IRQMASK_REG_NAME_R "	@ native_irq_save\n"
 		"	cpsid	i"
 		: "=r" (flags) : : "memory", "cc");
 	return flags;
 }
 
 #define arch_local_irq_enable arch_local_irq_enable
-static inline void arch_local_irq_enable(void)
+static inline void native_irq_enable(void)
 {
 	asm volatile(
-		"	cpsie i			@ arch_local_irq_enable"
+		"	cpsie i			@ native_irq_enable"
 		:
 		:
 		: "memory", "cc");
 }
 
 #define arch_local_irq_disable arch_local_irq_disable
-static inline void arch_local_irq_disable(void)
+static inline void native_irq_disable(void)
 {
 	asm volatile(
-		"	cpsid i			@ arch_local_irq_disable"
+		"	cpsid i			@ native_irq_disable"
 		:
 		:
 		: "memory", "cc");
@@ -69,12 +72,12 @@ static inline void arch_local_irq_disable(void)
  * Save the current interrupt enable state & disable IRQs
  */
 #define arch_local_irq_save arch_local_irq_save
-static inline unsigned long arch_local_irq_save(void)
+static inline unsigned long native_irq_save(void)
 {
 	unsigned long flags, temp;
 
 	asm volatile(
-		"	mrs	%0, cpsr	@ arch_local_irq_save\n"
+		"	mrs	%0, cpsr	@ native_irq_save\n"
 		"	orr	%1, %0, #128\n"
 		"	msr	cpsr_c, %1"
 		: "=r" (flags), "=r" (temp)
@@ -87,11 +90,11 @@ static inline unsigned long arch_local_irq_save(void)
  * Enable IRQs
  */
 #define arch_local_irq_enable arch_local_irq_enable
-static inline void arch_local_irq_enable(void)
+static inline void native_irq_enable(void)
 {
 	unsigned long temp;
 	asm volatile(
-		"	mrs	%0, cpsr	@ arch_local_irq_enable\n"
+		"	mrs	%0, cpsr	@ native_irq_enable\n"
 		"	bic	%0, %0, #128\n"
 		"	msr	cpsr_c, %0"
 		: "=r" (temp)
@@ -103,11 +106,11 @@ static inline void arch_local_irq_enable(void)
  * Disable IRQs
  */
 #define arch_local_irq_disable arch_local_irq_disable
-static inline void arch_local_irq_disable(void)
+static inline void native_irq_disable(void)
 {
 	unsigned long temp;
 	asm volatile(
-		"	mrs	%0, cpsr	@ arch_local_irq_disable\n"
+		"	mrs	%0, cpsr	@ native_irq_disable\n"
 		"	orr	%0, %0, #128\n"
 		"	msr	cpsr_c, %0"
 		: "=r" (temp)
@@ -149,15 +152,22 @@ static inline void arch_local_irq_disable(void)
 #define local_abt_disable()	do { } while (0)
 #endif
 
+static inline void native_irq_sync(void)
+{
+	native_irq_enable();
+	isb();
+	native_irq_disable();
+}
+
 /*
  * Save the current interrupt enable state.
  */
 #define arch_local_save_flags arch_local_save_flags
-static inline unsigned long arch_local_save_flags(void)
+static inline unsigned long native_save_flags(void)
 {
 	unsigned long flags;
 	asm volatile(
-		"	mrs	%0, " IRQMASK_REG_NAME_R "	@ local_save_flags"
+		"	mrs	%0, " IRQMASK_REG_NAME_R "	@ native_save_flags"
 		: "=r" (flags) : : "memory", "cc");
 	return flags;
 }
@@ -166,21 +176,28 @@ static inline unsigned long arch_local_save_flags(void)
  * restore saved IRQ & FIQ state
  */
 #define arch_local_irq_restore arch_local_irq_restore
-static inline void arch_local_irq_restore(unsigned long flags)
+static inline void native_irq_restore(unsigned long flags)
 {
 	asm volatile(
-		"	msr	" IRQMASK_REG_NAME_W ", %0	@ local_irq_restore"
+		"	msr	" IRQMASK_REG_NAME_W ", %0	@ native_irq_restore"
 		:
 		: "r" (flags)
 		: "memory", "cc");
 }
 
 #define arch_irqs_disabled_flags arch_irqs_disabled_flags
-static inline int arch_irqs_disabled_flags(unsigned long flags)
+static inline int native_irqs_disabled_flags(unsigned long flags)
 {
 	return flags & IRQMASK_I_BIT;
 }
 
+static inline bool native_irqs_disabled(void)
+{
+	unsigned long flags = native_save_flags();
+	return native_irqs_disabled_flags(flags);
+}
+
+#include <asm/irq_pipeline.h>
 #include <asm-generic/irqflags.h>
 
 #endif /* ifdef __KERNEL__ */
diff --git a/arch/arm/include/asm/mmu_context.h b/arch/arm/include/asm/mmu_context.h
index f99ed524fe41..08103f884766 100644
--- a/arch/arm/include/asm/mmu_context.h
+++ b/arch/arm/include/asm/mmu_context.h
@@ -72,6 +72,7 @@ static inline void check_and_switch_context(struct mm_struct *mm,
 static inline void finish_arch_post_lock_switch(void)
 {
 	struct mm_struct *mm = current->mm;
+	unsigned long flags;
 
 	if (mm && mm->context.switch_pending) {
 		/*
@@ -83,7 +84,9 @@ static inline void finish_arch_post_lock_switch(void)
 		preempt_disable();
 		if (mm->context.switch_pending) {
 			mm->context.switch_pending = 0;
+			protect_inband_mm(flags);
 			cpu_switch_mm(mm->pgd, mm);
+			unprotect_inband_mm(flags);
 		}
 		preempt_enable_no_resched();
 	}
@@ -102,7 +105,7 @@ init_new_context(struct task_struct *tsk, struct mm_struct *mm)
 #endif	/* CONFIG_CPU_HAS_ASID */
 
 #define destroy_context(mm)		do { } while(0)
-#define activate_mm(prev,next)		switch_mm(prev, next, NULL)
+#define activate_mm(prev,next)		__switch_mm(prev, next, NULL)
 
 /*
  * This is called when "tsk" is about to enter lazy TLB mode.
@@ -118,15 +121,9 @@ enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)
 {
 }
 
-/*
- * This is the actual mm switch as far as the scheduler
- * is concerned.  No registers are touched.  We avoid
- * calling the CPU specific function when the mm hasn't
- * actually changed.
- */
 static inline void
-switch_mm(struct mm_struct *prev, struct mm_struct *next,
-	  struct task_struct *tsk)
+__switch_mm(struct mm_struct *prev, struct mm_struct *next,
+	    struct task_struct *tsk)
 {
 #ifdef CONFIG_MMU
 	unsigned int cpu = smp_processor_id();
@@ -149,6 +146,30 @@ switch_mm(struct mm_struct *prev, struct mm_struct *next,
 #endif
 }
 
+/*
+ * This is the actual mm switch as far as the scheduler
+ * is concerned.  No registers are touched.  We avoid
+ * calling the CPU specific function when the mm hasn't
+ * actually changed.
+ */
+static inline void
+switch_mm(struct mm_struct *prev, struct mm_struct *next,
+	  struct task_struct *tsk)
+{
+	unsigned long flags;
+
+	protect_inband_mm(flags);
+	__switch_mm(prev, next, tsk);
+	unprotect_inband_mm(flags);
+}
+
 #define deactivate_mm(tsk,mm)	do { } while (0)
 
+static inline void
+switch_oob_mm(struct mm_struct *prev, struct mm_struct *next,
+	      struct task_struct *tsk)
+{
+	__switch_mm(prev, next, tsk);
+}
+
 #endif
diff --git a/arch/arm/include/asm/smp.h b/arch/arm/include/asm/smp.h
index a91f21e3c5b5..b386dc840ed4 100644
--- a/arch/arm/include/asm/smp.h
+++ b/arch/arm/include/asm/smp.h
@@ -30,10 +30,15 @@ extern void show_ipi_list(struct seq_file *, int);
 asmlinkage void do_IPI(int ipinr, struct pt_regs *regs);
 
 /*
- * Called from C code, this handles an IPI.
+ * Called from C code, this handles an IPI (including oob).
  */
 void handle_IPI(int ipinr, struct pt_regs *regs);
 
+/*
+ * Handles IPIs for the in-band stage exclusively.
+ */
+void __handle_IPI(int ipinr, struct pt_regs *regs);
+
 /*
  * Setup the set of possible CPUs (via set_cpu_possible)
  */
@@ -45,6 +50,11 @@ extern void smp_init_cpus(void);
  */
 extern void set_smp_cross_call(void (*)(const struct cpumask *, unsigned int));
 
+/*
+ * Raise an IPI.
+ */
+void smp_cross_call(const struct cpumask *target, unsigned int ipinr);
+
 /*
  * Called from platform specific assembly code, this is the
  * secondary CPU entry point.
diff --git a/arch/arm/include/asm/thread_info.h b/arch/arm/include/asm/thread_info.h
index 0d0d5178e2c3..d1469e7f01d3 100644
--- a/arch/arm/include/asm/thread_info.h
+++ b/arch/arm/include/asm/thread_info.h
@@ -21,6 +21,7 @@
 
 struct task_struct;
 
+#include <dovetail/thread_info.h>
 #include <asm/types.h>
 
 typedef unsigned long mm_segment_t;
@@ -45,6 +46,7 @@ struct cpu_context_save {
  */
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
+	__u32			local_flags;	/* local (synchronous) flags */
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
 	mm_segment_t		addr_limit;	/* address limit */
 	struct task_struct	*task;		/* main task structure */
@@ -65,16 +67,20 @@ struct thread_info {
 #ifdef CONFIG_ARM_THUMBEE
 	unsigned long		thumbee_state;	/* ThumbEE Handler Base register */
 #endif
+	struct oob_thread_state	oob_state; /* co-kernel thread state */
 };
 
 #define INIT_THREAD_INFO(tsk)						\
 {									\
 	.task		= &tsk,						\
 	.flags		= 0,						\
+	.local_flags	= 0,						\
 	.preempt_count	= INIT_PREEMPT_COUNT,				\
 	.addr_limit	= KERNEL_DS,					\
 }
 
+#define ti_local_flags(__ti)	((__ti)->local_flags)
+
 /*
  * how to get the current stack pointer in C
  */
@@ -145,6 +151,7 @@ extern int vfp_restore_user_hwstate(struct user_vfp *,
 #define TIF_USING_IWMMXT	17
 #define TIF_MEMDIE		18	/* is terminating due to OOM killer */
 #define TIF_RESTORE_SIGMASK	20
+#define TIF_MAYDAY		21	/* emergency trap pending */
 
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
@@ -155,6 +162,7 @@ extern int vfp_restore_user_hwstate(struct user_vfp *,
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
 #define _TIF_USING_IWMMXT	(1 << TIF_USING_IWMMXT)
+#define _TIF_MAYDAY		(1 << TIF_MAYDAY)
 
 /* Checks for any syscall work in entry-common.S */
 #define _TIF_SYSCALL_WORK (_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
@@ -166,5 +174,12 @@ extern int vfp_restore_user_hwstate(struct user_vfp *,
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
 				 _TIF_NOTIFY_RESUME | _TIF_UPROBE)
 
+/*
+ * Local (synchronous) thread flags.
+ */
+#define _TLF_OOB		0x0001
+#define _TLF_DOVETAIL		0x0002
+#define _TLF_OFFSTAGE		0x0004
+
 #endif /* __KERNEL__ */
 #endif /* __ASM_ARM_THREAD_INFO_H */
diff --git a/arch/arm/include/asm/trace/exceptions.h b/arch/arm/include/asm/trace/exceptions.h
new file mode 100644
index 000000000000..bdb666b3da4e
--- /dev/null
+++ b/arch/arm/include/asm/trace/exceptions.h
@@ -0,0 +1,62 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM exceptions
+
+#if !defined(_TRACE_EXCEPTIONS_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_EXCEPTIONS_H
+
+#include <linux/tracepoint.h>
+#include <asm/ptrace.h>
+#include <asm/dovetail.h>
+
+#define __trace_trap(__sym)	{ __sym, #__sym }
+
+#define trace_trap_symbolic(__trapnr)				\
+	__print_symbolic(__trapnr,				\
+			__trace_trap(ARM_TRAP_ACCESS),		\
+			__trace_trap(ARM_TRAP_SECTION),		\
+			__trace_trap(ARM_TRAP_DABT),		\
+			__trace_trap(ARM_TRAP_PABT),		\
+			__trace_trap(ARM_TRAP_BREAK),		\
+			__trace_trap(ARM_TRAP_FPU),		\
+			__trace_trap(ARM_TRAP_VFP),		\
+			__trace_trap(ARM_TRAP_UNDEFINSTR),	\
+			__trace_trap(ARM_TRAP_ALIGNMENT))
+
+DECLARE_EVENT_CLASS(ARM_trap_event,
+	TP_PROTO(int trapnr, struct pt_regs *regs),
+	TP_ARGS(trapnr, regs),
+
+	TP_STRUCT__entry(
+		__field(int, trapnr)
+		__field(struct pt_regs *, regs)
+		),
+
+	TP_fast_assign(
+		__entry->trapnr = trapnr;
+		__entry->regs = regs;
+		),
+
+	TP_printk("%s mode trap: %s",
+		user_mode(__entry->regs) ? "user" : "kernel",
+		trace_trap_symbolic(__entry->trapnr))
+);
+
+DEFINE_EVENT(ARM_trap_event, ARM_trap_entry,
+	TP_PROTO(int trapnr, struct pt_regs *regs),
+	TP_ARGS(trapnr, regs)
+);
+
+DEFINE_EVENT(ARM_trap_event, ARM_trap_exit,
+	TP_PROTO(int trapnr, struct pt_regs *regs),
+	TP_ARGS(trapnr, regs)
+);
+
+#undef TRACE_INCLUDE_PATH
+#undef TRACE_INCLUDE_FILE
+#define TRACE_INCLUDE_PATH asm/trace
+#define TRACE_INCLUDE_FILE exceptions
+#endif /*  _TRACE_EXCEPTIONS_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/arch/arm/include/asm/vdso_datapage.h b/arch/arm/include/asm/vdso_datapage.h
index 7910abf89b1c..7f40de454add 100644
--- a/arch/arm/include/asm/vdso_datapage.h
+++ b/arch/arm/include/asm/vdso_datapage.h
@@ -19,7 +19,7 @@
  */
 struct vdso_data {
 	u32 seq_count;		/* sequence count - odd during updates */
-	u16 tk_is_cntvct;	/* fall back to syscall if false */
+	u32 cs_type_and_seq;	/* clocksource type and change count */
 	u16 cs_shift;		/* clocksource shift */
 	u32 xtime_coarse_sec;	/* coarse time */
 	u32 xtime_coarse_nsec;
@@ -35,6 +35,7 @@ struct vdso_data {
 	u64 xtime_clock_snsec;	/* CLOCK_REALTIME sub-ns base */
 	u32 tz_minuteswest;	/* timezone info for gettimeofday(2) */
 	u32 tz_dsttime;
+	unsigned char mmio_dev_name[64];
 };
 
 union vdso_data_store {
diff --git a/arch/arm/include/uapi/asm/dovetail.h b/arch/arm/include/uapi/asm/dovetail.h
new file mode 100644
index 000000000000..8413e8f2389e
--- /dev/null
+++ b/arch/arm/include/uapi/asm/dovetail.h
@@ -0,0 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+#ifndef _UAPI_ASM_DOVETAIL_H
+#define _UAPI_ASM_DOVETAIL_H
+
+#define __ARM_NR_dovetail	0xf0042
+
+#endif /* _UAPI_ASM_DOVETAIL_H */
diff --git a/arch/arm/kernel/Makefile b/arch/arm/kernel/Makefile
index 8cad59465af3..4771a5876d59 100644
--- a/arch/arm/kernel/Makefile
+++ b/arch/arm/kernel/Makefile
@@ -88,6 +88,11 @@ obj-$(CONFIG_PARAVIRT)	+= paravirt.o
 head-y			:= head$(MMUEXT).o
 obj-$(CONFIG_DEBUG_LL)	+= debug.o
 obj-$(CONFIG_EARLY_PRINTK)	+= early_printk.o
+ifeq ($(CONFIG_DEBUG_LL),y)
+obj-$(CONFIG_RAW_PRINTK)	+= raw_printk.o
+endif
+
+obj-$(CONFIG_IRQ_PIPELINE)	+= irq_pipeline.o
 
 # This is executed very early using a temporary stack when no memory allocator
 # nor global data is available. Everything has to be allocated on the stack.
diff --git a/arch/arm/kernel/asm-offsets.c b/arch/arm/kernel/asm-offsets.c
index c773b829ee8e..95a876cbba06 100644
--- a/arch/arm/kernel/asm-offsets.c
+++ b/arch/arm/kernel/asm-offsets.c
@@ -52,6 +52,7 @@ int main(void)
 #endif
   BLANK();
   DEFINE(TI_FLAGS,		offsetof(struct thread_info, flags));
+  DEFINE(TI_LOCAL_FLAGS,	offsetof(struct thread_info, local_flags));
   DEFINE(TI_PREEMPT,		offsetof(struct thread_info, preempt_count));
   DEFINE(TI_ADDR_LIMIT,		offsetof(struct thread_info, addr_limit));
   DEFINE(TI_TASK,		offsetof(struct thread_info, task));
@@ -61,6 +62,7 @@ int main(void)
   DEFINE(TI_USED_CP,		offsetof(struct thread_info, used_cp));
   DEFINE(TI_TP_VALUE,		offsetof(struct thread_info, tp_value));
   DEFINE(TI_FPSTATE,		offsetof(struct thread_info, fpstate));
+  DEFINE(TI_OOB_MASK,		STAGE_MASK);
 #ifdef CONFIG_VFP
   DEFINE(TI_VFPSTATE,		offsetof(struct thread_info, vfpstate));
 #ifdef CONFIG_SMP
@@ -177,6 +179,7 @@ int main(void)
   BLANK();
 #ifdef CONFIG_VDSO
   DEFINE(VDSO_DATA_SIZE,	sizeof(union vdso_data_store));
+  DEFINE(VDSO_PRIV_SIZE,	PAGE_SIZE);
 #endif
   BLANK();
 #ifdef CONFIG_ARM_MPU
diff --git a/arch/arm/kernel/entry-armv.S b/arch/arm/kernel/entry-armv.S
index 858d4e541532..67517f26f3af 100644
--- a/arch/arm/kernel/entry-armv.S
+++ b/arch/arm/kernel/entry-armv.S
@@ -5,6 +5,7 @@
  *  Copyright (C) 1996,1997,1998 Russell King.
  *  ARM700 fix by Matthew Godbolt (linux-user@willothewisp.demon.co.uk)
  *  nommu support by Hyok S. Choi (hyok.choi@samsung.com)
+ *  Copyright (C) 2005 Stelian Pop.
  *
  *  Low-level vector interface routines
  *
@@ -31,16 +32,24 @@
 #include "entry-header.S"
 #include <asm/entry-macro-multi.S>
 #include <asm/probes.h>
+#include <asm/dovetail.h>
 
 /*
  * Interrupt handling.
  */
 	.macro	irq_handler
 #ifdef CONFIG_GENERIC_IRQ_MULTI_HANDLER
-	ldr	r1, =handle_arch_irq
 	mov	r0, sp
 	badr	lr, 9997f
+#ifdef CONFIG_IRQ_PIPELINE
+	ldr	r1, =handle_arch_irq_pipelined
+	mov	pc, r1
+#else
+	ldr	r1, =handle_arch_irq
 	ldr	pc, [r1]
+#endif
+#elif CONFIG_IRQ_PIPELINE
+#error "Legacy IRQ handling not pipelined"
 #else
 	arch_irq_handler_default
 #endif
@@ -210,6 +219,10 @@ ENDPROC(__dabt_svc)
 __irq_svc:
 	svc_entry
 	irq_handler
+#ifdef CONFIG_IRQ_PIPELINE
+	tst	r0, r0
+	beq	1f
+#endif
 
 #ifdef CONFIG_PREEMPT
 	ldr	r8, [tsk, #TI_PREEMPT]		@ get preempt count
@@ -220,6 +233,7 @@ __irq_svc:
 	blne	svc_preempt
 #endif
 
+1:
 	svc_exit r5, irq = 1			@ return from exception
  UNWIND(.fnend		)
 ENDPROC(__irq_svc)
@@ -258,6 +272,16 @@ __und_svc:
 	svc_entry MAX_STACK_SIZE
 #else
 	svc_entry
+#endif
+#ifdef CONFIG_DOVETAIL
+	get_thread_info tsk
+	ldr	r0, [tsk, #TI_PREEMPT]		@ get preempt count
+	tst	r0, #TI_OOB_MASK		@ oob stage?
+	beq	1f
+	mov	r0, #ARM_TRAP_UNDEFINSTR
+	mov	r1, sp				@ r1 = &regs
+	bl	__oob_trap_notify
+1:
 #endif
 	@
 	@ call emulation code, which returns using r9 if it has emulated
@@ -455,8 +479,13 @@ __irq_usr:
 	usr_entry
 	kuser_cmpxchg_check
 	irq_handler
-	get_thread_info tsk
 	mov	why, #0
+#ifdef CONFIG_IRQ_PIPELINE
+THUMB(	it ne)
+	tst	r0, r0
+	beq	fast_ret_to_user	@ fast exit if in-band stage is stalled
+#endif
+	get_thread_info tsk
 	b	ret_to_user_from_irq
  UNWIND(.fnend		)
 ENDPROC(__irq_usr)
@@ -467,6 +496,18 @@ ENDPROC(__irq_usr)
 __und_usr:
 	usr_entry uaccess=0
 
+#ifdef CONFIG_DOVETAIL
+	get_thread_info tsk
+	ldr	r0, [tsk, #TI_PREEMPT]		@ get preempt count
+	tst	r0, #TI_OOB_MASK		@ oob stage?
+	beq	1f
+	mov	r0, #ARM_TRAP_UNDEFINSTR
+	mov	r1, sp				@ r1 = &regs
+	bl	__oob_trap_notify
+	uaccess_enable ip
+1:
+#endif
+
 	mov	r2, r4
 	mov	r3, r5
 
@@ -751,7 +792,7 @@ ENTRY(ret_from_exception)
  UNWIND(.cantunwind	)
 	get_thread_info tsk
 	mov	why, #0
-	b	ret_to_user
+	ret_to_user_pipelined r1
  UNWIND(.fnend		)
 ENDPROC(__pabt_usr)
 ENDPROC(ret_from_exception)
diff --git a/arch/arm/kernel/entry-common.S b/arch/arm/kernel/entry-common.S
index 271cb8a1eba1..e4a587b5c3ec 100644
--- a/arch/arm/kernel/entry-common.S
+++ b/arch/arm/kernel/entry-common.S
@@ -3,6 +3,7 @@
  *  linux/arch/arm/kernel/entry-common.S
  *
  *  Copyright (C) 2000 Russell King
+ *  Copyright (C) 2005 Stelian Pop.
  */
 
 #include <asm/assembler.h>
@@ -12,6 +13,7 @@
 #include <asm/memory.h>
 #ifdef CONFIG_AEABI
 #include <asm/unistd-oabi.h>
+#include <uapi/asm/dovetail.h>
 #endif
 
 	.equ	NR_syscalls, __NR_syscalls
@@ -143,11 +145,16 @@ no_work_pending:
 	restore_user_regs fast = 0, offset = 0
 ENDPROC(ret_to_user_from_irq)
 ENDPROC(ret_to_user)
+ENTRY(fast_ret_to_user)
+	disable_irq_notrace			@ disable interrupts
+	b	no_work_pending
+ENDPROC(fast_ret_to_user)
 
 /*
  * This is how we return from a fork.
  */
 ENTRY(ret_from_fork)
+	enable_irq_if_pipelined
 	bl	schedule_tail
 	cmp	r5, #0
 	movne	r0, r4
@@ -248,6 +255,45 @@ ENTRY(vector_swi)
  TRACE(	ldmia	sp, {r0 - r3}		)
 
 local_restart:
+#ifdef CONFIG_DOVETAIL
+	ldr	r10, [tsk, #TI_LOCAL_FLAGS]	@ tsk(r9) is callee-saved
+	ldr	r0, =(__ARM_NR_dovetail - __NR_SYSCALL_BASE)
+	cmp	scno, r0
+	bne	slow_path
+	tst	r10, #_TLF_OOB
+	beq	slow_path
+	mov	r0, sp				@ regs
+	bl	handle_oob_syscall
+	ldr	r10, [tsk, #TI_LOCAL_FLAGS]
+	tst	r10, #_TLF_OOB
+	bne	fastcall_exit_check		@ check for MAYDAY
+	bl	sync_inband_irqs
+	b	ret_slow_syscall
+fastcall_exit_check:
+	ldr	r10, [tsk, #TI_FLAGS]
+	tst	r10, #_TIF_MAYDAY
+	beq	fast_ret_to_user
+	mov	r0, sp
+	bl	dovetail_call_mayday
+	b	fast_ret_to_user
+slow_path:
+	tst	r10, #_TLF_DOVETAIL
+	bne	pipeline_syscall
+	cmp	scno, r0
+	bne	root_syscall
+pipeline_syscall:
+	mov	r1, sp				@ regs
+	mov	r0, tsk
+	bl	__pipeline_syscall
+	ldr	r10, [tsk, #TI_LOCAL_FLAGS]
+	tst	r10, #_TLF_OOB
+	bne	fast_ret_to_user
+	cmp	r0, #0
+	bgt	ret_slow_syscall
+root_syscall:
+	ldmia	sp, { r0 - r3 }
+#endif /* CONFIG_DOVETAIL */
+
 	ldr	r10, [tsk, #TI_FLAGS]		@ check for syscall tracing
 	stmdb	sp!, {r4, r5}			@ push fifth and sixth args
 
diff --git a/arch/arm/kernel/entry-header.S b/arch/arm/kernel/entry-header.S
index 32051ec5b33f..9d9d06a158cb 100644
--- a/arch/arm/kernel/entry-header.S
+++ b/arch/arm/kernel/entry-header.S
@@ -205,14 +205,14 @@
 #ifdef CONFIG_TRACE_IRQFLAGS
 	@ The parent context IRQs must have been enabled to get here in
 	@ the first place, so there's no point checking the PSR I bit.
-	bl	trace_hardirqs_on
+	bl	trace_hardirqs_on_pipelined
 #endif
 	.else
 	@ IRQs off again before pulling preserved data off the stack
 	disable_irq_notrace
 #ifdef CONFIG_TRACE_IRQFLAGS
 	tst	\rpsr, #PSR_I_BIT
-	bleq	trace_hardirqs_on
+	bleq	trace_hardirqs_on_pipelined
 	tst	\rpsr, #PSR_I_BIT
 	blne	trace_hardirqs_off
 #endif
@@ -404,6 +404,19 @@
 #endif
 	.endm
 
+/*
+ * Branch to the exception epilogue, skipping the in-band work
+ * if running over the out-of-band interrupt stage.
+ */
+	.macro ret_to_user_pipelined, tmp
+#ifdef CONFIG_IRQ_PIPELINE
+	ldr	\tmp, [tsk, #TI_LOCAL_FLAGS]
+	tst	\tmp, #_TLF_OOB
+	bne	fast_ret_to_user
+#endif
+	b	ret_to_user
+	.endm
+
 /*
  * These are the registers used in the syscall handler, and allow us to
  * have in theory up to 7 arguments to a function - r0 to r6.
diff --git a/arch/arm/kernel/irq.c b/arch/arm/kernel/irq.c
index ee514034c0a1..d50a12ab2bba 100644
--- a/arch/arm/kernel/irq.c
+++ b/arch/arm/kernel/irq.c
@@ -24,6 +24,7 @@
 #include <linux/interrupt.h>
 #include <linux/irq.h>
 #include <linux/irqchip.h>
+#include <linux/irq_pipeline.h>
 #include <linux/random.h>
 #include <linux/smp.h>
 #include <linux/init.h>
@@ -98,6 +99,14 @@ void __init init_IRQ(void)
 	uniphier_cache_init();
 }
 
+#ifdef CONFIG_IRQ_PIPELINE
+asmlinkage int __exception_irq_entry
+handle_arch_irq_pipelined(struct pt_regs *regs)
+{
+	return handle_irq_pipelined(regs);
+}
+#endif
+
 #ifdef CONFIG_SPARSE_IRQ
 int __init arch_probe_nr_irqs(void)
 {
diff --git a/arch/arm/kernel/irq_pipeline.c b/arch/arm/kernel/irq_pipeline.c
new file mode 100644
index 000000000000..a275c6fc8305
--- /dev/null
+++ b/arch/arm/kernel/irq_pipeline.c
@@ -0,0 +1,93 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/irq.h>
+#include <linux/irq_pipeline.h>
+#include <asm/exception.h>
+
+#ifdef CONFIG_SMP
+
+static struct irq_domain *sipic_domain;
+
+static void sipic_irq_noop(struct irq_data *data) { }
+
+static unsigned int sipic_irq_noop_ret(struct irq_data *data)
+{
+	return 0;
+}
+
+static struct irq_chip sipic_chip = {
+	.name		= "SIPIC",
+	.irq_startup	= sipic_irq_noop_ret,
+	.irq_shutdown	= sipic_irq_noop,
+	.irq_enable	= sipic_irq_noop,
+	.irq_disable	= sipic_irq_noop,
+	.flags		= IRQCHIP_PIPELINE_SAFE | IRQCHIP_SKIP_SET_WAKE,
+};
+
+static int sipic_irq_map(struct irq_domain *d, unsigned int irq,
+			irq_hw_number_t hwirq)
+{
+	irq_set_percpu_devid(irq);
+	irq_set_chip_and_handler(irq, &sipic_chip, handle_synthetic_irq);
+
+	return 0;
+}
+
+static struct irq_domain_ops sipic_domain_ops = {
+	.map	= sipic_irq_map,
+};
+
+static void create_ipi_domain(void)
+{
+	/*
+	 * Create an IRQ domain for mapping all IPIs (in-band and
+	 * out-of-band), with fixed sirq numbers starting from
+	 * OOB_IPI_BASE. The sirqs obtained can be injected into the
+	 * pipeline upon IPI receipt like other interrupts.
+	 */
+	sipic_domain = irq_domain_add_simple(NULL, INBAND_NR_IPI + OOB_NR_IPI,
+					     OOB_IPI_BASE,
+					     &sipic_domain_ops, NULL);
+}
+
+void irq_pipeline_send_remote(unsigned int ipi,
+			      const struct cpumask *cpumask)
+{
+	unsigned int ipinr = ipi - OOB_IPI_BASE;
+	smp_cross_call(cpumask, ipinr);
+}
+EXPORT_SYMBOL_GPL(irq_pipeline_send_remote);
+
+#endif	/* CONFIG_SMP */
+
+void arch_do_IRQ_pipelined(struct irq_desc *desc)
+{
+	struct pt_regs *regs = raw_cpu_ptr(&irq_pipeline.tick_regs);
+	unsigned int irq = irq_desc_get_irq(desc);
+
+#ifdef CONFIG_SMP
+	/*
+	 * Check for IPIs, handing them over to the specific dispatch
+	 * code.
+	 */
+	if (irq >= OOB_IPI_BASE &&
+	    irq < OOB_IPI_BASE + INBAND_NR_IPI + OOB_NR_IPI) {
+		__handle_IPI(irq - OOB_IPI_BASE, regs);
+		return;
+	}
+#endif
+
+	__handle_domain_irq(NULL, irq, false, regs);
+}
+
+void __init arch_irq_pipeline_init(void)
+{
+#ifdef CONFIG_SMP
+	create_ipi_domain();
+#endif
+}
diff --git a/arch/arm/kernel/patch.c b/arch/arm/kernel/patch.c
index d0a05a3bdb96..86108122ae00 100644
--- a/arch/arm/kernel/patch.c
+++ b/arch/arm/kernel/patch.c
@@ -16,7 +16,7 @@ struct patch {
 	unsigned int insn;
 };
 
-static DEFINE_RAW_SPINLOCK(patch_lock);
+static DEFINE_HARD_SPINLOCK(patch_lock);
 
 static void __kprobes *patch_map(void *addr, int fixmap, unsigned long *flags)
 	__acquires(&patch_lock)
diff --git a/arch/arm/kernel/process.c b/arch/arm/kernel/process.c
index 9485acc520a4..7cd001e6e968 100644
--- a/arch/arm/kernel/process.c
+++ b/arch/arm/kernel/process.c
@@ -69,7 +69,7 @@ void arch_cpu_idle(void)
 		arm_pm_idle();
 	else
 		cpu_do_idle();
-	local_irq_enable();
+	local_irq_enable_full();
 }
 
 void arch_cpu_idle_prepare(void)
diff --git a/arch/arm/kernel/ptrace.c b/arch/arm/kernel/ptrace.c
index 324352787aea..0c484dcae91b 100644
--- a/arch/arm/kernel/ptrace.c
+++ b/arch/arm/kernel/ptrace.c
@@ -206,6 +206,7 @@ void ptrace_break(struct pt_regs *regs)
 
 static int break_trap(struct pt_regs *regs, unsigned int instr)
 {
+	oob_trap_notify(ARM_TRAP_BREAK, regs);
 	ptrace_break(regs);
 	return 0;
 }
diff --git a/arch/arm/kernel/raw_printk.c b/arch/arm/kernel/raw_printk.c
new file mode 100644
index 000000000000..9024b772fca4
--- /dev/null
+++ b/arch/arm/kernel/raw_printk.c
@@ -0,0 +1,30 @@
+#include <linux/kernel.h>
+#include <linux/console.h>
+#include <linux/init.h>
+
+/*
+ * If both CONFIG_DEBUG_LL and CONFIG_RAW_PRINTK are set, create a
+ * console device sending the raw output to printascii().
+ */
+void printascii(const char *s);
+
+static void raw_console_write(struct console *co,
+			      const char *s, unsigned count)
+{
+	printascii(s);
+}
+
+static struct console raw_console = {
+	.name		= "rawcon",
+	.write_raw	= raw_console_write,
+	.flags		= CON_PRINTBUFFER | CON_ENABLED,
+	.index		= -1,
+};
+
+static int __init raw_console_init(void)
+{
+	register_console(&raw_console);
+
+	return 0;
+}
+console_initcall(raw_console_init);
diff --git a/arch/arm/kernel/signal.c b/arch/arm/kernel/signal.c
index ab2568996ddb..7601879fa841 100644
--- a/arch/arm/kernel/signal.c
+++ b/arch/arm/kernel/signal.c
@@ -8,6 +8,7 @@
 #include <linux/random.h>
 #include <linux/signal.h>
 #include <linux/personality.h>
+#include <linux/irq_pipeline.h>
 #include <linux/uaccess.h>
 #include <linux/tracehook.h>
 #include <linux/uprobes.h>
@@ -642,13 +643,22 @@ static int do_signal(struct pt_regs *regs, int syscall)
 asmlinkage int
 do_work_pending(struct pt_regs *regs, unsigned int thread_flags, int syscall)
 {
+	WARN_ON_ONCE(irq_pipeline_debug() &&
+		(irqs_disabled() || running_oob()));
+
 	/*
 	 * The assembly code enters us with IRQs off, but it hasn't
 	 * informed the tracing code of that for efficiency reasons.
 	 * Update the trace code with the current status.
 	 */
-	trace_hardirqs_off();
+	if (!irqs_pipelined())
+		trace_hardirqs_off();
 	do {
+		if (irqs_pipelined()) {
+			local_irq_disable();
+			hard_cond_local_irq_enable();
+		}
+
 		if (likely(thread_flags & _TIF_NEED_RESCHED)) {
 			schedule();
 		} else {
@@ -674,9 +684,9 @@ do_work_pending(struct pt_regs *regs, unsigned int thread_flags, int syscall)
 				rseq_handle_notify_resume(NULL, regs);
 			}
 		}
-		local_irq_disable();
+		hard_local_irq_disable();
 		thread_flags = current_thread_info()->flags;
-	} while (thread_flags & _TIF_WORK_MASK);
+	} while (inband_irq_pending() || (thread_flags & _TIF_WORK_MASK));
 	return 0;
 }
 
diff --git a/arch/arm/kernel/smp.c b/arch/arm/kernel/smp.c
index 4b0bab2607e4..275e7474ff49 100644
--- a/arch/arm/kernel/smp.c
+++ b/arch/arm/kernel/smp.c
@@ -393,6 +393,13 @@ asmlinkage void secondary_start_kernel(void)
 	enter_lazy_tlb(mm, current);
 	local_flush_tlb_all();
 
+	/*
+	 * When pipelining IRQs, debug_smp_processor_id() accesses
+	 * percpu data.
+	 */
+	if (irqs_pipelined())
+		set_my_cpu_offset(per_cpu_offset(raw_smp_processor_id()));
+
 	/*
 	 * All kernel threads share the same mm context; grab a
 	 * reference and switch to it.
@@ -435,7 +442,7 @@ asmlinkage void secondary_start_kernel(void)
 
 	complete(&cpu_running);
 
-	local_irq_enable();
+	local_irq_enable_full();
 	local_fiq_enable();
 	local_abt_enable();
 
@@ -517,12 +524,68 @@ static const char *ipi_types[NR_IPI] __tracepoint_string = {
 	S(IPI_COMPLETION, "completion interrupts"),
 };
 
-static void smp_cross_call(const struct cpumask *target, unsigned int ipinr)
+#ifdef CONFIG_IRQ_PIPELINE
+
+static DEFINE_PER_CPU(unsigned long, ipi_messages);
+
+static inline
+void send_IPI_message(const struct cpumask *target, unsigned int ipinr)
+{
+	unsigned int cpu, sgi;
+
+	if (ipinr < INBAND_NR_IPI) {
+		/* regular in-band IPI (multiplexed over SGI0). */
+		trace_ipi_raise_rcuidle(target, ipi_types[ipinr]);
+		for_each_cpu(cpu, target)
+			set_bit(ipinr, &per_cpu(ipi_messages, cpu));
+		smp_mb();
+		sgi = 0;
+	} else	/* out-of-band IPI (SGI1-3). */
+		sgi = ipinr - INBAND_NR_IPI + 1;
+
+	__smp_cross_call(target, sgi);
+}
+
+static inline
+void handle_IPI_pipelined(int sgi, struct pt_regs *regs)
+{
+	unsigned int ipinr, irq;
+	unsigned long *pmsg;
+
+	if (sgi) {		/* SGI1-3 */
+		irq = sgi + INBAND_NR_IPI - 1 + OOB_IPI_BASE;
+		generic_pipeline_irq(irq, regs);
+		return;
+	}
+
+	/* In-band IPI (0..INBAND_NR_IPI - 1) multiplexed over SGI0. */
+	pmsg = raw_cpu_ptr(&ipi_messages);
+	while (*pmsg) {
+		ipinr = ffs(*pmsg) - 1;
+		clear_bit(ipinr, pmsg);
+		irq = OOB_IPI_BASE + ipinr;
+		generic_pipeline_irq(irq, regs);
+	}
+}
+
+#else
+
+static inline
+void send_IPI_message(const struct cpumask *target, unsigned int ipinr)
 {
-	trace_ipi_raise_rcuidle(target, ipi_types[ipinr]);
 	__smp_cross_call(target, ipinr);
 }
 
+static inline void handle_IPI_pipelined(int ipinr, struct pt_regs *regs)
+{ }
+
+#endif /* CONFIG_IRQ_PIPELINE */
+
+void smp_cross_call(const struct cpumask *target, unsigned int ipinr)
+{
+	send_IPI_message(target, ipinr);
+}
+
 void show_ipi_list(struct seq_file *p, int prec)
 {
 	unsigned int cpu, i;
@@ -596,7 +659,7 @@ static void ipi_cpu_stop(unsigned int cpu)
 	set_cpu_online(cpu, false);
 
 	local_fiq_disable();
-	local_irq_disable();
+	hard_local_irq_disable();
 
 	while (1) {
 		cpu_relax();
@@ -625,7 +688,7 @@ asmlinkage void __exception_irq_entry do_IPI(int ipinr, struct pt_regs *regs)
 	handle_IPI(ipinr, regs);
 }
 
-void handle_IPI(int ipinr, struct pt_regs *regs)
+void __handle_IPI(int ipinr, struct pt_regs *regs)
 {
 	unsigned int cpu = smp_processor_id();
 	struct pt_regs *old_regs = set_irq_regs(regs);
@@ -685,6 +748,18 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 		printk_nmi_exit();
 		break;
 
+#ifdef CONFIG_IRQ_PIPELINE
+	/*
+	 * In the unlikely event out-of-band IPIs have a in-band stage
+	 * handler.
+	 */
+	case INBAND_NR_IPI ... INBAND_NR_IPI + OOB_NR_IPI - 1:
+		irq_enter();
+		generic_handle_irq(OOB_IPI_BASE + ipinr);
+		irq_exit();
+		break;
+#endif
+
 	default:
 		pr_crit("CPU%u: Unknown IPI message 0x%x\n",
 		        cpu, ipinr);
@@ -696,6 +771,14 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 	set_irq_regs(old_regs);
 }
 
+void handle_IPI(int ipinr, struct pt_regs *regs)
+{
+	if (irqs_pipelined())
+		handle_IPI_pipelined(ipinr, regs);
+	else
+		__handle_IPI(ipinr, regs);
+}
+
 void smp_send_reschedule(int cpu)
 {
 	smp_cross_call(cpumask_of(cpu), IPI_RESCHEDULE);
diff --git a/arch/arm/kernel/smp_twd.c b/arch/arm/kernel/smp_twd.c
index 9a14f721a2b0..8377f1d030c3 100644
--- a/arch/arm/kernel/smp_twd.c
+++ b/arch/arm/kernel/smp_twd.c
@@ -31,7 +31,7 @@ static DEFINE_PER_CPU(bool, percpu_setup_called);
 
 static struct clock_event_device __percpu *twd_evt;
 static unsigned int twd_features =
-		CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT;
+		CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_PIPELINE;
 static int twd_ppi;
 
 static int twd_shutdown(struct clock_event_device *clk)
@@ -182,7 +182,7 @@ static irqreturn_t twd_handler(int irq, void *dev_id)
 	struct clock_event_device *evt = dev_id;
 
 	if (twd_timer_ack()) {
-		evt->event_handler(evt);
+		clockevents_handle_event(evt);
 		return IRQ_HANDLED;
 	}
 
@@ -279,7 +279,8 @@ static int __init twd_local_timer_common_register(struct device_node *np)
 		goto out_free;
 	}
 
-	err = request_percpu_irq(twd_ppi, twd_handler, "twd", twd_evt);
+	err = __request_percpu_irq(twd_ppi, twd_handler,
+				   IRQF_TIMER, "twd", twd_evt);
 	if (err) {
 		pr_err("twd: can't register interrupt %d (%d)\n", twd_ppi, err);
 		goto out_free;
diff --git a/arch/arm/kernel/traps.c b/arch/arm/kernel/traps.c
index c053abd1fb53..e48a85f03ea1 100644
--- a/arch/arm/kernel/traps.c
+++ b/arch/arm/kernel/traps.c
@@ -396,7 +396,7 @@ int is_valid_bugaddr(unsigned long pc)
 #endif
 
 static LIST_HEAD(undef_hook);
-static DEFINE_RAW_SPINLOCK(undef_lock);
+static DEFINE_HARD_SPINLOCK(undef_lock);
 
 void register_undef_hook(struct undef_hook *hook)
 {
diff --git a/arch/arm/kernel/vdso.c b/arch/arm/kernel/vdso.c
index 9bf16c93ee6a..749e3f60c22d 100644
--- a/arch/arm/kernel/vdso.c
+++ b/arch/arm/kernel/vdso.c
@@ -24,13 +24,14 @@
 #include <asm/vdso_datapage.h>
 #include <clocksource/arm_arch_timer.h>
 
-#define MAX_SYMNAME	64
-
 static struct page **vdso_text_pagelist;
 
 extern char vdso_start[], vdso_end[];
 
-/* Total number of pages needed for the data and text portions of the VDSO. */
+/*
+ * Total number of pages needed for the data, private and text
+ * portions of the VDSO.
+ */
 unsigned int vdso_total_pages __ro_after_init;
 
 /*
@@ -51,8 +52,8 @@ static int vdso_mremap(const struct vm_special_mapping *sm,
 	unsigned long new_size = new_vma->vm_end - new_vma->vm_start;
 	unsigned long vdso_size;
 
-	/* without VVAR page */
-	vdso_size = (vdso_total_pages - 1) << PAGE_SHIFT;
+	/* without VVAR and VPRIV pages */
+	vdso_size = (vdso_total_pages - 2) << PAGE_SHIFT;
 
 	if (vdso_size != new_size)
 		return -EINVAL;
@@ -67,122 +68,6 @@ static struct vm_special_mapping vdso_text_mapping __ro_after_init = {
 	.mremap = vdso_mremap,
 };
 
-struct elfinfo {
-	Elf32_Ehdr	*hdr;		/* ptr to ELF */
-	Elf32_Sym	*dynsym;	/* ptr to .dynsym section */
-	unsigned long	dynsymsize;	/* size of .dynsym section */
-	char		*dynstr;	/* ptr to .dynstr section */
-};
-
-/* Cached result of boot-time check for whether the arch timer exists,
- * and if so, whether the virtual counter is useable.
- */
-static bool cntvct_ok __ro_after_init;
-
-static bool __init cntvct_functional(void)
-{
-	struct device_node *np;
-	bool ret = false;
-
-	if (!IS_ENABLED(CONFIG_ARM_ARCH_TIMER))
-		goto out;
-
-	/* The arm_arch_timer core should export
-	 * arch_timer_use_virtual or similar so we don't have to do
-	 * this.
-	 */
-	np = of_find_compatible_node(NULL, NULL, "arm,armv7-timer");
-	if (!np)
-		goto out_put;
-
-	if (of_property_read_bool(np, "arm,cpu-registers-not-fw-configured"))
-		goto out_put;
-
-	ret = true;
-
-out_put:
-	of_node_put(np);
-out:
-	return ret;
-}
-
-static void * __init find_section(Elf32_Ehdr *ehdr, const char *name,
-				  unsigned long *size)
-{
-	Elf32_Shdr *sechdrs;
-	unsigned int i;
-	char *secnames;
-
-	/* Grab section headers and strings so we can tell who is who */
-	sechdrs = (void *)ehdr + ehdr->e_shoff;
-	secnames = (void *)ehdr + sechdrs[ehdr->e_shstrndx].sh_offset;
-
-	/* Find the section they want */
-	for (i = 1; i < ehdr->e_shnum; i++) {
-		if (strcmp(secnames + sechdrs[i].sh_name, name) == 0) {
-			if (size)
-				*size = sechdrs[i].sh_size;
-			return (void *)ehdr + sechdrs[i].sh_offset;
-		}
-	}
-
-	if (size)
-		*size = 0;
-	return NULL;
-}
-
-static Elf32_Sym * __init find_symbol(struct elfinfo *lib, const char *symname)
-{
-	unsigned int i;
-
-	for (i = 0; i < (lib->dynsymsize / sizeof(Elf32_Sym)); i++) {
-		char name[MAX_SYMNAME], *c;
-
-		if (lib->dynsym[i].st_name == 0)
-			continue;
-		strlcpy(name, lib->dynstr + lib->dynsym[i].st_name,
-			MAX_SYMNAME);
-		c = strchr(name, '@');
-		if (c)
-			*c = 0;
-		if (strcmp(symname, name) == 0)
-			return &lib->dynsym[i];
-	}
-	return NULL;
-}
-
-static void __init vdso_nullpatch_one(struct elfinfo *lib, const char *symname)
-{
-	Elf32_Sym *sym;
-
-	sym = find_symbol(lib, symname);
-	if (!sym)
-		return;
-
-	sym->st_name = 0;
-}
-
-static void __init patch_vdso(void *ehdr)
-{
-	struct elfinfo einfo;
-
-	einfo = (struct elfinfo) {
-		.hdr = ehdr,
-	};
-
-	einfo.dynsym = find_section(einfo.hdr, ".dynsym", &einfo.dynsymsize);
-	einfo.dynstr = find_section(einfo.hdr, ".dynstr", NULL);
-
-	/* If the virtual counter is absent or non-functional we don't
-	 * want programs to incur the slight additional overhead of
-	 * dispatching through the VDSO only to fall back to syscalls.
-	 */
-	if (!cntvct_ok) {
-		vdso_nullpatch_one(&einfo, "__vdso_gettimeofday");
-		vdso_nullpatch_one(&einfo, "__vdso_clock_gettime");
-	}
-}
-
 static int __init vdso_init(void)
 {
 	unsigned int text_pages;
@@ -214,17 +99,22 @@ static int __init vdso_init(void)
 
 	vdso_text_mapping.pages = vdso_text_pagelist;
 
-	vdso_total_pages = 1; /* for the data/vvar page */
+	vdso_total_pages = 2; /* for the data/vvar and vpriv pages */
 	vdso_total_pages += text_pages;
 
-	cntvct_ok = cntvct_functional();
-
-	patch_vdso(vdso_start);
+	vdso_data->cs_type_and_seq = ARM_CLOCK_NONE << 16 | 1;
 
 	return 0;
 }
 arch_initcall(vdso_init);
 
+static int install_vpriv(struct mm_struct *mm, unsigned long addr)
+{
+	return mmap_region(NULL, addr, PAGE_SIZE,
+			  VM_READ | VM_WRITE | VM_MAYREAD | VM_MAYWRITE,
+			   0, NULL) != addr ? -EINVAL : 0;
+}
+
 static int install_vvar(struct mm_struct *mm, unsigned long addr)
 {
 	struct vm_area_struct *vma;
@@ -232,8 +122,13 @@ static int install_vvar(struct mm_struct *mm, unsigned long addr)
 	vma = _install_special_mapping(mm, addr, PAGE_SIZE,
 				       VM_READ | VM_MAYREAD,
 				       &vdso_data_mapping);
+	if (IS_ERR(vma))
+		return PTR_ERR(vma);
 
-	return PTR_ERR_OR_ZERO(vma);
+	if (cache_is_vivt())
+		vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+	return vma->vm_start != addr ? -EINVAL : 0;
 }
 
 /* assumes mmap_sem is write-locked */
@@ -247,18 +142,29 @@ void arm_install_vdso(struct mm_struct *mm, unsigned long addr)
 	if (vdso_text_pagelist == NULL)
 		return;
 
-	if (install_vvar(mm, addr))
+	if (install_vpriv(mm, addr)) {
+		pr_err("cannot map VPRIV at expected address!\n");
+		return;
+	}
+
+	/* Account for the private storage. */
+	addr += PAGE_SIZE;
+	if (install_vvar(mm, addr)) {
+		WARN(1, "cannot map VVAR at expected address!\n");
 		return;
+	}
 
-	/* Account for vvar page. */
+	/* Account for vvar and vpriv pages. */
 	addr += PAGE_SIZE;
-	len = (vdso_total_pages - 1) << PAGE_SHIFT;
+	len = (vdso_total_pages - 2) << PAGE_SHIFT;
 
 	vma = _install_special_mapping(mm, addr, len,
 		VM_READ | VM_EXEC | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC,
 		&vdso_text_mapping);
 
-	if (!IS_ERR(vma))
+	if (IS_ERR(vma) || vma->vm_start != addr)
+		WARN(1, "cannot map VDSO at expected address!\n");
+	else
 		mm->context.vdso = addr;
 }
 
@@ -274,15 +180,9 @@ static void vdso_write_end(struct vdso_data *vdata)
 	++vdso_data->seq_count;
 }
 
-static bool tk_is_cntvct(const struct timekeeper *tk)
+static const struct arch_clocksource_data *tk_get_cd(const struct timekeeper *tk)
 {
-	if (!IS_ENABLED(CONFIG_ARM_ARCH_TIMER))
-		return false;
-
-	if (!tk->tkr_mono.clock->archdata.vdso_direct)
-		return false;
-
-	return true;
+	return &tk->tkr_mono.clock->archdata;
 }
 
 /**
@@ -306,24 +206,39 @@ static bool tk_is_cntvct(const struct timekeeper *tk)
 void update_vsyscall(struct timekeeper *tk)
 {
 	struct timespec64 *wtm = &tk->wall_to_monotonic;
+	const struct arch_clocksource_data *cd = tk_get_cd(tk);
+	unsigned long flags;
 
-	if (!cntvct_ok) {
-		/* The entry points have been zeroed, so there is no
-		 * point in updating the data page.
-		 */
-		return;
-	}
+	flags = hard_cond_local_irq_save();
 
 	vdso_write_begin(vdso_data);
 
-	vdso_data->tk_is_cntvct			= tk_is_cntvct(tk);
+	if (cd->clock_type != (vdso_data->cs_type_and_seq >> 16)) {
+		u32 type = cd->clock_type;
+		u16 seq = vdso_data->cs_type_and_seq;
+
+		if (++seq == 0)
+			seq = 1;
+		vdso_data->cs_type_and_seq	= type << 16 | seq;
+
+		/*
+		 * vdso does not have printf, so, prepare the device name for
+		 * it.
+		 */
+		if (cd->clock_type >= ARM_CLOCK_USER_MMIO_BASE)
+			snprintf(vdso_data->mmio_dev_name,
+				sizeof(vdso_data->mmio_dev_name),
+				"/dev/ucs/%u",
+				cd->clock_type - ARM_CLOCK_USER_MMIO_BASE);
+	}
+
 	vdso_data->xtime_coarse_sec		= tk->xtime_sec;
 	vdso_data->xtime_coarse_nsec		= (u32)(tk->tkr_mono.xtime_nsec >>
 							tk->tkr_mono.shift);
 	vdso_data->wtm_clock_sec		= wtm->tv_sec;
 	vdso_data->wtm_clock_nsec		= wtm->tv_nsec;
 
-	if (vdso_data->tk_is_cntvct) {
+	if (cd->clock_type != ARM_CLOCK_NONE) {
 		vdso_data->cs_cycle_last	= tk->tkr_mono.cycle_last;
 		vdso_data->xtime_clock_sec	= tk->xtime_sec;
 		vdso_data->xtime_clock_snsec	= tk->tkr_mono.xtime_nsec;
@@ -334,6 +249,8 @@ void update_vsyscall(struct timekeeper *tk)
 
 	vdso_write_end(vdso_data);
 
+	hard_cond_local_irq_restore(flags);
+
 	flush_dcache_page(virt_to_page(vdso_data));
 }
 
@@ -343,3 +260,20 @@ void update_vsyscall_tz(void)
 	vdso_data->tz_dsttime		= sys_tz.tz_dsttime;
 	flush_dcache_page(virt_to_page(vdso_data));
 }
+
+void arch_clocksource_user_mmio_init(struct clocksource *cs, unsigned id)
+{
+	struct arch_clocksource_data *d = &cs->archdata;
+
+	d->clock_type = ARM_CLOCK_USER_MMIO_BASE + id;
+}
+
+void arch_clocksource_arch_timer_init(struct clocksource *cs)
+{
+	struct arch_clocksource_data *d = &cs->archdata;
+
+	if (!cs->archdata.vdso_direct)
+		d->clock_type = ARM_CLOCK_NONE;
+	else
+		d->clock_type = ARM_CLOCK_ARCH_TIMER;
+}
diff --git a/arch/arm/mach-imx/gpc.c b/arch/arm/mach-imx/gpc.c
index b5b557fe2c49..d31631ef6728 100644
--- a/arch/arm/mach-imx/gpc.c
+++ b/arch/arm/mach-imx/gpc.c
@@ -62,28 +62,38 @@ void imx_gpc_set_l2_mem_power_in_lpm(bool power_off)
 void imx_gpc_pre_suspend(bool arm_power_off)
 {
 	void __iomem *reg_imr1 = gpc_base + GPC_IMR1;
+	unsigned long flags;
 	int i;
 
 	/* Tell GPC to power off ARM core when suspend */
 	if (arm_power_off)
 		imx_gpc_set_arm_power_in_lpm(arm_power_off);
 
+	flags = hard_cond_local_irq_save();
+
 	for (i = 0; i < IMR_NUM; i++) {
 		gpc_saved_imrs[i] = readl_relaxed(reg_imr1 + i * 4);
 		writel_relaxed(~gpc_wake_irqs[i], reg_imr1 + i * 4);
 	}
+
+	hard_cond_local_irq_restore(flags);
 }
 
 void imx_gpc_post_resume(void)
 {
 	void __iomem *reg_imr1 = gpc_base + GPC_IMR1;
+	unsigned long flags;
 	int i;
 
 	/* Keep ARM core powered on for other low-power modes */
 	imx_gpc_set_arm_power_in_lpm(false);
 
+	flags = hard_cond_local_irq_save();
+
 	for (i = 0; i < IMR_NUM; i++)
 		writel_relaxed(gpc_saved_imrs[i], reg_imr1 + i * 4);
+
+	hard_cond_local_irq_restore(flags);
 }
 
 static int imx_gpc_irq_set_wake(struct irq_data *d, unsigned int on)
@@ -105,22 +115,31 @@ static int imx_gpc_irq_set_wake(struct irq_data *d, unsigned int on)
 void imx_gpc_mask_all(void)
 {
 	void __iomem *reg_imr1 = gpc_base + GPC_IMR1;
+	unsigned long flags;
 	int i;
 
+	flags = hard_cond_local_irq_save();
+
 	for (i = 0; i < IMR_NUM; i++) {
 		gpc_saved_imrs[i] = readl_relaxed(reg_imr1 + i * 4);
 		writel_relaxed(~0, reg_imr1 + i * 4);
 	}
 
+	hard_cond_local_irq_restore(flags);
 }
 
 void imx_gpc_restore_all(void)
 {
 	void __iomem *reg_imr1 = gpc_base + GPC_IMR1;
+	unsigned long flags;
 	int i;
 
+	flags = hard_cond_local_irq_save();
+
 	for (i = 0; i < IMR_NUM; i++)
 		writel_relaxed(gpc_saved_imrs[i], reg_imr1 + i * 4);
+
+	hard_cond_local_irq_restore(flags);
 }
 
 void imx_gpc_hwirq_unmask(unsigned int hwirq)
@@ -168,6 +187,7 @@ static struct irq_chip imx_gpc_chip = {
 #ifdef CONFIG_SMP
 	.irq_set_affinity	= irq_chip_set_affinity_parent,
 #endif
+	.flags			= IRQCHIP_PIPELINE_SAFE,
 };
 
 static int imx_gpc_domain_translate(struct irq_domain *d,
diff --git a/arch/arm/mach-omap2/Kconfig b/arch/arm/mach-omap2/Kconfig
index fdb6743760a2..cf6ad49657dd 100644
--- a/arch/arm/mach-omap2/Kconfig
+++ b/arch/arm/mach-omap2/Kconfig
@@ -97,6 +97,7 @@ config ARCH_OMAP2PLUS
 	select ARCH_HAS_HOLES_MEMORYMODEL
 	select ARCH_OMAP
 	select CLKSRC_MMIO
+	select CLKSRC_VDSO_MAPPED
 	select GENERIC_IRQ_CHIP
 	select GPIOLIB
 	select MACH_OMAP_GENERIC
diff --git a/arch/arm/mach-omap2/timer.c b/arch/arm/mach-omap2/timer.c
index 07bea84c5d6e..bc7d177759c3 100644
--- a/arch/arm/mach-omap2/timer.c
+++ b/arch/arm/mach-omap2/timer.c
@@ -87,7 +87,7 @@ static irqreturn_t omap2_gp_timer_interrupt(int irq, void *dev_id)
 
 	__omap_dm_timer_write_status(&clkev, OMAP_TIMER_INT_OVERFLOW);
 
-	evt->event_handler(evt);
+	clockevents_handle_event(evt);
 	return IRQ_HANDLED;
 }
 
@@ -148,7 +148,8 @@ static void omap_clkevt_unidle(struct clock_event_device *unused)
 
 static struct clock_event_device clockevent_gpt = {
 	.features		= CLOCK_EVT_FEAT_PERIODIC |
-				  CLOCK_EVT_FEAT_ONESHOT,
+				  CLOCK_EVT_FEAT_ONESHOT |
+				  CLOCK_EVT_FEAT_PIPELINE,
 	.rating			= 300,
 	.set_next_event		= omap2_gp_timer_set_next_event,
 	.set_state_shutdown	= omap2_gp_timer_shutdown,
@@ -412,17 +413,14 @@ static bool use_gptimer_clksrc __initdata;
 /*
  * clocksource
  */
-static u64 clocksource_read_cycles(struct clocksource *cs)
-{
-	return (u64)__omap_dm_timer_read_counter(&clksrc,
-						     OMAP_TIMER_NONPOSTED);
-}
 
-static struct clocksource clocksource_gpt = {
-	.rating		= 300,
-	.read		= clocksource_read_cycles,
-	.mask		= CLOCKSOURCE_MASK(32),
-	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
+static struct clocksource_user_mmio clocksource_gpt = {
+	.mmio.clksrc = {
+		.rating		= 300,
+		.read		= clocksource_mmio_readl_up,
+		.mask		= CLOCKSOURCE_MASK(32),
+		.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
+	},
 };
 
 static u64 notrace dmtimer_read_sched_clock(void)
@@ -504,21 +502,22 @@ static void __init omap2_gptimer_clocksource_init(int gptimer_id,
 						  const char *fck_source,
 						  const char *property)
 {
+	struct clocksource_mmio_regs mmr;
 	int res;
 
 	clksrc.id = gptimer_id;
 	clksrc.errata = omap_dm_timer_get_errata();
 
 	res = omap_dm_timer_init_one(&clksrc, fck_source, property,
-				     &clocksource_gpt.name,
+				     &clocksource_gpt.mmio.clksrc.name,
 				     OMAP_TIMER_NONPOSTED);
 
 	if (soc_is_am43xx()) {
-		clocksource_gpt.suspend = omap2_gptimer_clksrc_suspend;
-		clocksource_gpt.resume = omap2_gptimer_clksrc_resume;
+		clocksource_gpt.mmio.clksrc.suspend = omap2_gptimer_clksrc_suspend;
+		clocksource_gpt.mmio.clksrc.resume = omap2_gptimer_clksrc_resume;
 
 		clocksource_gpt_hwmod =
-			omap_hwmod_lookup(clocksource_gpt.name);
+			omap_hwmod_lookup(clocksource_gpt.mmio.clksrc.name);
 	}
 
 	BUG_ON(res);
@@ -528,12 +527,18 @@ static void __init omap2_gptimer_clocksource_init(int gptimer_id,
 				   OMAP_TIMER_NONPOSTED);
 	sched_clock_register(dmtimer_read_sched_clock, 32, clksrc.rate);
 
-	if (clocksource_register_hz(&clocksource_gpt, clksrc.rate))
+	mmr.reg_lower = clksrc.func_base + (OMAP_TIMER_COUNTER_REG & 0xff);
+	mmr.bits_lower = 32;
+	mmr.reg_upper = 0;
+	mmr.bits_upper = 0;
+	mmr.revmap = NULL;
+
+	if (clocksource_user_mmio_init(&clocksource_gpt, &mmr, clksrc.rate))
 		pr_err("Could not register clocksource %s\n",
-			clocksource_gpt.name);
+			clocksource_gpt.mmio.clksrc.name);
 	else
 		pr_info("OMAP clocksource: %s at %lu Hz\n",
-			clocksource_gpt.name, clksrc.rate);
+			clocksource_gpt.mmio.clksrc.name, clksrc.rate);
 }
 
 static void __init __omap_sync32k_timer_init(int clkev_nr, const char *clkev_src,
diff --git a/arch/arm/mm/Kconfig b/arch/arm/mm/Kconfig
index 0ab3a86b1f52..7676a7db9230 100644
--- a/arch/arm/mm/Kconfig
+++ b/arch/arm/mm/Kconfig
@@ -894,7 +894,7 @@ config KUSER_HELPERS
 
 config VDSO
 	bool "Enable VDSO for acceleration of some system calls"
-	depends on AEABI && MMU && CPU_V7
+	depends on AEABI && MMU
 	default y if ARM_ARCH_TIMER
 	select GENERIC_TIME_VSYSCALL
 	help
diff --git a/arch/arm/mm/alignment.c b/arch/arm/mm/alignment.c
index 788c5cf46de5..760d021fcf72 100644
--- a/arch/arm/mm/alignment.c
+++ b/arch/arm/mm/alignment.c
@@ -19,6 +19,7 @@
 #include <linux/init.h>
 #include <linux/sched/signal.h>
 #include <linux/uaccess.h>
+#include <linux/dovetail.h>
 
 #include <asm/cp15.h>
 #include <asm/system_info.h>
@@ -810,7 +811,9 @@ do_alignment(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 	int fault;
 
 	if (interrupts_enabled(regs))
-		local_irq_enable();
+		hard_local_irq_enable();
+
+	oob_trap_notify(ARM_TRAP_ALIGNMENT, regs);
 
 	instrptr = instruction_pointer(regs);
 
@@ -989,7 +992,7 @@ do_alignment(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 		 * entry-common.S) and disable the alignment trap only if
 		 * there is no work pending for this thread.
 		 */
-		raw_local_irq_disable();
+		hard_local_irq_disable();
 		if (!(current_thread_info()->flags & _TIF_WORK_MASK))
 			set_cr(cr_no_alignment);
 	}
diff --git a/arch/arm/mm/cache-l2x0.c b/arch/arm/mm/cache-l2x0.c
index 12c26eb88afb..6a4ce04b2a00 100644
--- a/arch/arm/mm/cache-l2x0.c
+++ b/arch/arm/mm/cache-l2x0.c
@@ -38,7 +38,7 @@ struct l2c_init_data {
 
 static void __iomem *l2x0_base;
 static const struct l2c_init_data *l2x0_data;
-static DEFINE_RAW_SPINLOCK(l2x0_lock);
+static DEFINE_HARD_SPINLOCK(l2x0_lock);
 static u32 l2x0_way_mask;	/* Bitmask of active ways */
 static u32 l2x0_size;
 static unsigned long sync_reg_offset = L2X0_CACHE_SYNC;
@@ -48,6 +48,19 @@ struct l2x0_regs l2x0_saved_regs;
 static bool l2x0_bresp_disable;
 static bool l2x0_flz_disable;
 
+#ifdef CONFIG_IRQ_PIPELINE
+#define CACHE_RANGE_ATOMIC_MAX	512UL
+static int l2x0_wa = -1;
+static int __init l2x0_setup_wa(char *str)
+{
+	l2x0_wa = !!simple_strtol(str, NULL, 0);
+	return 0;
+}
+early_param("l2x0_write_allocate", l2x0_setup_wa);
+#else
+#define CACHE_RANGE_ATOMIC_MAX	4096UL
+#endif
+
 /*
  * Common code for all cache controllers.
  */
@@ -284,10 +297,10 @@ static void l2c220_op_way(void __iomem *base, unsigned reg)
 static unsigned long l2c220_op_pa_range(void __iomem *reg, unsigned long start,
 	unsigned long end, unsigned long flags)
 {
-	raw_spinlock_t *lock = &l2x0_lock;
+	typeof(l2x0_lock) *lock = &l2x0_lock;
 
 	while (start < end) {
-		unsigned long blk_end = start + min(end - start, 4096UL);
+		unsigned long blk_end = start + min(end - start, CACHE_RANGE_ATOMIC_MAX);
 
 		while (start < blk_end) {
 			l2c_wait_mask(reg, 1);
@@ -498,13 +511,13 @@ static void l2c310_inv_range_erratum(unsigned long start, unsigned long end)
 
 static void l2c310_flush_range_erratum(unsigned long start, unsigned long end)
 {
-	raw_spinlock_t *lock = &l2x0_lock;
+	typeof(l2x0_lock) *lock = &l2x0_lock;
 	unsigned long flags;
 	void __iomem *base = l2x0_base;
 
 	raw_spin_lock_irqsave(lock, flags);
 	while (start < end) {
-		unsigned long blk_end = start + min(end - start, 4096UL);
+		unsigned long blk_end = start + min(end - start, CACHE_RANGE_ATOMIC_MAX);
 
 		l2c_set_debug(base, 0x03);
 		while (start < blk_end) {
@@ -800,6 +813,24 @@ static int __init __l2c_init(const struct l2c_init_data *data,
 	if (aux_val & aux_mask)
 		pr_alert("L2C: platform provided aux values permit register corruption.\n");
 
+#ifdef CONFIG_IRQ_PIPELINE
+	if (!l2x0_wa) {
+		/*
+		 * Disable WA by setting bit 23 in the auxiliary
+		 * control register.
+		 */
+		aux_mask &= ~L220_AUX_CTRL_FWA_MASK;
+		aux_val &= ~L220_AUX_CTRL_FWA_MASK;
+		aux_val |= 1 << L220_AUX_CTRL_FWA_SHIFT;
+		pr_warn("%s: irq_pipeline: write-allocate disabled via command line\n",
+			data->type);
+	} else if ((cache_id & L2X0_CACHE_ID_PART_MASK) == L2X0_CACHE_ID_PART_L220 ||
+		   ((cache_id & L2X0_CACHE_ID_PART_MASK) == L2X0_CACHE_ID_PART_L310 &&
+		    (cache_id & L2X0_CACHE_ID_RTL_MASK) < L310_CACHE_ID_RTL_R3P2))
+		pr_alert("%s: irq_pipeline: write-allocate enabled, may induce high latency\n",
+			 data->type);
+#endif
+
 	old_aux = aux = readl_relaxed(l2x0_base + L2X0_AUX_CTRL);
 	aux &= aux_mask;
 	aux |= aux_val;
diff --git a/arch/arm/mm/context.c b/arch/arm/mm/context.c
index b7525b433f3e..0cf14bd0c265 100644
--- a/arch/arm/mm/context.c
+++ b/arch/arm/mm/context.c
@@ -39,7 +39,7 @@
 #define ASID_FIRST_VERSION	(1ULL << ASID_BITS)
 #define NUM_USER_ASIDS		ASID_FIRST_VERSION
 
-static DEFINE_RAW_SPINLOCK(cpu_asid_lock);
+static DEFINE_HARD_SPINLOCK(cpu_asid_lock);
 static atomic64_t asid_generation = ATOMIC64_INIT(ASID_FIRST_VERSION);
 static DECLARE_BITMAP(asid_map, NUM_USER_ASIDS);
 
@@ -237,9 +237,12 @@ static u64 new_context(struct mm_struct *mm, unsigned int cpu)
 void check_and_switch_context(struct mm_struct *mm, struct task_struct *tsk)
 {
 	unsigned long flags;
-	unsigned int cpu = smp_processor_id();
+	unsigned int cpu = raw_smp_processor_id();
+	bool need_flush;
 	u64 asid;
 
+	WARN_ON_ONCE(dovetail_debug() && !hard_irqs_disabled());
+
 	if (unlikely(mm->context.vmalloc_seq != init_mm.context.vmalloc_seq))
 		__check_vmalloc_seq(mm);
 
@@ -263,15 +266,16 @@ void check_and_switch_context(struct mm_struct *mm, struct task_struct *tsk)
 		atomic64_set(&mm->context.id, asid);
 	}
 
-	if (cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending)) {
-		local_flush_bp_all();
-		local_flush_tlb_all();
-	}
-
+	need_flush = cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending);
 	atomic64_set(&per_cpu(active_asids, cpu), asid);
 	cpumask_set_cpu(cpu, mm_cpumask(mm));
 	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);
 
+	if (need_flush) {
+		local_flush_bp_all();
+		local_flush_tlb_all();
+	}
+
 switch_mm_fastpath:
 	cpu_switch_mm(mm->pgd, mm);
 }
diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
index bd0f4821f7e1..27fc2ad3d65b 100644
--- a/arch/arm/mm/fault.c
+++ b/arch/arm/mm/fault.c
@@ -9,6 +9,7 @@
 #include <linux/signal.h>
 #include <linux/mm.h>
 #include <linux/hardirq.h>
+#include <linux/irq_pipeline.h>
 #include <linux/init.h>
 #include <linux/kprobes.h>
 #include <linux/uaccess.h>
@@ -22,11 +23,98 @@
 #include <asm/system_misc.h>
 #include <asm/system_info.h>
 #include <asm/tlbflush.h>
+#include <asm/dovetail.h>
+#define CREATE_TRACE_POINTS
+#include <asm/trace/exceptions.h>
 
 #include "fault.h"
 
 #ifdef CONFIG_MMU
 
+#ifdef CONFIG_DOVETAIL
+#define fault_entry(__exception, __regs)	__fault_entry(__exception, __regs)
+#else
+#define fault_entry(__exception, __regs)	__fault_entry(-1, __regs)
+#endif
+
+#ifdef CONFIG_IRQ_PIPELINE
+/*
+ * We need to synchronize the virtual interrupt state with the hard
+ * interrupt state we received on entry, then turn hardirqs back on to
+ * allow code which does not require strict serialization to be
+ * preempted by an out-of-band activity.
+ *
+ * TRACING: the entry code already told lockdep and tracers about the
+ * hard interrupt state on entry to fault handlers, so no need to
+ * reflect changes to that state via calls to trace_hardirqs_*
+ * helpers. From the main kernel's point of view, there is no change.
+ */
+static inline
+unsigned long __fault_entry(int exception, struct pt_regs *regs)
+{
+	unsigned long flags;
+	int nosync = 1;
+
+	trace_ARM_trap_entry(exception, regs);
+
+	oob_trap_notify(exception, regs);
+
+	flags = hard_local_irq_save();
+
+	if (hard_irqs_disabled_flags(flags))
+		nosync = test_and_set_stage_bit(STAGE_STALL_BIT,
+					this_inband_staged());
+	hard_local_irq_enable();
+
+	return irqs_merge_flags(flags, nosync);
+}
+
+static inline void fault_exit(int exception, struct pt_regs *regs,
+			unsigned long combo)
+{
+	unsigned long flags;
+	int nosync;
+
+	WARN_ON_ONCE(irq_pipeline_debug() && hard_irqs_disabled());
+
+	/*
+	 * '!nosync' here means that we had to turn on the stall bit
+	 * in fault_entry() to mirror the hard interrupt state,
+	 * because hard irqs were off but the stall bit was
+	 * clear. Conversely, nosync in fault_exit() means that the
+	 * stall bit state currently reflects the hard interrupt state
+	 * we received on fault_entry().
+	 *
+	 * No hard_local_irq_restore() below, ever, but
+	 * hard_local_irq_{enable|disable}() exclusively. See
+	 * restore_stage() for an explanation.
+	 */
+	flags = irqs_split_flags(combo, &nosync);
+	if (!nosync) {
+		hard_local_irq_disable();
+		clear_stage_bit(STAGE_STALL_BIT, this_inband_staged());
+		if (!hard_irqs_disabled_flags(flags))
+			hard_local_irq_enable();
+	} else if (hard_irqs_disabled_flags(flags))
+		hard_local_irq_disable();
+
+	trace_ARM_trap_exit(exception, regs);
+}
+
+#else	/* !CONFIG_IRQ_PIPELINE */
+
+static inline
+unsigned long __fault_entry(int exception, struct pt_regs *regs)
+{
+	return 0;
+}
+
+static inline void fault_exit(int exception, struct pt_regs *regs,
+			unsigned long combo)
+{ }
+
+#endif	/* !CONFIG_IRQ_PIPELINE */
+
 /*
  * This is useful to dump out the page tables associated with
  * 'addr' in mm 'mm'.
@@ -95,6 +183,15 @@ void show_pte(const char *lvl, struct mm_struct *mm, unsigned long addr)
 	pr_cont("\n");
 }
 #else					/* CONFIG_MMU */
+unsigned long fault_entry(int exception, struct pt_regs *regs)
+{
+	return 0;
+}
+
+static inline void fault_exit(int exception, struct pt_regs *regs,
+			unsigned long combo)
+{ }
+
 void show_pte(const char *lvl, struct mm_struct *mm, unsigned long addr)
 { }
 #endif					/* CONFIG_MMU */
@@ -115,6 +212,7 @@ __do_kernel_fault(struct mm_struct *mm, unsigned long addr, unsigned int fsr,
 	/*
 	 * No handler, we'll have to terminate things with extreme prejudice.
 	 */
+	irq_pipeline_oops();
 	bust_spinlocks(1);
 	pr_alert("8<--- cut here ---\n");
 	pr_alert("Unable to handle kernel %s at virtual address %08lx\n",
@@ -167,14 +265,22 @@ void do_bad_area(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 {
 	struct task_struct *tsk = current;
 	struct mm_struct *mm = tsk->active_mm;
+	unsigned long irqflags;
 
 	/*
 	 * If we are in kernel mode at this point, we
 	 * have no context to handle this fault with.
 	 */
-	if (user_mode(regs))
+	  if (user_mode(regs)) {
+		irqflags = fault_entry(ARM_TRAP_ACCESS, regs);
 		__do_user_fault(addr, fsr, SIGSEGV, SEGV_MAPERR, regs);
-	else
+		fault_exit(ARM_TRAP_ACCESS, regs, irqflags);
+	  } else
+		/*
+		 * irq_pipeline: kernel faults are either quickly
+		 * recoverable via fixup, or lethal. In both cases, we
+		 * can skip the interrupt state synchronization.
+		 */
 		__do_kernel_fault(mm, addr, fsr, regs);
 }
 
@@ -242,9 +348,12 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 	int sig, code;
 	vm_fault_t fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
+	unsigned long irqflags;
+
+	irqflags = fault_entry(ARM_TRAP_ACCESS, regs);
 
 	if (kprobe_page_fault(regs, fsr))
-		return 0;
+		goto out;
 
 	tsk = current;
 	mm  = tsk->mm;
@@ -298,7 +407,7 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current)) {
 		if (!user_mode(regs))
 			goto no_context;
-		return 0;
+		goto out;
 	}
 
 	/*
@@ -333,7 +442,7 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 	 * Handle the "normal" case first - VM_FAULT_MAJOR
 	 */
 	if (likely(!(fault & (VM_FAULT_ERROR | VM_FAULT_BADMAP | VM_FAULT_BADACCESS))))
-		return 0;
+		goto out;
 
 	/*
 	 * If we are in kernel mode at this point, we
@@ -349,7 +458,7 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 		 * got oom-killed)
 		 */
 		pagefault_out_of_memory();
-		return 0;
+		goto out;
 	}
 
 	if (fault & VM_FAULT_SIGBUS) {
@@ -370,10 +479,13 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 	}
 
 	__do_user_fault(addr, fsr, sig, code, regs);
-	return 0;
+	goto out;
 
 no_context:
 	__do_kernel_fault(mm, addr, fsr, regs);
+out:
+	fault_exit(ARM_TRAP_ACCESS, regs, irqflags);
+
 	return 0;
 }
 #else					/* CONFIG_MMU */
@@ -411,6 +523,8 @@ do_translation_fault(unsigned long addr, unsigned int fsr,
 	pud_t *pud, *pud_k;
 	pmd_t *pmd, *pmd_k;
 
+	WARN_ON_ONCE(irqs_pipelined() && !hard_irqs_disabled());
+
 	if (addr < TASK_SIZE)
 		return do_page_fault(addr, fsr, regs);
 
@@ -481,7 +595,11 @@ do_translation_fault(unsigned long addr, unsigned int fsr,
 static int
 do_sect_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 {
+	unsigned long irqflags;
+
+	irqflags = fault_entry(ARM_TRAP_SECTION, regs);
 	do_bad_area(addr, fsr, regs);
+	fault_exit(ARM_TRAP_SECTION, regs, irqflags);
 	return 0;
 }
 #endif /* CONFIG_ARM_LPAE */
@@ -529,10 +647,12 @@ asmlinkage void
 do_DataAbort(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 {
 	const struct fsr_info *inf = fsr_info + fsr_fs(fsr);
+	unsigned long irqflags;
 
 	if (!inf->fn(addr, fsr & ~FSR_LNX_PF, regs))
 		return;
 
+	irqflags = fault_entry(ARM_TRAP_DABT, regs);
 	pr_alert("8<--- cut here ---\n");
 	pr_alert("Unhandled fault: %s (0x%03x) at 0x%08lx\n",
 		inf->name, fsr, addr);
@@ -540,6 +660,7 @@ do_DataAbort(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 
 	arm_notify_die("", regs, inf->sig, inf->code, (void __user *)addr,
 		       fsr, 0);
+	fault_exit(ARM_TRAP_DABT, regs, irqflags);
 }
 
 void __init
@@ -559,15 +680,18 @@ asmlinkage void
 do_PrefetchAbort(unsigned long addr, unsigned int ifsr, struct pt_regs *regs)
 {
 	const struct fsr_info *inf = ifsr_info + fsr_fs(ifsr);
+	unsigned long irqflags;
 
 	if (!inf->fn(addr, ifsr | FSR_LNX_PF, regs))
 		return;
 
+	irqflags = fault_entry(ARM_TRAP_PABT, regs);
 	pr_alert("Unhandled prefetch abort: %s (0x%03x) at 0x%08lx\n",
 		inf->name, ifsr, addr);
 
 	arm_notify_die("", regs, inf->sig, inf->code, (void __user *)addr,
 		       ifsr, 0);
+	fault_exit(ARM_TRAP_PABT, regs, irqflags);
 }
 
 /*
diff --git a/arch/arm/vdso/datapage.S b/arch/arm/vdso/datapage.S
index 9cd73b725d9f..9beb76db2ec3 100644
--- a/arch/arm/vdso/datapage.S
+++ b/arch/arm/vdso/datapage.S
@@ -5,6 +5,8 @@
 	.align 2
 .L_vdso_data_ptr:
 	.long	_start - . - VDSO_DATA_SIZE
+.L_vdso_priv_ptr:
+	.long	_start - . - VDSO_DATA_SIZE - VDSO_PRIV_SIZE
 
 ENTRY(__get_datapage)
 	.fnstart
@@ -14,3 +16,12 @@ ENTRY(__get_datapage)
 	bx	lr
 	.fnend
 ENDPROC(__get_datapage)
+
+ENTRY(__get_privpage)
+	.fnstart
+	adr	r0, .L_vdso_priv_ptr
+	ldr	r1, [r0]
+	add	r0, r0, r1
+	bx	lr
+	.fnend
+ENDPROC(__get_privpage)
diff --git a/arch/arm/vdso/vgettimeofday.c b/arch/arm/vdso/vgettimeofday.c
index d1fdbb12760a..61f0fbc09eb6 100644
--- a/arch/arm/vdso/vgettimeofday.c
+++ b/arch/arm/vdso/vgettimeofday.c
@@ -6,6 +6,13 @@
 #include <linux/compiler.h>
 #include <linux/hrtimer.h>
 #include <linux/time.h>
+#include <linux/io.h>
+#include <linux/fcntl.h>
+#include <linux/err.h>
+#include <linux/mman.h>
+#include <linux/compiler.h>
+#include <linux/ioctl.h>
+#include <linux/clocksource.h>
 #include <asm/barrier.h>
 #include <asm/bug.h>
 #include <asm/cp15.h>
@@ -19,6 +26,182 @@
 
 extern struct vdso_data *__get_datapage(void);
 
+struct clksrc_info;
+
+typedef u64 vdso_read_cycles_fn(const struct clksrc_info *info);
+
+struct clksrc_info {
+	vdso_read_cycles_fn *read_cycles;
+	struct clksrc_user_mmio_info mmio;
+};
+
+struct vdso_priv {
+	u32 current_cs_type_and_seq;
+	struct clksrc_info clksrc_info[ARM_CLOCK_USER_MMIO_BASE + CLKSRC_USER_MMIO_MAX];
+};
+extern struct vdso_priv *__get_privpage(void);
+
+#define syscall3(nr, a0, a1, a2)			\
+	_syscall3((u32)a0, (u32)a1, (u32)a2, nr)
+
+#define syscall2(nr, a0, a1) \
+	_syscall2((u32)a0, (u32)a1, nr)
+
+#define syscall1(nr, a0) \
+	syscall2(nr, a0, 0)
+
+#define sys_open(filename, flags) \
+	syscall2(__NR_open, filename, flags)
+
+#define sys_ioctl(fd, cmd, ptr)			\
+	syscall3(__NR_ioctl, fd, cmd, ptr)
+
+#define sys_close(fd) \
+	syscall1(__NR_close, fd)
+
+#define sys_clock_gettime(id, ts) \
+	syscall2(__NR_clock_gettime, id, ts)
+
+#define sys_gettimeofday(tv, tz) \
+	syscall2(__NR_gettimeofday, tv, tz)
+
+static notrace u64 read_none(const struct clksrc_info *info)
+{
+	return 0;
+}
+
+static notrace u64 read_arch_timer(const struct clksrc_info *info)
+{
+#ifdef CONFIG_ARM_ARCH_TIMER
+	isb();
+	return read_sysreg(CNTVCT);
+#else
+	return 0;
+#endif
+}
+
+static notrace u64 readl_mmio_up(const struct clksrc_info *vinfo)
+{
+	const struct clksrc_user_mmio_info *info = &vinfo->mmio;
+	return readl_relaxed(info->reg_lower);
+}
+
+static notrace u64 readl_mmio_down(const struct clksrc_info *vinfo)
+{
+	const struct clksrc_user_mmio_info *info = &vinfo->mmio;
+	return ~(u64)readl_relaxed(info->reg_lower) & info->mask_lower;
+}
+
+static notrace u64 readw_mmio_up(const struct clksrc_info *vinfo)
+{
+	const struct clksrc_user_mmio_info *info = &vinfo->mmio;
+	return readw_relaxed(info->reg_lower);
+}
+
+static notrace u64 readw_mmio_down(const struct clksrc_info *vinfo)
+{
+	const struct clksrc_user_mmio_info *info = &vinfo->mmio;
+	return ~(u64)readl_relaxed(info->reg_lower) & info->mask_lower;
+}
+
+static notrace u64 readl_dmmio_up(const struct clksrc_info *vinfo)
+{
+	const struct clksrc_user_mmio_info *info = &vinfo->mmio;
+	void __iomem *reg_lower, *reg_upper;
+	u32 upper, old_upper, lower;
+
+	reg_lower = info->reg_lower;
+	reg_upper = info->reg_upper;
+
+	upper = readl_relaxed(reg_upper);
+	do {
+		old_upper = upper;
+		lower = readl_relaxed(reg_lower);
+		upper = readl_relaxed(reg_upper);
+	} while (upper != old_upper);
+
+	return (((u64)upper) << info->bits_lower) | lower;
+}
+
+static notrace u64 readw_dmmio_up(const struct clksrc_info *vinfo)
+{
+	const struct clksrc_user_mmio_info *info = &vinfo->mmio;
+	void __iomem *reg_lower, *reg_upper;
+	u16 upper, old_upper, lower;
+
+	reg_lower = info->reg_lower;
+	reg_upper = info->reg_upper;
+
+	upper = readw_relaxed(reg_upper);
+	do {
+		old_upper = upper;
+		lower = readw_relaxed(reg_lower);
+		upper = readw_relaxed(reg_upper);
+	} while (upper != old_upper);
+
+	return (((u64)upper) << info->bits_lower) | lower;
+}
+
+static inline notrace u16 to_type(u32 type_and_seq)
+{
+	return type_and_seq >> 16;
+}
+
+static inline notrace u16 to_seq(u32 type_and_seq)
+{
+	return type_and_seq;
+}
+
+static inline notrace u32 to_type_and_seq(u16 type, u16 seq)
+{
+	return (u32)type << 16U | seq;
+}
+
+static inline notrace bool clock_accessible(struct vdso_priv *vpriv)
+{
+	return to_type(vpriv->current_cs_type_and_seq) != ARM_CLOCK_NONE;
+}
+
+static notrace u64 read_cycles(struct vdso_priv *vpriv)
+{
+	const struct clksrc_info *info;
+	unsigned cs;
+
+	cs = to_type(READ_ONCE(vpriv->current_cs_type_and_seq));
+	info = &vpriv->clksrc_info[cs];
+	return info->read_cycles(info);
+}
+
+static notrace __cold vdso_read_cycles_fn *get_nommio_read_cycles(unsigned type)
+{
+	switch (type) {
+	case ARM_CLOCK_ARCH_TIMER:
+		return &read_arch_timer;
+	default:
+		return &read_none;
+	}
+}
+
+static notrace __cold vdso_read_cycles_fn *get_mmio_read_cycles(unsigned type)
+{
+	switch (type) {
+	case CLKSRC_MMIO_L_UP:
+		return &readl_mmio_up;
+	case CLKSRC_MMIO_L_DOWN:
+		return &readl_mmio_down;
+	case CLKSRC_MMIO_W_UP:
+		return &readw_mmio_up;
+	case CLKSRC_MMIO_W_DOWN:
+		return &readw_mmio_down;
+	case CLKSRC_DMMIO_L_UP:
+		return &readl_dmmio_up;
+	case CLKSRC_DMMIO_W_UP:
+		return &readw_dmmio_up;
+	default:
+		return &read_none;
+	}
+}
+
 static notrace u32 __vdso_read_begin(const struct vdso_data *vdata)
 {
 	u32 seq;
@@ -31,7 +214,7 @@ static notrace u32 __vdso_read_begin(const struct vdso_data *vdata)
 	return seq;
 }
 
-static notrace u32 vdso_read_begin(const struct vdso_data *vdata)
+static notrace u32 _vdso_read_begin(const struct vdso_data *vdata)
 {
 	u32 seq;
 
@@ -47,30 +230,119 @@ static notrace int vdso_read_retry(const struct vdso_data *vdata, u32 start)
 	return vdata->seq_count != start;
 }
 
-static notrace long clock_gettime_fallback(clockid_t _clkid,
-					   struct timespec *_ts)
+static notrace long _syscall3(u32 a0, u32 a1, u32 a2, u32 nr)
 {
-	register struct timespec *ts asm("r1") = _ts;
-	register clockid_t clkid asm("r0") = _clkid;
+	register u32 r0 asm("r0") = a0;
+	register u32 r1 asm("r1") = a1;
+	register u32 r2 asm("r2") = a2;
 	register long ret asm ("r0");
-	register long nr asm("r7") = __NR_clock_gettime;
+	register long _nr asm("r7") = nr;
 
 	asm volatile(
 	"	swi #0\n"
 	: "=r" (ret)
-	: "r" (clkid), "r" (ts), "r" (nr)
+	: "r"(r0), "r"(r1), "r"(r2), "r"(_nr)
 	: "memory");
 
 	return ret;
 }
 
+static notrace long _syscall2(u32 a0, u32 a1, u32 nr)
+{
+	register u32 r0 asm("r0") = a0;
+	register u32 r1 asm("r1") = a1;
+	register long ret asm ("r0");
+	register long _nr asm("r7") = nr;
+
+	asm volatile(
+	"	swi #0\n"
+	: "=r" (ret)
+	: "r"(r0), "r"(r1), "r"(_nr)
+	: "memory");
+
+	return ret;
+}
+
+static notrace noinline __cold
+void vdso_map_clock(const struct vdso_data *vdata, struct vdso_priv *vpriv,
+		    u32 seq, u32 new_type_and_seq)
+{
+	vdso_read_cycles_fn *read_cycles;
+	u32 new_cs_seq, new_cs_type;
+	struct clksrc_info *info;
+	int fd, err;
+
+	new_cs_seq = to_seq(new_type_and_seq);
+	new_cs_type = to_type(new_type_and_seq);
+	info = &vpriv->clksrc_info[new_cs_type];
+
+	if (new_cs_type < ARM_CLOCK_USER_MMIO_BASE) {
+		read_cycles = get_nommio_read_cycles(new_cs_type);
+		goto done;
+	}
+
+	err = sys_open(vdata->mmio_dev_name, O_RDONLY);
+	if (err < 0)
+		goto fallback_to_syscall;
+	fd = err;
+
+	if (vdso_read_retry(vdata, seq)) {
+		_vdso_read_begin(vdata);
+		if (to_seq(vdata->cs_type_and_seq) != new_cs_seq) {
+			/*
+			 * mmio_dev_name no longer corresponds to
+			 * vdata->cs_type_and_seq
+			 */
+			sys_close(fd);
+			return;
+		}
+	}
+
+	err = sys_ioctl(fd, CLKSRC_USER_MMIO_MAP, &info->mmio);
+	sys_close(fd);
+	if (err < 0)
+		goto fallback_to_syscall;
+
+	read_cycles = get_mmio_read_cycles(info->mmio.type);
+  done:
+	info->read_cycles = read_cycles;
+	smp_wmb();
+	new_type_and_seq = to_type_and_seq(new_cs_type, new_cs_seq);
+	WRITE_ONCE(vpriv->current_cs_type_and_seq, new_type_and_seq);
+
+	return;
+
+  fallback_to_syscall:
+	new_cs_type = ARM_CLOCK_NONE;
+	info = &vpriv->clksrc_info[new_cs_type];
+	read_cycles = get_nommio_read_cycles(new_cs_type);
+	goto done;
+}
+
+static notrace u32 vdso_read_begin(const struct vdso_data *vdata,
+				   struct vdso_priv *vpriv)
+{
+	u32 seq, cs_type_and_seq;
+
+	for (;;) {
+		seq = _vdso_read_begin(vdata);
+
+		cs_type_and_seq = READ_ONCE(vpriv->current_cs_type_and_seq);
+		if (likely(to_seq(cs_type_and_seq) == to_seq(vdata->cs_type_and_seq)))
+			return seq;
+
+		vdso_map_clock(vdata, vpriv, seq, vdata->cs_type_and_seq);
+	}
+}
+
 static notrace int do_realtime_coarse(struct timespec *ts,
-				      struct vdso_data *vdata)
+				      struct vdso_data *vdata,
+				      struct vdso_priv *vpriv)
 {
 	u32 seq;
 
 	do {
-		seq = vdso_read_begin(vdata);
+		seq = vdso_read_begin(vdata, vpriv);
 
 		ts->tv_sec = vdata->xtime_coarse_sec;
 		ts->tv_nsec = vdata->xtime_coarse_nsec;
@@ -81,13 +353,14 @@ static notrace int do_realtime_coarse(struct timespec *ts,
 }
 
 static notrace int do_monotonic_coarse(struct timespec *ts,
-				       struct vdso_data *vdata)
+				       struct vdso_data *vdata,
+				       struct vdso_priv *vpriv)
 {
 	struct timespec tomono;
 	u32 seq;
 
 	do {
-		seq = vdso_read_begin(vdata);
+		seq = vdso_read_begin(vdata, vpriv);
 
 		ts->tv_sec = vdata->xtime_coarse_sec;
 		ts->tv_nsec = vdata->xtime_coarse_nsec;
@@ -103,16 +376,13 @@ static notrace int do_monotonic_coarse(struct timespec *ts,
 	return 0;
 }
 
-#ifdef CONFIG_ARM_ARCH_TIMER
-
-static notrace u64 get_ns(struct vdso_data *vdata)
+static notrace u64 get_ns(struct vdso_data *vdata, struct vdso_priv *vpriv)
 {
 	u64 cycle_delta;
 	u64 cycle_now;
 	u64 nsec;
 
-	isb();
-	cycle_now = read_sysreg(CNTVCT);
+	cycle_now = read_cycles(vpriv);
 
 	cycle_delta = (cycle_now - vdata->cs_cycle_last) & vdata->cs_mask;
 
@@ -122,19 +392,21 @@ static notrace u64 get_ns(struct vdso_data *vdata)
 	return nsec;
 }
 
-static notrace int do_realtime(struct timespec *ts, struct vdso_data *vdata)
+static notrace int do_realtime(struct timespec *ts,
+			       struct vdso_data *vdata,
+			       struct vdso_priv *vpriv)
 {
 	u64 nsecs;
 	u32 seq;
 
 	do {
-		seq = vdso_read_begin(vdata);
+		seq = vdso_read_begin(vdata, vpriv);
 
-		if (!vdata->tk_is_cntvct)
+		if (!clock_accessible(vpriv))
 			return -1;
 
 		ts->tv_sec = vdata->xtime_clock_sec;
-		nsecs = get_ns(vdata);
+		nsecs = get_ns(vdata, vpriv);
 
 	} while (vdso_read_retry(vdata, seq));
 
@@ -144,20 +416,22 @@ static notrace int do_realtime(struct timespec *ts, struct vdso_data *vdata)
 	return 0;
 }
 
-static notrace int do_monotonic(struct timespec *ts, struct vdso_data *vdata)
+static notrace int do_monotonic(struct timespec *ts,
+				struct vdso_data *vdata,
+				struct vdso_priv *vpriv)
 {
 	struct timespec tomono;
 	u64 nsecs;
 	u32 seq;
 
 	do {
-		seq = vdso_read_begin(vdata);
+		seq = vdso_read_begin(vdata, vpriv);
 
-		if (!vdata->tk_is_cntvct)
+		if (!clock_accessible(vpriv))
 			return -1;
 
 		ts->tv_sec = vdata->xtime_clock_sec;
-		nsecs = get_ns(vdata);
+		nsecs = get_ns(vdata, vpriv);
 
 		tomono.tv_sec = vdata->wtm_clock_sec;
 		tomono.tv_nsec = vdata->wtm_clock_nsec;
@@ -171,63 +445,34 @@ static notrace int do_monotonic(struct timespec *ts, struct vdso_data *vdata)
 	return 0;
 }
 
-#else /* CONFIG_ARM_ARCH_TIMER */
-
-static notrace int do_realtime(struct timespec *ts, struct vdso_data *vdata)
-{
-	return -1;
-}
-
-static notrace int do_monotonic(struct timespec *ts, struct vdso_data *vdata)
-{
-	return -1;
-}
-
-#endif /* CONFIG_ARM_ARCH_TIMER */
-
 notrace int __vdso_clock_gettime(clockid_t clkid, struct timespec *ts)
 {
 	struct vdso_data *vdata;
+	struct vdso_priv *vpriv;
 	int ret = -1;
 
 	vdata = __get_datapage();
+	vpriv = __get_privpage();
 
 	switch (clkid) {
 	case CLOCK_REALTIME_COARSE:
-		ret = do_realtime_coarse(ts, vdata);
+		ret = do_realtime_coarse(ts, vdata, vpriv);
 		break;
 	case CLOCK_MONOTONIC_COARSE:
-		ret = do_monotonic_coarse(ts, vdata);
+		ret = do_monotonic_coarse(ts, vdata, vpriv);
 		break;
 	case CLOCK_REALTIME:
-		ret = do_realtime(ts, vdata);
+		ret = do_realtime(ts, vdata, vpriv);
 		break;
 	case CLOCK_MONOTONIC:
-		ret = do_monotonic(ts, vdata);
+		ret = do_monotonic(ts, vdata, vpriv);
 		break;
 	default:
 		break;
 	}
 
 	if (ret)
-		ret = clock_gettime_fallback(clkid, ts);
-
-	return ret;
-}
-
-static notrace long gettimeofday_fallback(struct timeval *_tv,
-					  struct timezone *_tz)
-{
-	register struct timezone *tz asm("r1") = _tz;
-	register struct timeval *tv asm("r0") = _tv;
-	register long ret asm ("r0");
-	register long nr asm("r7") = __NR_gettimeofday;
-
-	asm volatile(
-	"	swi #0\n"
-	: "=r" (ret)
-	: "r" (tv), "r" (tz), "r" (nr)
-	: "memory");
+		ret = sys_clock_gettime(clkid, ts);
 
 	return ret;
 }
@@ -236,13 +481,15 @@ notrace int __vdso_gettimeofday(struct timeval *tv, struct timezone *tz)
 {
 	struct timespec ts;
 	struct vdso_data *vdata;
+	struct vdso_priv *vpriv;
 	int ret;
 
 	vdata = __get_datapage();
+	vpriv = __get_privpage();
 
-	ret = do_realtime(&ts, vdata);
+	ret = do_realtime(&ts, vdata, vpriv);
 	if (ret)
-		return gettimeofday_fallback(tv, tz);
+		return sys_gettimeofday(tv, tz);
 
 	if (tv) {
 		tv->tv_sec = ts.tv_sec;
diff --git a/arch/arm/vfp/entry.S b/arch/arm/vfp/entry.S
index 0186cf9da890..cc56142a4191 100644
--- a/arch/arm/vfp/entry.S
+++ b/arch/arm/vfp/entry.S
@@ -23,6 +23,7 @@
 @
 ENTRY(do_vfp)
 	inc_preempt_count r10, r4
+	disable_irq_if_pipelined
  	ldr	r4, .LCvfp
 	ldr	r11, [r10, #TI_CPU]	@ CPU number
 	add	r10, r10, #TI_VFPSTATE	@ r10 = workspace
@@ -30,6 +31,7 @@ ENTRY(do_vfp)
 ENDPROC(do_vfp)
 
 ENTRY(vfp_null_entry)
+	enable_irq_if_pipelined
 	dec_preempt_count_ti r10, r4
 	ret	lr
 ENDPROC(vfp_null_entry)
@@ -43,6 +45,7 @@ ENDPROC(vfp_null_entry)
 
 	__INIT
 ENTRY(vfp_testing_entry)
+	enable_irq_if_pipelined
 	dec_preempt_count_ti r10, r4
 	ldr	r0, VFP_arch_address
 	str	r0, [r0]		@ set to non-zero value
diff --git a/arch/arm/vfp/vfphw.S b/arch/arm/vfp/vfphw.S
index b2e560290860..1f07393c8907 100644
--- a/arch/arm/vfp/vfphw.S
+++ b/arch/arm/vfp/vfphw.S
@@ -174,6 +174,7 @@ vfp_hw_state_valid:
 					@ out before setting an FPEXC that
 					@ stops us reading stuff
 	VFPFMXR	FPEXC, r1		@ Restore FPEXC last
+	enable_irq_if_pipelined
 	sub	r2, r2, #4		@ Retry current instruction - if Thumb
 	str	r2, [sp, #S_PC]		@ mode it's two 16-bit instructions,
 					@ else it's one 32-bit instruction, so
@@ -203,6 +204,7 @@ skip:
 	@ Fall into hand on to next handler - appropriate coproc instr
 	@ not recognised by VFP
 
+	enable_irq_if_pipelined
 	DBGSTR	"not VFP"
 	dec_preempt_count_ti r10, r4
 	ret	lr
diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 8c9e7f9f0277..54acebb4cd4d 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -18,6 +18,7 @@
 #include <linux/uaccess.h>
 #include <linux/user.h>
 #include <linux/export.h>
+#include <linux/smp.h>
 
 #include <asm/cp15.h>
 #include <asm/cputype.h>
@@ -90,6 +91,7 @@ static void vfp_force_reload(unsigned int cpu, struct thread_info *thread)
 static void vfp_thread_flush(struct thread_info *thread)
 {
 	union vfp_state *vfp = &thread->vfpstate;
+	unsigned long flags;
 	unsigned int cpu;
 
 	/*
@@ -100,11 +102,11 @@ static void vfp_thread_flush(struct thread_info *thread)
 	 * Do this first to ensure that preemption won't overwrite our
 	 * state saving should access to the VFP be enabled at this point.
 	 */
-	cpu = get_cpu();
+	cpu = hard_get_cpu(flags);
 	if (vfp_current_hw_state[cpu] == vfp)
 		vfp_current_hw_state[cpu] = NULL;
 	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
-	put_cpu();
+	hard_put_cpu(flags);
 
 	memset(vfp, 0, sizeof(union vfp_state));
 
@@ -119,11 +121,12 @@ static void vfp_thread_exit(struct thread_info *thread)
 {
 	/* release case: Per-thread VFP cleanup. */
 	union vfp_state *vfp = &thread->vfpstate;
-	unsigned int cpu = get_cpu();
+	unsigned long flags;
+	unsigned int cpu = hard_get_cpu(flags);
 
 	if (vfp_current_hw_state[cpu] == vfp)
 		vfp_current_hw_state[cpu] = NULL;
-	put_cpu();
+	hard_put_cpu(flags);
 }
 
 static void vfp_thread_copy(struct thread_info *thread)
@@ -159,6 +162,7 @@ static void vfp_thread_copy(struct thread_info *thread)
 static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 {
 	struct thread_info *thread = v;
+	unsigned long flags;
 	u32 fpexc;
 #ifdef CONFIG_SMP
 	unsigned int cpu;
@@ -166,6 +170,7 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 
 	switch (cmd) {
 	case THREAD_NOTIFY_SWITCH:
+		flags = hard_cond_local_irq_save();
 		fpexc = fmrx(FPEXC);
 
 #ifdef CONFIG_SMP
@@ -185,6 +190,7 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 		 * old state.
 		 */
 		fmxr(FPEXC, fpexc & ~FPEXC_EN);
+		hard_cond_local_irq_restore(flags);
 		break;
 
 	case THREAD_NOTIFY_FLUSH:
@@ -322,7 +328,7 @@ static u32 vfp_emulate_instruction(u32 inst, u32 fpscr, struct pt_regs *regs)
  */
 void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 {
-	u32 fpscr, orig_fpscr, fpsid, exceptions;
+	u32 fpscr, orig_fpscr, fpsid, exceptions, next_trigger = 0;
 
 	pr_debug("VFP: bounce: trigger %08x fpexc %08x\n", trigger, fpexc);
 
@@ -352,6 +358,7 @@ void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 		/*
 		 * Synchronous exception, emulate the trigger instruction
 		 */
+		hard_cond_local_irq_enable();
 		goto emulate;
 	}
 
@@ -364,7 +371,18 @@ void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 		trigger = fmrx(FPINST);
 		regs->ARM_pc -= 4;
 #endif
-	} else if (!(fpexc & FPEXC_DEX)) {
+		if (fpexc & FPEXC_FP2V) {
+			/*
+			 * The barrier() here prevents fpinst2 being read
+			 * before the condition above.
+			 */
+			barrier();
+			next_trigger = fmrx(FPINST2);
+		}
+	}
+	hard_cond_local_irq_enable();
+
+	if (!(fpexc & (FPEXC_EX | FPEXC_DEX))) {
 		/*
 		 * Illegal combination of bits. It can be caused by an
 		 * unallocated VFP instruction but with FPSCR.IXE set and not
@@ -404,18 +422,14 @@ void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	if ((fpexc & (FPEXC_EX | FPEXC_FP2V)) != (FPEXC_EX | FPEXC_FP2V))
 		goto exit;
 
-	/*
-	 * The barrier() here prevents fpinst2 being read
-	 * before the condition above.
-	 */
-	barrier();
-	trigger = fmrx(FPINST2);
+	trigger = next_trigger;
 
  emulate:
 	exceptions = vfp_emulate_instruction(trigger, orig_fpscr, regs);
 	if (exceptions)
 		vfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);
  exit:
+	hard_cond_local_irq_enable();
 	preempt_enable();
 }
 
@@ -515,7 +529,8 @@ static inline void vfp_pm_init(void) { }
  */
 void vfp_sync_hwstate(struct thread_info *thread)
 {
-	unsigned int cpu = get_cpu();
+	unsigned long flags;
+	unsigned int cpu = hard_get_cpu(flags);
 
 	if (vfp_state_in_hw(cpu, thread)) {
 		u32 fpexc = fmrx(FPEXC);
@@ -528,17 +543,18 @@ void vfp_sync_hwstate(struct thread_info *thread)
 		fmxr(FPEXC, fpexc);
 	}
 
-	put_cpu();
+	hard_put_cpu(flags);
 }
 
 /* Ensure that the thread reloads the hardware VFP state on the next use. */
 void vfp_flush_hwstate(struct thread_info *thread)
 {
-	unsigned int cpu = get_cpu();
+	unsigned long flags;
+	unsigned int cpu = hard_get_cpu(flags);
 
 	vfp_force_reload(cpu, thread);
 
-	put_cpu();
+	hard_put_cpu(flags);
 }
 
 /*
diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
index 3f047afb982c..4a2a4859a8f4 100644
--- a/arch/arm64/Kconfig
+++ b/arch/arm64/Kconfig
@@ -151,6 +151,8 @@ config ARM64
 	select HAVE_FUNCTION_GRAPH_TRACER
 	select HAVE_GCC_PLUGINS
 	select HAVE_HW_BREAKPOINT if PERF_EVENTS
+	select HAVE_IRQ_PIPELINE
+	select HAVE_DOVETAIL
 	select HAVE_IRQ_TIME_ACCOUNTING
 	select HAVE_MEMBLOCK_NODE_MAP if NUMA
 	select HAVE_NMI
@@ -961,6 +963,8 @@ config SECCOMP
 	  and the task is only allowed to execute a few safe syscalls
 	  defined by each seccomp mode.
 
+source "kernel/Kconfig.dovetail"
+
 config PARAVIRT
 	bool "Enable paravirtualization code"
 	help
diff --git a/arch/arm64/boot/dts/broadcom/Makefile b/arch/arm64/boot/dts/broadcom/Makefile
index d1d31ccad758..473da7c12853 100644
--- a/arch/arm64/boot/dts/broadcom/Makefile
+++ b/arch/arm64/boot/dts/broadcom/Makefile
@@ -2,6 +2,7 @@
 dtb-$(CONFIG_ARCH_BCM2835) += bcm2837-rpi-3-a-plus.dtb \
 			      bcm2837-rpi-3-b.dtb \
 			      bcm2837-rpi-3-b-plus.dtb \
+			      bcm2837-rpi-3-b-nobt.dtb \
 			      bcm2837-rpi-cm3-io3.dtb
 
 subdir-y	+= northstar2
diff --git a/arch/arm64/boot/dts/broadcom/bcm2837-rpi-3-b-nobt.dts b/arch/arm64/boot/dts/broadcom/bcm2837-rpi-3-b-nobt.dts
new file mode 100644
index 000000000000..43f9d0f665cb
--- /dev/null
+++ b/arch/arm64/boot/dts/broadcom/bcm2837-rpi-3-b-nobt.dts
@@ -0,0 +1,12 @@
+/dts-v1/;
+#include "bcm2837-rpi-3-b.dts"
+
+&uart0 {
+	status = "okay";
+	pinctrl-names = "default";
+	pinctrl-0 = <&uart0_gpio32>;
+};
+
+&uart1 {
+	status = "disabled";
+};
diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index b8cf7c85ffa2..a0f962d95fe8 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -63,6 +63,12 @@
 	msr	daif, \flags
 	.endm
 
+	.macro	enable_irq_if_pipelined
+#ifdef CONFIG_IRQ_PIPELINE
+	msr	daifclr, #2
+#endif
+	.endm
+
 	.macro	enable_dbg
 	msr	daifclr, #8
 	.endm
diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index 063c964af705..9379fb0d1caf 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -10,6 +10,12 @@
 #include <asm/arch_gicv3.h>
 #include <asm/cpufeature.h>
 
+/*
+ * irq_pipeline: DAIF masking is only used in contexts where hard
+ * interrupt masking applies, so no need to virtualize for the inband
+ * stage here (the pipeline core does assume this).
+ */
+
 #define DAIF_PROCCTX		0
 #define DAIF_PROCCTX_NOIRQ	PSR_I_BIT
 #define DAIF_ERRCTX		(PSR_I_BIT | PSR_A_BIT)
diff --git a/arch/arm64/include/asm/dovetail.h b/arch/arm64/include/asm/dovetail.h
new file mode 100644
index 000000000000..4802df0c347c
--- /dev/null
+++ b/arch/arm64/include/asm/dovetail.h
@@ -0,0 +1,29 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#ifndef _ASM_ARM64_DOVETAIL_H
+#define _ASM_ARM64_DOVETAIL_H
+
+#include <asm/fpsimd.h>
+
+/* ARM64 traps */
+#define ARM64_TRAP_ACCESS	0	/* Data or instruction access exception */
+#define ARM64_TRAP_ABRT		1	/* Memory/alignment abort */
+#define ARM64_TRAP_SEA		2	/* Synchronous external abort */
+#define ARM64_TRAP_DEBUG	3	/* Debug trap */
+#define ARM64_TRAP_UNDI		4	/* Undefined instruction */
+#define ARM64_TRAP_UNDSE	5	/* Undefined synchronous exception */
+#define ARM64_TRAP_FPE		6	/* FPSIMD exception */
+#define ARM64_TRAP_SVE		7	/* SVE access trap */
+
+static inline void arch_dovetail_switch_prepare(bool leave_inband)
+{ }
+
+static inline void arch_dovetail_switch_finish(bool enter_inband)
+{
+	fpsimd_restore_current_oob();
+}
+
+#endif /* _ASM_ARM64_DOVETAIL_H */
diff --git a/arch/arm64/include/asm/efi.h b/arch/arm64/include/asm/efi.h
index b54d3a86c444..0bdb392861ca 100644
--- a/arch/arm64/include/asm/efi.h
+++ b/arch/arm64/include/asm/efi.h
@@ -135,6 +135,10 @@ static inline void efifb_setup_from_dmi(struct screen_info *si, const char *opt)
 
 static inline void efi_set_pgd(struct mm_struct *mm)
 {
+	unsigned long flags;
+
+	protect_inband_mm(flags);
+
 	__switch_mm(mm);
 
 	if (system_uses_ttbr0_pan()) {
@@ -159,6 +163,8 @@ static inline void efi_set_pgd(struct mm_struct *mm)
 			update_saved_ttbr0(current, current->active_mm);
 		}
 	}
+
+	unprotect_inband_mm(flags);
 }
 
 void efi_virtmap_load(void);
diff --git a/arch/arm64/include/asm/fpsimd.h b/arch/arm64/include/asm/fpsimd.h
index 59f10dd13f12..1f8faac0dacf 100644
--- a/arch/arm64/include/asm/fpsimd.h
+++ b/arch/arm64/include/asm/fpsimd.h
@@ -43,6 +43,7 @@ extern void fpsimd_flush_thread(void);
 extern void fpsimd_signal_preserve_current_state(void);
 extern void fpsimd_preserve_current_state(void);
 extern void fpsimd_restore_current_state(void);
+extern void fpsimd_restore_current_oob(void);
 extern void fpsimd_update_current_state(struct user_fpsimd_state const *state);
 
 extern void fpsimd_bind_task_to_cpu(void);
diff --git a/arch/arm64/include/asm/irq_pipeline.h b/arch/arm64/include/asm/irq_pipeline.h
new file mode 100644
index 000000000000..86b7f9521ad8
--- /dev/null
+++ b/arch/arm64/include/asm/irq_pipeline.h
@@ -0,0 +1,125 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#ifndef _ASM_ARM64_IRQ_PIPELINE_H
+#define _ASM_ARM64_IRQ_PIPELINE_H
+
+#include <asm-generic/irq_pipeline.h>
+
+#ifdef CONFIG_IRQ_PIPELINE
+
+/*
+ * Out-of-band IPIs are directly mapped to SGI1-2, instead of
+ * multiplexed over SGI0 like regular in-band messages.
+ */
+#define OOB_IPI_BASE		2048
+#define OOB_NR_IPI		2
+#define TIMER_OOB_IPI		(OOB_IPI_BASE + NR_IPI)
+#define RESCHEDULE_OOB_IPI	(OOB_IPI_BASE + NR_IPI + 1)
+
+static inline notrace
+unsigned long arch_irqs_virtual_to_native_flags(int stalled)
+{
+	return (!!stalled) << IRQMASK_I_POS;
+}
+
+static inline notrace
+unsigned long arch_irqs_native_to_virtual_flags(unsigned long flags)
+{
+	return (!!hard_irqs_disabled_flags(flags)) << IRQMASK_i_POS;
+}
+
+static inline notrace unsigned long arch_local_irq_save(void)
+{
+	int stalled = inband_irq_save();
+	barrier();
+	return arch_irqs_virtual_to_native_flags(stalled);
+}
+
+static inline notrace void arch_local_irq_enable(void)
+{
+	barrier();
+	inband_irq_enable();
+}
+
+static inline notrace void arch_local_irq_disable(void)
+{
+	inband_irq_disable();
+	barrier();
+}
+
+static inline notrace unsigned long arch_local_save_flags(void)
+{
+	int stalled = inband_irqs_disabled();
+	barrier();
+	return arch_irqs_virtual_to_native_flags(stalled);
+}
+
+static inline int arch_irqs_disabled_flags(unsigned long flags)
+{
+	return native_irqs_disabled_flags(flags);
+}
+
+static inline notrace void arch_local_irq_restore(unsigned long flags)
+{
+	inband_irq_restore(arch_irqs_disabled_flags(flags));
+	barrier();
+}
+
+static inline
+void arch_save_timer_regs(struct pt_regs *dst,
+			  struct pt_regs *src, bool head_context)
+{
+	dst->pstate = src->pstate;
+	dst->pc = src->pc;
+	if (head_context)
+		dst->pstate |= IRQMASK_I_BIT;
+}
+
+static inline bool arch_steal_pipelined_tick(struct pt_regs *regs)
+{
+	return !!(regs->pstate & IRQMASK_I_BIT);
+}
+
+static inline int arch_enable_oob_stage(void)
+{
+	return 0;
+}
+
+#else  /* !CONFIG_IRQ_PIPELINE */
+
+static inline unsigned long arch_local_irq_save(void)
+{
+	return native_irq_save();
+}
+
+static inline void arch_local_irq_enable(void)
+{
+	native_irq_enable();
+}
+
+static inline void arch_local_irq_disable(void)
+{
+	native_irq_disable();
+}
+
+static inline unsigned long arch_local_save_flags(void)
+{
+	return native_save_flags();
+}
+
+static inline void arch_local_irq_restore(unsigned long flags)
+{
+	native_irq_restore(flags);
+}
+
+static inline int arch_irqs_disabled_flags(unsigned long flags)
+{
+	return native_irqs_disabled_flags(flags);
+}
+
+#endif /* !CONFIG_IRQ_PIPELINE */
+
+#endif /* _ASM_ARM64_IRQ_PIPELINE_H */
diff --git a/arch/arm64/include/asm/irqflags.h b/arch/arm64/include/asm/irqflags.h
index 1a59f0ed1ae3..5e4306e63cad 100644
--- a/arch/arm64/include/asm/irqflags.h
+++ b/arch/arm64/include/asm/irqflags.h
@@ -9,6 +9,10 @@
 #include <asm/ptrace.h>
 #include <asm/sysreg.h>
 
+#define IRQMASK_I_BIT	PSR_I_BIT
+#define IRQMASK_I_POS	7
+#define IRQMASK_i_POS	31
+
 /*
  * Aarch64 has flags for masking: Debug, Asynchronous (serror), Interrupts and
  * FIQ exceptions, in the 'daif' register. We mask and unmask them in 'dai'
@@ -25,7 +29,7 @@
 /*
  * CPU interrupt mask handling.
  */
-static inline void arch_local_irq_enable(void)
+static inline void native_irq_enable(void)
 {
 	if (system_has_prio_mask_debugging()) {
 		u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
@@ -34,7 +38,7 @@ static inline void arch_local_irq_enable(void)
 	}
 
 	asm volatile(ALTERNATIVE(
-		"msr	daifclr, #2		// arch_local_irq_enable\n"
+		"msr	daifclr, #2		// native_irq_enable\n"
 		"nop",
 		__msr_s(SYS_ICC_PMR_EL1, "%0")
 		"dsb	sy",
@@ -44,7 +48,7 @@ static inline void arch_local_irq_enable(void)
 		: "memory");
 }
 
-static inline void arch_local_irq_disable(void)
+static inline void native_irq_disable(void)
 {
 	if (system_has_prio_mask_debugging()) {
 		u32 pmr = read_sysreg_s(SYS_ICC_PMR_EL1);
@@ -53,7 +57,7 @@ static inline void arch_local_irq_disable(void)
 	}
 
 	asm volatile(ALTERNATIVE(
-		"msr	daifset, #2		// arch_local_irq_disable",
+		"msr	daifset, #2		// native_irq_disable",
 		__msr_s(SYS_ICC_PMR_EL1, "%0"),
 		ARM64_HAS_IRQ_PRIO_MASKING)
 		:
@@ -61,10 +65,17 @@ static inline void arch_local_irq_disable(void)
 		: "memory");
 }
 
+static inline void native_irq_sync(void)
+{
+	native_irq_enable();
+	isb();
+	native_irq_disable();
+}
+
 /*
  * Save the current interrupt enable state.
  */
-static inline unsigned long arch_local_save_flags(void)
+static inline unsigned long native_save_flags(void)
 {
 	unsigned long flags;
 
@@ -79,7 +90,7 @@ static inline unsigned long arch_local_save_flags(void)
 	return flags;
 }
 
-static inline int arch_irqs_disabled_flags(unsigned long flags)
+static inline int native_irqs_disabled_flags(unsigned long flags)
 {
 	int res;
 
@@ -94,18 +105,18 @@ static inline int arch_irqs_disabled_flags(unsigned long flags)
 	return res;
 }
 
-static inline unsigned long arch_local_irq_save(void)
+static inline unsigned long native_irq_save(void)
 {
 	unsigned long flags;
 
-	flags = arch_local_save_flags();
+	flags = native_save_flags();
 
 	/*
 	 * There are too many states with IRQs disabled, just keep the current
 	 * state if interrupts are already disabled/masked.
 	 */
-	if (!arch_irqs_disabled_flags(flags))
-		arch_local_irq_disable();
+	if (!native_irqs_disabled_flags(flags))
+		native_irq_disable();
 
 	return flags;
 }
@@ -113,7 +124,7 @@ static inline unsigned long arch_local_irq_save(void)
 /*
  * restore saved IRQ state
  */
-static inline void arch_local_irq_restore(unsigned long flags)
+static inline void native_irq_restore(unsigned long flags)
 {
 	asm volatile(ALTERNATIVE(
 			"msr	daif, %0\n"
@@ -126,4 +137,12 @@ static inline void arch_local_irq_restore(unsigned long flags)
 		: "memory");
 }
 
+static inline bool native_irqs_disabled(void)
+{
+	unsigned long flags = native_save_flags();
+	return native_irqs_disabled_flags(flags);
+}
+
+#include <asm/irq_pipeline.h>
+
 #endif /* __ASM_IRQFLAGS_H */
diff --git a/arch/arm64/include/asm/mmu_context.h b/arch/arm64/include/asm/mmu_context.h
index 3827ff4040a3..35c659fc9c1e 100644
--- a/arch/arm64/include/asm/mmu_context.h
+++ b/arch/arm64/include/asm/mmu_context.h
@@ -113,6 +113,9 @@ static inline void __cpu_set_tcr_t0sz(unsigned long t0sz)
 static inline void cpu_uninstall_idmap(void)
 {
 	struct mm_struct *mm = current->active_mm;
+	unsigned long flags;
+
+	flags = hard_cond_local_irq_save();
 
 	cpu_set_reserved_ttbr0();
 	local_flush_tlb_all();
@@ -120,15 +123,23 @@ static inline void cpu_uninstall_idmap(void)
 
 	if (mm != &init_mm && !system_uses_ttbr0_pan())
 		cpu_switch_mm(mm->pgd, mm);
+
+	hard_cond_local_irq_restore(flags);
 }
 
 static inline void cpu_install_idmap(void)
 {
+	unsigned long flags;
+
+	flags = hard_cond_local_irq_save();
+
 	cpu_set_reserved_ttbr0();
 	local_flush_tlb_all();
 	cpu_set_idmap_tcr_t0sz();
 
 	cpu_switch_mm(lm_alias(idmap_pg_dir), &init_mm);
+
+	hard_cond_local_irq_restore(flags);
 }
 
 /*
@@ -227,7 +238,7 @@ static inline void __switch_mm(struct mm_struct *next)
 }
 
 static inline void
-switch_mm(struct mm_struct *prev, struct mm_struct *next,
+do_switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	  struct task_struct *tsk)
 {
 	if (prev != next)
@@ -242,8 +253,26 @@ switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	update_saved_ttbr0(tsk, next);
 }
 
+static inline void
+switch_mm(struct mm_struct *prev, struct mm_struct *next,
+	  struct task_struct *tsk)
+{
+	unsigned long flags;
+
+	protect_inband_mm(flags);
+	do_switch_mm(prev, next, tsk);
+	unprotect_inband_mm(flags);
+}
+
 #define deactivate_mm(tsk,mm)	do { } while (0)
-#define activate_mm(prev,next)	switch_mm(prev, next, current)
+#define activate_mm(prev,next)	do_switch_mm(prev, next, current)
+
+static inline void
+switch_oob_mm(struct mm_struct *prev, struct mm_struct *next,
+	      struct task_struct *tsk)
+{
+	do_switch_mm(prev, next, tsk);
+}
 
 void verify_cpu_asid_bits(void);
 void post_ttbr_update_workaround(void);
diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index a0c8a0b65259..7a75ddfc14d3 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -56,10 +56,15 @@ struct seq_file;
 extern void show_ipi_list(struct seq_file *p, int prec);
 
 /*
- * Called from C code, this handles an IPI.
+ * Called from C code, this handles an IPI (including pipelined ones).
  */
 extern void handle_IPI(int ipinr, struct pt_regs *regs);
 
+/*
+ * Handles IPIs for the in-band stage exclusively.
+ */
+void __handle_IPI(int ipinr, struct pt_regs *regs);
+
 /*
  * Discover the set of possible CPUs and determine their
  * SMP operations.
@@ -73,6 +78,8 @@ extern void set_smp_cross_call(void (*)(const struct cpumask *, unsigned int));
 
 extern void (*__smp_cross_call)(const struct cpumask *, unsigned int);
 
+void smp_cross_call(const struct cpumask *target, unsigned int ipinr);
+
 /*
  * Called from the secondary holding pen, this is the secondary CPU entry point.
  */
diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index f0cec4160136..1da8cc3ca560 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -14,6 +14,7 @@
 
 struct task_struct;
 
+#include <dovetail/thread_info.h>
 #include <asm/memory.h>
 #include <asm/stack_pointer.h>
 #include <asm/types.h>
@@ -25,6 +26,7 @@ typedef unsigned long mm_segment_t;
  */
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
+	unsigned long		local_flags;	/* local (synchronous) flags */
 	mm_segment_t		addr_limit;	/* address limit */
 #ifdef CONFIG_ARM64_SW_TTBR0_PAN
 	u64			ttbr0;		/* saved TTBR0_EL1 */
@@ -41,6 +43,7 @@ struct thread_info {
 #endif
 		} preempt;
 	};
+	struct oob_thread_state	oob_state;	/* co-kernel thread state */
 };
 
 #define thread_saved_pc(tsk)	\
@@ -55,6 +58,8 @@ void arch_setup_new_exec(void);
 
 void arch_release_task_struct(struct task_struct *tsk);
 
+#define ti_local_flags(__ti)	((__ti)->local_flags)
+
 #endif
 
 #define TIF_SIGPENDING		0	/* signal pending */
@@ -78,6 +83,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define TIF_SVE_VL_INHERIT	24	/* Inherit sve_vl_onexec across exec */
 #define TIF_SSBD		25	/* Wants SSB mitigation */
 #define TIF_TAGGED_ADDR		26	/* Allow tagged user addresses */
+#define TIF_MAYDAY		27	/* Emergency trap pending */
 
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
@@ -93,6 +99,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define _TIF_FSCHECK		(1 << TIF_FSCHECK)
 #define _TIF_32BIT		(1 << TIF_32BIT)
 #define _TIF_SVE		(1 << TIF_SVE)
+#define _TIF_MAYDAY		(1 << TIF_MAYDAY)
 
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
 				 _TIF_NOTIFY_RESUME | _TIF_FOREIGN_FPSTATE | \
@@ -109,4 +116,11 @@ void arch_release_task_struct(struct task_struct *tsk);
 	.addr_limit	= KERNEL_DS,					\
 }
 
+/*
+ * Local (synchronous) thread flags.
+ */
+#define _TLF_OOB		0x0001
+#define _TLF_DOVETAIL		0x0002
+#define _TLF_OFFSTAGE		0x0004
+
 #endif /* __ASM_THREAD_INFO_H */
diff --git a/arch/arm64/include/asm/uaccess.h b/arch/arm64/include/asm/uaccess.h
index 127712b0b970..3d13c9f34264 100644
--- a/arch/arm64/include/asm/uaccess.h
+++ b/arch/arm64/include/asm/uaccess.h
@@ -104,7 +104,7 @@ static inline void __uaccess_ttbr0_disable(void)
 {
 	unsigned long flags, ttbr;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	ttbr = read_sysreg(ttbr1_el1);
 	ttbr &= ~TTBR_ASID_MASK;
 	/* reserved_ttbr0 placed before swapper_pg_dir */
@@ -113,7 +113,7 @@ static inline void __uaccess_ttbr0_disable(void)
 	/* Set reserved ASID */
 	write_sysreg(ttbr, ttbr1_el1);
 	isb();
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static inline void __uaccess_ttbr0_enable(void)
@@ -125,7 +125,7 @@ static inline void __uaccess_ttbr0_enable(void)
 	 * variable and the MSR. A context switch could trigger an ASID
 	 * roll-over and an update of 'ttbr0'.
 	 */
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	ttbr0 = READ_ONCE(current_thread_info()->ttbr0);
 
 	/* Restore active ASID */
@@ -138,7 +138,7 @@ static inline void __uaccess_ttbr0_enable(void)
 	/* Restore user page table */
 	write_sysreg(ttbr0, ttbr0_el1);
 	isb();
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static inline bool uaccess_ttbr0_disable(void)
diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
index 478491f07b4f..7028bca90cc2 100644
--- a/arch/arm64/kernel/Makefile
+++ b/arch/arm64/kernel/Makefile
@@ -51,6 +51,7 @@ obj-$(CONFIG_ACPI)			+= acpi.o
 obj-$(CONFIG_ACPI_NUMA)			+= acpi_numa.o
 obj-$(CONFIG_ARM64_ACPI_PARKING_PROTOCOL)	+= acpi_parking_protocol.o
 obj-$(CONFIG_PARAVIRT)			+= paravirt.o
+obj-$(CONFIG_IRQ_PIPELINE)		+= irq_pipeline.o
 obj-$(CONFIG_RANDOMIZE_BASE)		+= kaslr.o
 obj-$(CONFIG_HIBERNATION)		+= hibernate.o hibernate-asm.o
 obj-$(CONFIG_KEXEC_CORE)		+= machine_kexec.o relocate_kernel.o	\
diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
index 214685760e1c..65587e6657f5 100644
--- a/arch/arm64/kernel/asm-offsets.c
+++ b/arch/arm64/kernel/asm-offsets.c
@@ -29,6 +29,7 @@ int main(void)
   DEFINE(TSK_ACTIVE_MM,		offsetof(struct task_struct, active_mm));
   BLANK();
   DEFINE(TSK_TI_FLAGS,		offsetof(struct task_struct, thread_info.flags));
+  DEFINE(TSK_TI_LOCAL_FLAGS,	offsetof(struct task_struct, thread_info.local_flags));
   DEFINE(TSK_TI_PREEMPT,	offsetof(struct task_struct, thread_info.preempt_count));
   DEFINE(TSK_TI_ADDR_LIMIT,	offsetof(struct task_struct, thread_info.addr_limit));
 #ifdef CONFIG_ARM64_SW_TTBR0_PAN
diff --git a/arch/arm64/kernel/entry.S b/arch/arm64/kernel/entry.S
index cf3bd2976e57..0d61832e7578 100644
--- a/arch/arm64/kernel/entry.S
+++ b/arch/arm64/kernel/entry.S
@@ -384,6 +384,21 @@ alternative_insn eret, nop, ARM64_UNMAP_KERNEL_AT_EL0
 	.macro	irq_stack_entry
 	mov	x19, sp			// preserve the original sp
 
+#ifdef CONFIG_DOVETAIL
+	/*
+	 * When the pipeline is enabled, context switches over the irq
+	 * stack are allowed (for the co-kernel), and more interrupts
+	 * can be taken over sibling stack contexts. So we need a not so
+	 * subtle way of figuring out whether the irq stack was actually
+	 * exited, which cannot depend on the current task pointer.
+	 */
+	adr_this_cpu x25, irq_nesting, x26
+	ldr	w26, [x25]
+	cmp	w26, #0
+	add	w26, w26, #1
+	str	w26, [x25]
+	b.ne	9998f
+#else
 	/*
 	 * Compare sp with the base of the task stack.
 	 * If the top ~(THREAD_SIZE - 1) bits match, we are on a task stack,
@@ -393,6 +408,7 @@ alternative_insn eret, nop, ARM64_UNMAP_KERNEL_AT_EL0
 	eor	x25, x25, x19
 	and	x25, x25, #~(THREAD_SIZE - 1)
 	cbnz	x25, 9998f
+#endif
 
 	ldr_this_cpu x25, irq_stack_ptr, x26
 	mov	x26, #IRQ_STACK_SIZE
@@ -405,10 +421,17 @@ alternative_insn eret, nop, ARM64_UNMAP_KERNEL_AT_EL0
 
 	/*
 	 * x19 should be preserved between irq_stack_entry and
-	 * irq_stack_exit.
+	 * irq_stack_exit. IRQ_PIPELINE: caution, we have to
+	 * preserve w0.
 	 */
 	.macro	irq_stack_exit
 	mov	sp, x19
+#ifdef CONFIG_DOVETAIL
+	adr_this_cpu x1, irq_nesting, x2
+	ldr	w2, [x1]
+	add	w2, w2, #-1
+	str	w2, [x1]
+#endif
 	.endm
 
 /* GPRs used by entry code */
@@ -418,7 +441,11 @@ tsk	.req	x28		// current thread_info
  * Interrupt handling.
  */
 	.macro	irq_handler
+#ifdef CONFIG_IRQ_PIPELINE
+	ldr	x1, =handle_arch_irq_pipelined
+#else
 	ldr_l	x1, handle_arch_irq
+#endif
 	mov	x0, sp
 	irq_stack_entry
 	blr	x1
@@ -669,6 +696,10 @@ el1_irq:
 
 	irq_handler
 
+#ifdef CONFIG_IRQ_PIPELINE
+	cbz     w0, 66f
+#endif
+
 #ifdef CONFIG_PREEMPT
 	ldr	x24, [tsk, #TSK_TI_PREEMPT]	// get preempt count
 alternative_if ARM64_HAS_IRQ_PRIO_MASKING
@@ -684,6 +715,7 @@ alternative_else_nop_endif
 1:
 #endif
 
+66:
 #ifdef CONFIG_ARM64_PSEUDO_NMI
 	/*
 	 * When using IRQ priority masking, we can get spurious interrupts while
@@ -701,7 +733,7 @@ alternative_else_nop_endif
 	test_irqs_unmasked	res=x0, pmr=x20
 	cbnz	x0, 1f
 #endif
-	bl	trace_hardirqs_on
+	bl	trace_hardirqs_on_pipelined
 1:
 #endif
 
@@ -940,13 +972,24 @@ el0_irq_naked:
 1:
 #endif
 	irq_handler
-
+#ifdef CONFIG_IRQ_PIPELINE
+	cbz	w0, work_done_el0
+#endif
 #ifdef CONFIG_TRACE_IRQFLAGS
-	bl	trace_hardirqs_on
+	bl	trace_hardirqs_on_pipelined
 #endif
-	b	ret_to_user
+	disable_daif
+	b	ret_to_user_noirq
 ENDPROC(el0_irq)
 
+#ifdef CONFIG_IRQ_PIPELINE
+work_done_el0:
+#ifdef CONFIG_TRACE_IRQFLAGS
+	bl	trace_hardirqs_on_pipelined
+#endif
+	b	work_done
+#endif
+
 el1_error:
 	kernel_entry 1
 	mrs	x1, esr_el1
@@ -980,6 +1023,7 @@ work_pending:
 #ifdef CONFIG_TRACE_IRQFLAGS
 	bl	trace_hardirqs_on		// enabled while in userspace
 #endif
+work_done:
 	ldr	x1, [tsk, #TSK_TI_FLAGS]	// re-check for single-step
 	b	finish_ret_to_user
 /*
@@ -988,6 +1032,12 @@ work_pending:
 ret_to_user:
 	disable_daif
 	gic_prio_kentry_setup tmp=x3
+#ifdef CONFIG_IRQ_PIPELINE
+	ldr	x0, [tsk, #TSK_TI_LOCAL_FLAGS]
+	tst	x0, #_TLF_OOB
+	b.ne	work_done
+#endif
+ret_to_user_noirq:
 	ldr	x1, [tsk, #TSK_TI_FLAGS]
 	and	x2, x1, #_TIF_WORK_MASK
 	cbnz	x2, work_pending
@@ -1163,6 +1213,7 @@ NOKPROBE(cpu_switch_to)
  * This is how we return from a fork.
  */
 ENTRY(ret_from_fork)
+	enable_irq_if_pipelined
 	bl	schedule_tail
 	cbz	x19, 1f				// not a kernel thread
 	mov	x0, x20
diff --git a/arch/arm64/kernel/fpsimd.c b/arch/arm64/kernel/fpsimd.c
index 37d3912cfe06..d194bff98777 100644
--- a/arch/arm64/kernel/fpsimd.c
+++ b/arch/arm64/kernel/fpsimd.c
@@ -29,7 +29,7 @@
 #include <linux/stddef.h>
 #include <linux/sysctl.h>
 #include <linux/swab.h>
-
+#include <linux/dovetail.h>
 #include <asm/esr.h>
 #include <asm/fpsimd.h>
 #include <asm/cpufeature.h>
@@ -156,6 +156,42 @@ static void __get_cpu_fpsimd_context(void)
 	WARN_ON(busy);
 }
 
+static void __put_cpu_fpsimd_context(void)
+{
+	bool busy = __this_cpu_xchg(fpsimd_context_busy, false);
+
+	WARN_ON(!busy); /* No matching get_cpu_fpsimd_context()? */
+}
+
+#ifdef CONFIG_DOVETAIL
+
+#define get_cpu_fpsimd_context(__flags)			\
+	do {						\
+		(__flags) = hard_preempt_disable();	\
+		__get_cpu_fpsimd_context();		\
+	} while (0)
+
+#define put_cpu_fpsimd_context(__flags)		\
+	do {					\
+		__put_cpu_fpsimd_context();	\
+		hard_preempt_enable(__flags);	\
+	} while (0)
+
+void fpsimd_restore_current_oob(void)
+{
+	/*
+	 * Restore the fpsimd context for the current task as it
+	 * resumes from dovetail_context_switch(), which always happen
+	 * on the out-of-band stage. Skip this for kernel threads
+	 * which have no such context but always bear
+	 * TIF_FOREIGN_FPSTATE.
+	 */
+	if (current->mm)
+		fpsimd_restore_current_state();
+}
+
+#else
+
 /*
  * Claim ownership of the CPU FPSIMD context for use by the calling context.
  *
@@ -165,18 +201,12 @@ static void __get_cpu_fpsimd_context(void)
  * The double-underscore version must only be called if you know the task
  * can't be preempted.
  */
-static void get_cpu_fpsimd_context(void)
-{
-	preempt_disable();
-	__get_cpu_fpsimd_context();
-}
-
-static void __put_cpu_fpsimd_context(void)
-{
-	bool busy = __this_cpu_xchg(fpsimd_context_busy, false);
-
-	WARN_ON(!busy); /* No matching get_cpu_fpsimd_context()? */
-}
+#define get_cpu_fpsimd_context(__flags)			\
+	do {						\
+		preempt_disable();			\
+		__get_cpu_fpsimd_context();		\
+		(void)(__flags);			\
+	} while (0)
 
 /*
  * Release the CPU FPSIMD context.
@@ -185,11 +215,14 @@ static void __put_cpu_fpsimd_context(void)
  * previously called, with no call to put_cpu_fpsimd_context() in the
  * meantime.
  */
-static void put_cpu_fpsimd_context(void)
-{
-	__put_cpu_fpsimd_context();
-	preempt_enable();
-}
+#define put_cpu_fpsimd_context(__flags)			\
+	do {						\
+		__put_cpu_fpsimd_context();		\
+		preempt_enable();			\
+		(void)(__flags);			\
+	} while (0)
+
+#endif	/* !CONFIG_DOVETAIL */
 
 static bool have_cpu_fpsimd_context(void)
 {
@@ -269,7 +302,7 @@ static void sve_free(struct task_struct *task)
  */
 static void task_fpsimd_load(void)
 {
-	WARN_ON(!have_cpu_fpsimd_context());
+	WARN_ON(!hard_irqs_disabled() && !have_cpu_fpsimd_context());
 
 	if (system_supports_sve() && test_thread_flag(TIF_SVE))
 		sve_load_state(sve_pffr(&current->thread),
@@ -283,13 +316,13 @@ static void task_fpsimd_load(void)
  * Ensure FPSIMD/SVE storage in memory for the loaded context is up to
  * date with respect to the CPU registers.
  */
-static void fpsimd_save(void)
+static void __fpsimd_save(void)
 {
 	struct fpsimd_last_state_struct const *last =
 		this_cpu_ptr(&fpsimd_last_state);
 	/* set by fpsimd_bind_task_to_cpu() or fpsimd_bind_state_to_cpu() */
 
-	WARN_ON(!have_cpu_fpsimd_context());
+	WARN_ON(!hard_irqs_disabled() && !have_cpu_fpsimd_context());
 
 	if (!test_thread_flag(TIF_FOREIGN_FPSTATE)) {
 		if (system_supports_sve() && test_thread_flag(TIF_SVE)) {
@@ -311,6 +344,15 @@ static void fpsimd_save(void)
 	}
 }
 
+void fpsimd_save(void)
+{
+	unsigned long flags;
+
+	flags = hard_cond_local_irq_save();
+	__fpsimd_save();
+	hard_cond_local_irq_restore(flags);
+}
+
 /*
  * All vector length selection from userspace comes through here.
  * We're on a slow path, so some sanity-checks are included.
@@ -430,7 +472,7 @@ static void __fpsimd_to_sve(void *sst, struct user_fpsimd_state const *fst,
  * task->thread.uw.fpsimd_state must be up to date before calling this
  * function.
  */
-static void fpsimd_to_sve(struct task_struct *task)
+static void _fpsimd_to_sve(struct task_struct *task)
 {
 	unsigned int vq;
 	void *sst = task->thread.sve_state;
@@ -443,6 +485,15 @@ static void fpsimd_to_sve(struct task_struct *task)
 	__fpsimd_to_sve(sst, fst, vq);
 }
 
+static void fpsimd_to_sve(struct task_struct *task)
+{
+	unsigned long flags;
+
+	flags = hard_cond_local_irq_save();
+	_fpsimd_to_sve(task);
+	hard_cond_local_irq_restore(flags);
+}
+
 /*
  * Transfer the SVE state in task->thread.sve_state to
  * task->thread.uw.fpsimd_state.
@@ -461,15 +512,20 @@ static void sve_to_fpsimd(struct task_struct *task)
 	struct user_fpsimd_state *fst = &task->thread.uw.fpsimd_state;
 	unsigned int i;
 	__uint128_t const *p;
+	unsigned long flags;
 
 	if (!system_supports_sve())
 		return;
 
+	flags = hard_cond_local_irq_save();
+
 	vq = sve_vq_from_vl(task->thread.sve_vl);
 	for (i = 0; i < SVE_NUM_ZREGS; ++i) {
 		p = (__uint128_t const *)ZREG(sst, vq, i);
 		fst->vregs[i] = arm64_le128_to_cpu(*p);
 	}
+
+	hard_cond_local_irq_restore(flags);
 }
 
 #ifdef CONFIG_ARM64_SVE
@@ -570,6 +626,8 @@ void sve_sync_from_fpsimd_zeropad(struct task_struct *task)
 int sve_set_vector_length(struct task_struct *task,
 			  unsigned long vl, unsigned long flags)
 {
+	unsigned long irqflags = 0;
+
 	if (flags & ~(unsigned long)(PR_SVE_VL_INHERIT |
 				     PR_SVE_SET_VL_ONEXEC))
 		return -EINVAL;
@@ -607,9 +665,9 @@ int sve_set_vector_length(struct task_struct *task,
 	 * non-SVE thread.
 	 */
 	if (task == current) {
-		get_cpu_fpsimd_context();
+		get_cpu_fpsimd_context(irqflags);
 
-		fpsimd_save();
+		__fpsimd_save();
 	}
 
 	fpsimd_flush_task_state(task);
@@ -617,7 +675,7 @@ int sve_set_vector_length(struct task_struct *task,
 		sve_to_fpsimd(task);
 
 	if (task == current)
-		put_cpu_fpsimd_context();
+		put_cpu_fpsimd_context(irqflags);
 
 	/*
 	 * Force reallocation of task SVE state to the correct size
@@ -922,6 +980,10 @@ void fpsimd_release_task(struct task_struct *dead_task)
  */
 asmlinkage void do_sve_acc(unsigned int esr, struct pt_regs *regs)
 {
+	unsigned long flags;
+
+	oob_trap_notify(ARM64_TRAP_SVE, regs);
+
 	/* Even if we chose not to use SVE, the hardware could still trap: */
 	if (unlikely(!system_supports_sve()) || WARN_ON(is_compat_task())) {
 		force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
@@ -930,9 +992,9 @@ asmlinkage void do_sve_acc(unsigned int esr, struct pt_regs *regs)
 
 	sve_alloc(current);
 
-	get_cpu_fpsimd_context();
+	get_cpu_fpsimd_context(flags);
 
-	fpsimd_save();
+	__fpsimd_save();
 
 	/* Force ret_to_user to reload the registers: */
 	fpsimd_flush_task_state(current);
@@ -941,7 +1003,7 @@ asmlinkage void do_sve_acc(unsigned int esr, struct pt_regs *regs)
 	if (test_and_set_thread_flag(TIF_SVE))
 		WARN_ON(1); /* SVE access shouldn't have trapped */
 
-	put_cpu_fpsimd_context();
+	put_cpu_fpsimd_context(flags);
 }
 
 /*
@@ -973,6 +1035,8 @@ asmlinkage void do_fpsimd_exc(unsigned int esr, struct pt_regs *regs)
 			si_code = FPE_FLTRES;
 	}
 
+	oob_trap_notify(ARM64_TRAP_FPE, regs);
+
 	send_sig_fault(SIGFPE, si_code,
 		       (void __user *)instruction_pointer(regs),
 		       current);
@@ -981,14 +1045,17 @@ asmlinkage void do_fpsimd_exc(unsigned int esr, struct pt_regs *regs)
 void fpsimd_thread_switch(struct task_struct *next)
 {
 	bool wrong_task, wrong_cpu;
+	unsigned long flags;
 
 	if (!system_supports_fpsimd())
 		return;
 
+	flags = hard_cond_local_irq_save();
+
 	__get_cpu_fpsimd_context();
 
 	/* Save unsaved fpsimd state, if any: */
-	fpsimd_save();
+	__fpsimd_save();
 
 	/*
 	 * Fix up TIF_FOREIGN_FPSTATE to correctly describe next's
@@ -1003,16 +1070,19 @@ void fpsimd_thread_switch(struct task_struct *next)
 			       wrong_task || wrong_cpu);
 
 	__put_cpu_fpsimd_context();
+
+	hard_cond_local_irq_restore(flags);
 }
 
 void fpsimd_flush_thread(void)
 {
 	int vl, supported_vl;
+	unsigned long flags;
 
 	if (!system_supports_fpsimd())
 		return;
 
-	get_cpu_fpsimd_context();
+	get_cpu_fpsimd_context(flags);
 
 	fpsimd_flush_task_state(current);
 	memset(&current->thread.uw.fpsimd_state, 0,
@@ -1053,7 +1123,7 @@ void fpsimd_flush_thread(void)
 			current->thread.sve_vl_onexec = 0;
 	}
 
-	put_cpu_fpsimd_context();
+	put_cpu_fpsimd_context(flags);
 }
 
 /*
@@ -1062,12 +1132,14 @@ void fpsimd_flush_thread(void)
  */
 void fpsimd_preserve_current_state(void)
 {
+	unsigned long flags;
+
 	if (!system_supports_fpsimd())
 		return;
 
-	get_cpu_fpsimd_context();
-	fpsimd_save();
-	put_cpu_fpsimd_context();
+	get_cpu_fpsimd_context(flags);
+	__fpsimd_save();
+	put_cpu_fpsimd_context(flags);
 }
 
 /*
@@ -1108,19 +1180,29 @@ void fpsimd_bind_task_to_cpu(void)
 	}
 }
 
-void fpsimd_bind_state_to_cpu(struct user_fpsimd_state *st, void *sve_state,
+static void __fpsimd_bind_state_to_cpu(struct user_fpsimd_state *st, void *sve_state,
 			      unsigned int sve_vl)
 {
 	struct fpsimd_last_state_struct *last =
 		this_cpu_ptr(&fpsimd_last_state);
 
-	WARN_ON(!in_softirq() && !irqs_disabled());
-
 	last->st = st;
 	last->sve_state = sve_state;
 	last->sve_vl = sve_vl;
 }
 
+void fpsimd_bind_state_to_cpu(struct user_fpsimd_state *st, void *sve_state,
+			      unsigned int sve_vl)
+{
+	unsigned long flags;
+
+	WARN_ON(!in_softirq() && !irqs_disabled());
+
+	flags = hard_cond_local_irq_save();
+	__fpsimd_bind_state_to_cpu(st, sve_state, sve_vl);
+	hard_cond_local_irq_restore(flags);
+}
+
 /*
  * Load the userland FPSIMD state of 'current' from memory, but only if the
  * FPSIMD state already held in the registers is /not/ the most recent FPSIMD
@@ -1128,17 +1210,19 @@ void fpsimd_bind_state_to_cpu(struct user_fpsimd_state *st, void *sve_state,
  */
 void fpsimd_restore_current_state(void)
 {
+	unsigned long flags;
+
 	if (!system_supports_fpsimd())
 		return;
 
-	get_cpu_fpsimd_context();
+	get_cpu_fpsimd_context(flags);
 
 	if (test_and_clear_thread_flag(TIF_FOREIGN_FPSTATE)) {
 		task_fpsimd_load();
 		fpsimd_bind_task_to_cpu();
 	}
 
-	put_cpu_fpsimd_context();
+	put_cpu_fpsimd_context(flags);
 }
 
 /*
@@ -1148,21 +1232,23 @@ void fpsimd_restore_current_state(void)
  */
 void fpsimd_update_current_state(struct user_fpsimd_state const *state)
 {
+	unsigned long flags;
+
 	if (!system_supports_fpsimd())
 		return;
 
-	get_cpu_fpsimd_context();
+	get_cpu_fpsimd_context(flags);
 
 	current->thread.uw.fpsimd_state = *state;
 	if (system_supports_sve() && test_thread_flag(TIF_SVE))
-		fpsimd_to_sve(current);
+		_fpsimd_to_sve(current);
 
 	task_fpsimd_load();
 	fpsimd_bind_task_to_cpu();
 
 	clear_thread_flag(TIF_FOREIGN_FPSTATE);
 
-	put_cpu_fpsimd_context();
+	put_cpu_fpsimd_context(flags);
 }
 
 /*
@@ -1203,9 +1289,9 @@ static void fpsimd_flush_cpu_state(void)
  */
 void fpsimd_save_and_flush_cpu_state(void)
 {
-	WARN_ON(preemptible());
+	WARN_ON(!hard_irqs_disabled() && preemptible());
 	__get_cpu_fpsimd_context();
-	fpsimd_save();
+	__fpsimd_save();
 	fpsimd_flush_cpu_state();
 	__put_cpu_fpsimd_context();
 }
@@ -1231,18 +1317,23 @@ void fpsimd_save_and_flush_cpu_state(void)
  */
 void kernel_neon_begin(void)
 {
+	unsigned long flags;
+
 	if (WARN_ON(!system_supports_fpsimd()))
 		return;
 
 	BUG_ON(!may_use_simd());
 
-	get_cpu_fpsimd_context();
+	get_cpu_fpsimd_context(flags);
 
 	/* Save unsaved fpsimd state, if any: */
-	fpsimd_save();
+	__fpsimd_save();
 
 	/* Invalidate any task state remaining in the fpsimd regs: */
 	fpsimd_flush_cpu_state();
+
+	if (dovetailing())
+		hard_cond_local_irq_restore(flags);
 }
 EXPORT_SYMBOL(kernel_neon_begin);
 
@@ -1257,10 +1348,12 @@ EXPORT_SYMBOL(kernel_neon_begin);
  */
 void kernel_neon_end(void)
 {
+	unsigned long flags = hard_local_save_flags();
+
 	if (!system_supports_fpsimd())
 		return;
 
-	put_cpu_fpsimd_context();
+	put_cpu_fpsimd_context(flags);
 }
 EXPORT_SYMBOL(kernel_neon_end);
 
@@ -1350,9 +1443,13 @@ void __efi_fpsimd_end(void)
 static int fpsimd_cpu_pm_notifier(struct notifier_block *self,
 				  unsigned long cmd, void *v)
 {
+	unsigned long flags;
+
 	switch (cmd) {
 	case CPU_PM_ENTER:
+		flags = hard_cond_local_irq_save();
 		fpsimd_save_and_flush_cpu_state();
+		hard_cond_local_irq_restore(flags);
 		break;
 	case CPU_PM_EXIT:
 		break;
diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 04a327ccf84d..f5cac9986b59 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -12,6 +12,7 @@
 
 #include <linux/kernel_stat.h>
 #include <linux/irq.h>
+#include <linux/irq_pipeline.h>
 #include <linux/memory.h>
 #include <linux/smp.h>
 #include <linux/init.h>
@@ -21,6 +22,7 @@
 #include <linux/vmalloc.h>
 #include <asm/daifflags.h>
 #include <asm/vmap_stack.h>
+#include <asm/exception.h>
 
 unsigned long irq_err_count;
 
@@ -36,6 +38,16 @@ int arch_show_interrupts(struct seq_file *p, int prec)
 	return 0;
 }
 
+#ifdef CONFIG_IRQ_PIPELINE
+
+asmlinkage int __exception_irq_entry
+handle_arch_irq_pipelined(struct pt_regs *regs)
+{
+	return handle_irq_pipelined(regs);
+}
+
+#endif
+
 #ifdef CONFIG_VMAP_STACK
 static void init_irq_stacks(void)
 {
diff --git a/arch/arm64/kernel/irq_pipeline.c b/arch/arm64/kernel/irq_pipeline.c
new file mode 100644
index 000000000000..0c06922a5fad
--- /dev/null
+++ b/arch/arm64/kernel/irq_pipeline.c
@@ -0,0 +1,95 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2018 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/irq.h>
+#include <linux/irq_pipeline.h>
+
+/* irq_nesting tracks the interrupt nesting level for a CPU. */
+DEFINE_PER_CPU(int, irq_nesting);
+
+#ifdef CONFIG_SMP
+
+static struct irq_domain *sipic_domain;
+
+static void sipic_irq_noop(struct irq_data *data) { }
+
+static unsigned int sipic_irq_noop_ret(struct irq_data *data)
+{
+	return 0;
+}
+
+static struct irq_chip sipic_chip = {
+	.name		= "SIPIC",
+	.irq_startup	= sipic_irq_noop_ret,
+	.irq_shutdown	= sipic_irq_noop,
+	.irq_enable	= sipic_irq_noop,
+	.irq_disable	= sipic_irq_noop,
+	.flags		= IRQCHIP_PIPELINE_SAFE | IRQCHIP_SKIP_SET_WAKE,
+};
+
+static int sipic_irq_map(struct irq_domain *d, unsigned int irq,
+			irq_hw_number_t hwirq)
+{
+	irq_set_percpu_devid(irq);
+	irq_set_chip_and_handler(irq, &sipic_chip, handle_synthetic_irq);
+
+	return 0;
+}
+
+static struct irq_domain_ops sipic_domain_ops = {
+	.map	= sipic_irq_map,
+};
+
+static void create_ipi_domain(void)
+{
+	/*
+	 * Create an IRQ domain for mapping all IPIs (in-band and
+	 * out-of-band), with fixed sirq numbers starting from
+	 * OOB_IPI_BASE. The sirqs obtained can be injected into the
+	 * pipeline upon IPI receipt like other interrupts.
+	 */
+	sipic_domain = irq_domain_add_simple(NULL, NR_IPI + OOB_NR_IPI,
+					     OOB_IPI_BASE,
+					     &sipic_domain_ops, NULL);
+}
+
+void irq_pipeline_send_remote(unsigned int ipi,
+			      const struct cpumask *cpumask)
+{
+	unsigned int ipinr = ipi - OOB_IPI_BASE;
+	smp_cross_call(cpumask, ipinr);
+}
+EXPORT_SYMBOL_GPL(irq_pipeline_send_remote);
+
+#endif	/* CONFIG_SMP */
+
+void __init arch_irq_pipeline_init(void)
+{
+#ifdef CONFIG_SMP
+	create_ipi_domain();
+#endif
+}
+
+void arch_do_IRQ_pipelined(struct irq_desc *desc)
+{
+	struct pt_regs *regs = raw_cpu_ptr(&irq_pipeline.tick_regs);
+	unsigned int irq = irq_desc_get_irq(desc);
+
+#ifdef CONFIG_SMP
+	/*
+	 * Check for IPIs, handing them over to the specific dispatch
+	 * code.
+	 */
+	if (irq >= OOB_IPI_BASE &&
+	    irq < OOB_IPI_BASE + NR_IPI + OOB_NR_IPI) {
+		__handle_IPI(irq - OOB_IPI_BASE, regs);
+		return;
+	}
+#endif
+
+	__handle_domain_irq(NULL, irq, false, regs);
+}
diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 71f788cd2b18..2a285dc3edd5 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -123,7 +123,7 @@ void arch_cpu_idle(void)
 	 */
 	trace_cpu_idle_rcuidle(1, smp_processor_id());
 	cpu_do_idle();
-	local_irq_enable();
+	local_irq_enable_full();
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 
diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 21176d02e21a..46a82841a261 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -174,6 +174,8 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 	struct arch_hw_breakpoint *bkpt = counter_arch_bp(bp);
 	const char *desc = "Hardware breakpoint trap (ptrace)";
 
+	oob_trap_notify(ARM64_TRAP_DEBUG, regs);
+
 #ifdef CONFIG_COMPAT
 	if (is_compat_task()) {
 		int si_errno = 0;
diff --git a/arch/arm64/kernel/signal.c b/arch/arm64/kernel/signal.c
index dd2cdc0d5be2..eb4f695858e6 100644
--- a/arch/arm64/kernel/signal.c
+++ b/arch/arm64/kernel/signal.c
@@ -11,6 +11,7 @@
 #include <linux/errno.h>
 #include <linux/kernel.h>
 #include <linux/signal.h>
+#include <linux/irq_pipeline.h>
 #include <linux/personality.h>
 #include <linux/freezer.h>
 #include <linux/stddef.h>
@@ -899,24 +900,34 @@ static void do_signal(struct pt_regs *regs)
 asmlinkage void do_notify_resume(struct pt_regs *regs,
 				 unsigned long thread_flags)
 {
+	WARN_ON_ONCE(irq_pipeline_debug() &&
+		(irqs_disabled() || running_oob()));
+
 	/*
 	 * The assembly code enters us with IRQs off, but it hasn't
 	 * informed the tracing code of that for efficiency reasons.
 	 * Update the trace code with the current status.
 	 */
-	trace_hardirqs_off();
+	if (!irqs_pipelined())
+		trace_hardirqs_off();
 
 	do {
+		if (irqs_pipelined())
+			local_irq_disable();
+
 		/* Check valid user FS if needed */
 		addr_limit_user_check();
 
 		if (thread_flags & _TIF_NEED_RESCHED) {
 			/* Unmask Debug and SError for the next task */
-			local_daif_restore(DAIF_PROCCTX_NOIRQ);
+			local_daif_restore(irqs_pipelined() ?
+					DAIF_PROCCTX : DAIF_PROCCTX_NOIRQ);
 
 			schedule();
 		} else {
 			local_daif_restore(DAIF_PROCCTX);
+			if (irqs_pipelined())
+				local_irq_enable();
 
 			if (thread_flags & _TIF_UPROBE)
 				uprobe_notify_resume(regs);
@@ -934,9 +945,17 @@ asmlinkage void do_notify_resume(struct pt_regs *regs,
 				fpsimd_restore_current_state();
 		}
 
+		/*
+		 * CAUTION: we may have restored the fpsimd state for
+		 * current with no other opportunity to check for
+		 * _TIF_FOREIGN_FPSTATE until we are back running on
+		 * el0, so we must not take any interrupt until then,
+		 * otherwise we may end up resuming with some OOB
+		 * thread's fpsimd state.
+		 */
 		local_daif_mask();
 		thread_flags = READ_ONCE(current_thread_info()->flags);
-	} while (thread_flags & _TIF_WORK_MASK);
+	} while (inband_irq_pending() || (thread_flags & _TIF_WORK_MASK));
 }
 
 unsigned long __ro_after_init signal_minsigstksz;
diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index dc9fe879c279..1d4a59f0dcde 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -72,7 +72,7 @@ enum ipi_msg_type {
 	IPI_CPU_CRASH_STOP,
 	IPI_TIMER,
 	IPI_IRQ_WORK,
-	IPI_WAKEUP
+	IPI_WAKEUP,
 };
 
 #ifdef CONFIG_HOTPLUG_CPU
@@ -254,6 +254,7 @@ asmlinkage notrace void secondary_start_kernel(void)
 	complete(&cpu_running);
 
 	local_daif_restore(DAIF_PROCCTX);
+	local_irq_enable_full();
 
 	/*
 	 * OK, it's off to the idle thread for us
@@ -771,12 +772,68 @@ static const char *ipi_types[NR_IPI] __tracepoint_string = {
 	S(IPI_WAKEUP, "CPU wake-up interrupts"),
 };
 
-static void smp_cross_call(const struct cpumask *target, unsigned int ipinr)
+#ifdef CONFIG_IRQ_PIPELINE
+
+static DEFINE_PER_CPU(unsigned long, ipi_messages);
+
+static inline
+void send_IPI_message(const struct cpumask *target, unsigned int ipinr)
+{
+	unsigned int cpu, sgi;
+
+	if (ipinr < NR_IPI) {
+		/* regular in-band IPI (multiplexed over SGI0). */
+		trace_ipi_raise_rcuidle(target, ipi_types[ipinr]);
+		for_each_cpu(cpu, target)
+			set_bit(ipinr, &per_cpu(ipi_messages, cpu));
+		smp_mb();
+		sgi = 0;
+	} else	/* out-of-band IPI (SGI1-3). */
+		sgi = ipinr - NR_IPI + 1;
+
+	__smp_cross_call(target, sgi);
+}
+
+static inline
+void handle_IPI_pipelined(int sgi, struct pt_regs *regs)
+{
+	unsigned int ipinr, irq;
+	unsigned long *pmsg;
+
+	if (sgi) {		/* SGI1-3 */
+		irq = sgi + NR_IPI - 1 + OOB_IPI_BASE;
+		generic_pipeline_irq(irq, regs);
+		return;
+	}
+
+	/* In-band IPI (0..NR_IPI - 1) multiplexed over SGI0. */
+	pmsg = raw_cpu_ptr(&ipi_messages);
+	while (*pmsg) {
+		ipinr = ffs(*pmsg) - 1;
+		clear_bit(ipinr, pmsg);
+		irq = OOB_IPI_BASE + ipinr;
+		generic_pipeline_irq(irq, regs);
+	}
+}
+
+#else
+
+static inline
+void send_IPI_message(const struct cpumask *target, unsigned int ipinr)
 {
-	trace_ipi_raise(target, ipi_types[ipinr]);
 	__smp_cross_call(target, ipinr);
 }
 
+static inline void handle_IPI_pipelined(int ipinr, struct pt_regs *regs)
+{ }
+
+#endif /* CONFIG_IRQ_PIPELINE */
+
+void smp_cross_call(const struct cpumask *target, unsigned int ipinr)
+{
+	send_IPI_message(target, ipinr);
+}
+
 void show_ipi_list(struct seq_file *p, int prec)
 {
 	unsigned int cpu, i;
@@ -873,7 +930,7 @@ static void ipi_cpu_crash_stop(unsigned int cpu, struct pt_regs *regs)
 /*
  * Main handler for inter-processor interrupts
  */
-void handle_IPI(int ipinr, struct pt_regs *regs)
+void __handle_IPI(int ipinr, struct pt_regs *regs)
 {
 	unsigned int cpu = smp_processor_id();
 	struct pt_regs *old_regs = set_irq_regs(regs);
@@ -933,6 +990,18 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 		break;
 #endif
 
+#ifdef CONFIG_IRQ_PIPELINE
+	/*
+	 * In the unlikely event out-of-band IPIs have a in-band stage
+	 * handler.
+	 */
+	case NR_IPI ... NR_IPI + OOB_NR_IPI - 1:
+		irq_enter();
+		generic_handle_irq(OOB_IPI_BASE + ipinr);
+		irq_exit();
+		break;
+#endif
+
 	default:
 		pr_crit("CPU%u: Unknown IPI message 0x%x\n", cpu, ipinr);
 		break;
@@ -943,6 +1012,14 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 	set_irq_regs(old_regs);
 }
 
+void handle_IPI(int ipinr, struct pt_regs *regs)
+{
+	if (irqs_pipelined())
+		handle_IPI_pipelined(ipinr, regs);
+	else
+		__handle_IPI(ipinr, regs);
+}
+
 void smp_send_reschedule(int cpu)
 {
 	smp_cross_call(cpumask_of(cpu), IPI_RESCHEDULE);
diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 871c739f060a..15ce595fee11 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -93,13 +93,30 @@ static void cortex_a76_erratum_1463225_svc_handler(void) { }
 static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
 			   const syscall_fn_t syscall_table[])
 {
-	unsigned long flags = current_thread_info()->flags;
+	bool stalled = irqs_pipelined() && running_inband() && irqs_disabled();
+	struct thread_info *ti = current_thread_info();
+	unsigned long flags = ti->flags;
+	int ret;
 
 	regs->orig_x0 = regs->regs[0];
 	regs->syscallno = scno;
 
 	cortex_a76_erratum_1463225_svc_handler();
 	local_daif_restore(DAIF_PROCCTX);
+
+	if (stalled)
+		local_irq_enable();
+
+	ret = pipeline_syscall(ti, scno, regs);
+	if (ret) {
+		if (stalled)
+			local_irq_disable();
+		local_daif_mask();
+		if (ret > 0 || !has_syscall_work(flags))
+			trace_hardirqs_on();
+		return;
+	}
+
 	user_exit();
 
 	if (has_syscall_work(flags)) {
@@ -122,6 +139,8 @@ static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
 		local_daif_mask();
 		flags = current_thread_info()->flags;
 		if (!has_syscall_work(flags)) {
+			if (stalled)
+				local_irq_disable();
 			/*
 			 * We're off to userspace, where interrupts are
 			 * always enabled after we restore the flags from
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 34739e80211b..23ad284404bf 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -169,7 +169,7 @@ static int __die(const char *str, int err, struct pt_regs *regs)
 	return ret;
 }
 
-static DEFINE_RAW_SPINLOCK(die_lock);
+static DEFINE_HARD_SPINLOCK(die_lock);
 
 /*
  * This function is protected against re-entrancy.
@@ -281,7 +281,7 @@ void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 }
 
 static LIST_HEAD(undef_hook);
-static DEFINE_RAW_SPINLOCK(undef_lock);
+static DEFINE_HARD_SPINLOCK(undef_lock);
 
 void register_undef_hook(struct undef_hook *hook)
 {
@@ -395,6 +395,8 @@ void arm64_notify_segfault(unsigned long addr)
 
 asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 {
+	oob_trap_notify(ARM64_TRAP_UNDI, regs);
+
 	/* check for AArch32 breakpoint instructions */
 	if (!aarch32_break_handler(regs))
 		return;
@@ -797,6 +799,8 @@ asmlinkage void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr)
 {
 	void __user *pc = (void __user *)instruction_pointer(regs);
 
+	oob_trap_notify(ARM64_TRAP_UNDSE, regs);
+
 	current->thread.fault_address = 0;
 	current->thread.fault_code = esr;
 
diff --git a/arch/arm64/mm/context.c b/arch/arm64/mm/context.c
index b5e329fde2dd..addb7d14e589 100644
--- a/arch/arm64/mm/context.c
+++ b/arch/arm64/mm/context.c
@@ -17,7 +17,7 @@
 #include <asm/tlbflush.h>
 
 static u32 asid_bits;
-static DEFINE_RAW_SPINLOCK(cpu_asid_lock);
+static DEFINE_HARD_SPINLOCK(cpu_asid_lock);
 
 static atomic64_t asid_generation;
 static unsigned long *asid_map;
@@ -184,6 +184,9 @@ void check_and_switch_context(struct mm_struct *mm, unsigned int cpu)
 {
 	unsigned long flags;
 	u64 asid, old_active_asid;
+	bool need_flush;
+
+	WARN_ON_ONCE(dovetail_debug() && !hard_irqs_disabled());
 
 	if (system_supports_cnp())
 		cpu_set_reserved_ttbr0();
@@ -219,12 +222,14 @@ void check_and_switch_context(struct mm_struct *mm, unsigned int cpu)
 		atomic64_set(&mm->context.id, asid);
 	}
 
-	if (cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending))
-		local_flush_tlb_all();
+	need_flush = cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending);
 
 	atomic64_set(&per_cpu(active_asids, cpu), asid);
 	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);
 
+	if (need_flush)
+		local_flush_tlb_all();
+
 switch_mm_fastpath:
 
 	arm64_apply_bp_hardening();
diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 9fc6db0bcbad..427936ce957a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -13,6 +13,7 @@
 #include <linux/signal.h>
 #include <linux/mm.h>
 #include <linux/hardirq.h>
+#include <linux/irq_pipeline.h>
 #include <linux/init.h>
 #include <linux/kprobes.h>
 #include <linux/uaccess.h>
@@ -38,6 +39,7 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/traps.h>
+#include <asm/dovetail.h>
 
 struct fault_info {
 	int	(*fn)(unsigned long addr, unsigned int esr,
@@ -60,6 +62,90 @@ static inline const struct fault_info *esr_to_debug_fault_info(unsigned int esr)
 	return debug_fault_info + DBG_ESR_EVT(esr);
 }
 
+#ifdef CONFIG_DOVETAIL
+#define fault_entry(__exception, __regs)	__fault_entry(__exception, __regs)
+#else
+/* Do not depend on trap id. definitions from asm/dovetail.h */
+#define fault_entry(__exception, __regs)	__fault_entry(-1, __regs)
+#endif
+
+#ifdef CONFIG_IRQ_PIPELINE
+/*
+ * We need to synchronize the virtual interrupt state with the hard
+ * interrupt state we received on entry, then turn hardirqs back on to
+ * allow code which does not require strict serialization to be
+ * preempted by an out-of-band activity.
+ *
+ * TRACING: the entry code already told lockdep and tracers about the
+ * hard interrupt state on entry to fault handlers, so no need to
+ * reflect changes to that state via calls to trace_hardirqs_*
+ * helpers. From the main kernel's point of view, there is no change.
+ */
+
+static inline
+unsigned long __fault_entry(unsigned int exception, struct pt_regs *regs)
+{
+	unsigned long flags;
+	int nosync = 1;
+
+	/*
+	 * Do not propagate SEA traps to the out-of-band handler in
+	 * NMI mode, there is nothing it could do about it.
+	 */
+	if (likely(exception != ARM64_TRAP_SEA || interrupts_enabled(regs)))
+		oob_trap_notify(exception, regs);
+
+	flags = hard_local_irq_save();
+
+	if (hard_irqs_disabled_flags(flags))
+		nosync = test_and_set_stage_bit(STAGE_STALL_BIT,
+					this_inband_staged());
+	hard_local_irq_enable();
+
+	return irqs_merge_flags(flags, nosync);
+}
+
+static inline void fault_exit(unsigned long combo)
+{
+	unsigned long flags;
+	int nosync;
+
+	WARN_ON_ONCE(irq_pipeline_debug() && hard_irqs_disabled());
+
+	/*
+	 * '!nosync' here means that we had to turn on the stall bit
+	 * in fault_entry() to mirror the hard interrupt state,
+	 * because hard irqs were off but the stall bit was
+	 * clear. Conversely, nosync in fault_exit() means that the
+	 * stall bit state currently reflects the hard interrupt state
+	 * we received on fault_entry().
+	 *
+	 * No hard_local_irq_restore() below, ever, but
+	 * hard_local_irq_{enable|disable}() exclusively. See
+	 * restore_stage() for an explanation.
+	 */
+	flags = irqs_split_flags(combo, &nosync);
+	if (!nosync) {
+		hard_local_irq_disable();
+		clear_stage_bit(STAGE_STALL_BIT, this_inband_staged());
+		if (!hard_irqs_disabled_flags(flags))
+			hard_local_irq_enable();
+	} else if (hard_irqs_disabled_flags(flags))
+		hard_local_irq_disable();
+}
+
+#else	/* !CONFIG_IRQ_PIPELINE */
+
+static inline
+unsigned long __fault_entry(unsigned int exception, struct pt_regs *regs)
+{
+	return 0;
+}
+
+static inline void fault_exit(unsigned long x) { }
+
+#endif	/* !CONFIG_IRQ_PIPELINE */
+
 static void data_abort_decode(unsigned int esr)
 {
 	pr_alert("Data abort info:\n");
@@ -286,6 +372,8 @@ static bool __kprobes is_spurious_el1_translation_fault(unsigned long addr,
 static void die_kernel_fault(const char *msg, unsigned long addr,
 			     unsigned int esr, struct pt_regs *regs)
 {
+	irq_pipeline_oops();
+
 	bust_spinlocks(1);
 
 	pr_alert("Unable to handle kernel %s at virtual address %016lx\n", msg,
@@ -394,11 +482,19 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 	 */
 	if (user_mode(regs)) {
 		const struct fault_info *inf = esr_to_fault_info(esr);
+		unsigned long irqflags;
 
+		irqflags = fault_entry(ARM64_TRAP_ACCESS, regs);
 		set_thread_esr(addr, esr);
 		arm64_force_sig_fault(inf->sig, inf->code, (void __user *)addr,
 				      inf->name);
+		fault_exit(irqflags);
 	} else {
+		/*
+		 * irq_pipeline: kernel faults are either quickly
+		 * recoverable via fixup, or lethal. In both cases, we
+		 * can skip the interrupt state synchronization.
+		 */
 		__do_kernel_fault(addr, esr, regs);
 	}
 }
@@ -454,11 +550,13 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	const struct fault_info *inf;
 	struct mm_struct *mm = current->mm;
 	vm_fault_t fault, major = 0;
-	unsigned long vm_flags = VM_READ | VM_WRITE;
+	unsigned long vm_flags = VM_READ | VM_WRITE, irqflags;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
+	irqflags = fault_entry(ARM64_TRAP_ACCESS, regs);
+
 	if (kprobe_page_fault(regs, esr))
-		return 0;
+		goto out;
 
 	/*
 	 * If we're in an interrupt or have no user context, we must not take
@@ -532,7 +630,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		if (fatal_signal_pending(current)) {
 			if (!user_mode(regs))
 				goto no_context;
-			return 0;
+			goto out;
 		}
 
 		/*
@@ -568,7 +666,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 				      addr);
 		}
 
-		return 0;
+		goto out;
 	}
 
 	/*
@@ -585,7 +683,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		 * oom-killed).
 		 */
 		pagefault_out_of_memory();
-		return 0;
+		goto out;
 	}
 
 	inf = esr_to_fault_info(esr);
@@ -616,18 +714,22 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 				      (void __user *)addr,
 				      inf->name);
 	}
+out:
+	fault_exit(irqflags);
 
 	return 0;
 
 no_context:
 	__do_kernel_fault(addr, esr, regs);
-	return 0;
+	goto out;
 }
 
 static int __kprobes do_translation_fault(unsigned long addr,
 					  unsigned int esr,
 					  struct pt_regs *regs)
 {
+	/* irq_pipeline: hard irqs may be on upon el1_sync. */
+
 	if (is_ttbr0_addr(addr))
 		return do_page_fault(addr, esr, regs);
 
@@ -650,8 +752,11 @@ static int do_bad(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
 	const struct fault_info *inf;
+	unsigned long irqflags;
 	void __user *siaddr;
 
+	irqflags = fault_entry(ARM64_TRAP_SEA, regs);
+
 	inf = esr_to_fault_info(esr);
 
 	/*
@@ -666,6 +771,8 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 		siaddr  = (void __user *)addr;
 	arm64_notify_die(inf->name, regs, inf->sig, inf->code, siaddr, esr);
 
+	fault_exit(irqflags);
+
 	return 0;
 }
 
@@ -740,10 +847,13 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 					 struct pt_regs *regs)
 {
 	const struct fault_info *inf = esr_to_fault_info(esr);
+	unsigned long irqflags;
 
 	if (!inf->fn(addr, esr, regs))
 		return;
 
+	irqflags = fault_entry(ARM64_TRAP_ABRT, regs);
+
 	if (!user_mode(regs)) {
 		pr_alert("Unhandled fault at 0x%016lx\n", addr);
 		mem_abort_decode(esr);
@@ -752,11 +862,17 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 
 	arm64_notify_die(inf->name, regs,
 			 inf->sig, inf->code, (void __user *)addr, esr);
+
+	fault_exit(irqflags);
 }
 
 asmlinkage void __exception do_el0_irq_bp_hardening(void)
 {
-	/* PC has already been checked in entry.S */
+	/*
+	 * PC has already been checked in entry.S.
+	 * irq_pipeline: assume that branch predictor hardening
+	 * workarounds can safely run on any stage.
+	 */
 	arm64_apply_bp_hardening();
 }
 
@@ -781,14 +897,20 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 					   unsigned int esr,
 					   struct pt_regs *regs)
 {
+	unsigned long irqflags;
+
 	if (user_mode(regs)) {
 		if (!is_ttbr0_addr(instruction_pointer(regs)))
 			arm64_apply_bp_hardening();
 		local_daif_restore(DAIF_PROCCTX);
 	}
 
+	irqflags = fault_entry(ARM64_TRAP_ABRT, regs);
+
 	arm64_notify_die("SP/PC alignment exception", regs,
 			 SIGBUS, BUS_ADRALN, (void __user *)addr, esr);
+
+	fault_exit(irqflags);
 }
 
 int __init early_brk64(unsigned long addr, unsigned int esr,
@@ -905,6 +1027,7 @@ asmlinkage void __exception do_debug_exception(unsigned long addr_if_watchpoint,
 {
 	const struct fault_info *inf = esr_to_debug_fault_info(esr);
 	unsigned long pc = instruction_pointer(regs);
+	unsigned long irqflags;
 
 	if (cortex_a76_erratum_1463225_debug_handler(regs))
 		return;
@@ -914,11 +1037,15 @@ asmlinkage void __exception do_debug_exception(unsigned long addr_if_watchpoint,
 	if (user_mode(regs) && !is_ttbr0_addr(pc))
 		arm64_apply_bp_hardening();
 
+	irqflags = fault_entry(ARM64_TRAP_DEBUG, regs);
+
 	if (inf->fn(addr_if_watchpoint, esr, regs)) {
 		arm64_notify_die(inf->name, regs,
 				 inf->sig, inf->code, (void __user *)pc, esr);
 	}
 
+	fault_exit(irqflags);
+
 	debug_exception_exit(regs);
 }
 NOKPROBE_SYMBOL(do_debug_exception);
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 8ef85139553f..f63a1ab891a1 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -27,6 +27,8 @@ config X86_64
 	select ARCH_SUPPORTS_INT128
 	select ARCH_USE_CMPXCHG_LOCKREF
 	select HAVE_ARCH_SOFT_DIRTY
+	select HAVE_IRQ_PIPELINE
+	select HAVE_DOVETAIL
 	select MODULES_USE_ELF_RELA
 	select NEED_DMA_MAP_STATE
 	select SWIOTLB
@@ -190,6 +192,7 @@ config X86
 	select HAVE_MOD_ARCH_SPECIFIC
 	select HAVE_MOVE_PMD
 	select HAVE_NMI
+	select HAVE_PERCPU_PREEMPT_COUNT
 	select HAVE_OPROFILE
 	select HAVE_OPTPROBES
 	select HAVE_PCSPKR_PLATFORM
@@ -861,6 +864,7 @@ config ACRN_GUEST
 
 endif #HYPERVISOR_GUEST
 
+source "kernel/Kconfig.dovetail"
 source "arch/x86/Kconfig.cpu"
 
 config HPET_TIMER
diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c
index 3f8e22615812..1fa9aed83746 100644
--- a/arch/x86/entry/common.c
+++ b/arch/x86/entry/common.c
@@ -17,6 +17,7 @@
 #include <linux/tracehook.h>
 #include <linux/audit.h>
 #include <linux/seccomp.h>
+#include <linux/unistd.h>
 #include <linux/signal.h>
 #include <linux/export.h>
 #include <linux/context_tracking.h>
@@ -41,6 +42,8 @@
 /* Called on entry from user mode with IRQs off. */
 __visible inline void enter_from_user_mode(void)
 {
+	if (irqs_pipelined() && (!running_inband() || irqs_disabled()))
+		return;
 	CT_WARN_ON(ct_state() != CONTEXT_USER);
 	user_exit_irqoff();
 }
@@ -48,6 +51,22 @@ __visible inline void enter_from_user_mode(void)
 static inline void enter_from_user_mode(void) {}
 #endif
 
+#ifdef CONFIG_IRQ_PIPELINE
+#define disable_local_irqs()	do {	\
+	hard_local_irq_disable();	\
+	trace_hardirqs_off();		\
+} while (0)
+#define enable_local_irqs()	do {	\
+	trace_hardirqs_on();		\
+	hard_local_irq_enable();	\
+} while (0)
+#define check_irqs_disabled()	hard_irqs_disabled()
+#else
+#define disable_local_irqs()	local_irq_disable()
+#define enable_local_irqs()	local_irq_enable()
+#define check_irqs_disabled()	irqs_disabled()
+#endif
+
 static void do_audit_syscall_entry(struct pt_regs *regs, u32 arch)
 {
 #ifdef CONFIG_X86_64
@@ -143,7 +162,7 @@ static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)
 	 */
 	while (true) {
 		/* We have work to do. */
-		local_irq_enable();
+		enable_local_irqs();
 
 		if (cached_flags & _TIF_NEED_RESCHED)
 			schedule();
@@ -168,7 +187,7 @@ static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)
 			fire_user_return_notifiers();
 
 		/* Disable IRQs and retry */
-		local_irq_disable();
+		disable_local_irqs();
 
 		cached_flags = READ_ONCE(current_thread_info()->flags);
 
@@ -183,6 +202,8 @@ __visible inline void prepare_exit_to_usermode(struct pt_regs *regs)
 	struct thread_info *ti = current_thread_info();
 	u32 cached_flags;
 
+	WARN_ON_ONCE(irq_pipeline_debug() && running_oob());
+
 	addr_limit_user_check();
 
 	lockdep_assert_irqs_disabled();
@@ -259,7 +280,7 @@ __visible inline void syscall_return_slowpath(struct pt_regs *regs)
 
 	if (IS_ENABLED(CONFIG_PROVE_LOCKING) &&
 	    WARN(irqs_disabled(), "syscall %ld left IRQs disabled", regs->orig_ax))
-		local_irq_enable();
+		enable_local_irqs();
 
 	rseq_syscall(regs);
 
@@ -267,10 +288,13 @@ __visible inline void syscall_return_slowpath(struct pt_regs *regs)
 	 * First do one-time work.  If these work items are enabled, we
 	 * want to run them exactly once per syscall exit with IRQs on.
 	 */
-	if (unlikely(cached_flags & SYSCALL_EXIT_WORK_FLAGS))
+
+	if (unlikely(cached_flags & SYSCALL_EXIT_WORK_FLAGS) &&
+		(!IS_ENABLED(CONFIG_IRQ_PIPELINE) ||
+			syscall_get_nr(current, regs) < NR_syscalls))
 		syscall_slow_exit_work(regs, cached_flags);
 
-	local_irq_disable();
+	disable_local_irqs();
 	prepare_exit_to_usermode(regs);
 }
 
@@ -278,10 +302,20 @@ __visible inline void syscall_return_slowpath(struct pt_regs *regs)
 __visible void do_syscall_64(unsigned long nr, struct pt_regs *regs)
 {
 	struct thread_info *ti;
+	int ret;
 
 	enter_from_user_mode();
-	local_irq_enable();
+	enable_local_irqs();
 	ti = current_thread_info();
+
+	ret = pipeline_syscall(ti, nr, regs);
+	if (ret > 0) {
+		disable_local_irqs();
+		return;
+	}
+	if (ret < 0)
+		goto done;
+
 	if (READ_ONCE(ti->flags) & _TIF_WORK_SYSCALL_ENTRY)
 		nr = syscall_trace_enter(regs);
 
@@ -296,12 +330,38 @@ __visible void do_syscall_64(unsigned long nr, struct pt_regs *regs)
 		regs->ax = x32_sys_call_table[nr](regs);
 #endif
 	}
-
+done:
 	syscall_return_slowpath(regs);
 }
 #endif
 
 #if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)
+
+#if defined(CONFIG_DOVETAIL) && defined(CONFIG_X86_64)
+static inline int pipeline_syscall32(struct thread_info *ti,
+				unsigned long nr, struct pt_regs *regs)
+{
+	struct pt_regs regs64 = *regs;
+	int ret;
+
+	regs64.di = (unsigned int)regs->bx;
+	regs64.si = (unsigned int)regs->cx;
+	regs64.r10 = (unsigned int)regs->si;
+	regs64.r8 = (unsigned int)regs->di;
+	regs64.r9 = (unsigned int)regs->bp;
+	ret = pipeline_syscall(ti, nr, &regs64);
+	regs->ax = (unsigned int)regs64.ax;
+
+	return ret;
+}
+#else
+static inline int pipeline_syscall32(struct thread_info *ti,
+				   unsigned long nr, struct pt_regs *regs)
+{
+	return 0;
+}
+#endif /* DOVETAIL && X86_64 */
+
 /*
  * Does a 32-bit syscall.  Called with IRQs on in CONTEXT_KERNEL.  Does
  * all entry and exit work and returns with IRQs off.  This function is
@@ -312,11 +372,20 @@ static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)
 {
 	struct thread_info *ti = current_thread_info();
 	unsigned int nr = (unsigned int)regs->orig_ax;
+	int ret;
 
 #ifdef CONFIG_IA32_EMULATION
 	ti->status |= TS_COMPAT;
 #endif
 
+	ret = pipeline_syscall32(ti, nr, regs);
+	if (ret > 0) {
+		disable_local_irqs();
+		return;
+	}
+	if (ret < 0)
+		goto done;
+
 	if (READ_ONCE(ti->flags) & _TIF_WORK_SYSCALL_ENTRY) {
 		/*
 		 * Subtlety here: if ptrace pokes something larger than
@@ -345,6 +414,7 @@ static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)
 #endif /* CONFIG_IA32_EMULATION */
 	}
 
+done:
 	syscall_return_slowpath(regs);
 }
 
@@ -352,7 +422,7 @@ static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)
 __visible void do_int80_syscall_32(struct pt_regs *regs)
 {
 	enter_from_user_mode();
-	local_irq_enable();
+	enable_local_irqs();
 	do_syscall_32_irqs_on(regs);
 }
 
@@ -376,7 +446,7 @@ __visible long do_fast_syscall_32(struct pt_regs *regs)
 
 	enter_from_user_mode();
 
-	local_irq_enable();
+	enable_local_irqs();
 
 	/* Fetch EBP from where the vDSO stashed it. */
 	if (
@@ -394,7 +464,7 @@ __visible long do_fast_syscall_32(struct pt_regs *regs)
 		) {
 
 		/* User code screwed up. */
-		local_irq_disable();
+		disable_local_irqs();
 		regs->ax = -EFAULT;
 		prepare_exit_to_usermode(regs);
 		return 0;	/* Keep it simple: use IRET. */
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index b7c3ea4cb19d..05cdb8cf2bcd 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -57,7 +57,7 @@ END(native_usergs_sysret64)
 #ifdef CONFIG_TRACE_IRQFLAGS
 	btl	$9, \flags		/* interrupts off? */
 	jnc	1f
-	TRACE_IRQS_ON
+	TRACE_IRQS_ON_PIPELINED
 1:
 #endif
 .endm
@@ -87,7 +87,7 @@ END(native_usergs_sysret64)
 
 .macro TRACE_IRQS_ON_DEBUG
 	call	debug_stack_set_zero
-	TRACE_IRQS_ON
+	TRACE_IRQS_ON_PIPELINED
 	call	debug_stack_reset
 .endm
 
@@ -332,6 +332,7 @@ END(__switch_to_asm)
  */
 ENTRY(ret_from_fork)
 	UNWIND_HINT_EMPTY
+	ENABLE_INTERRUPTS_IF_PIPELINED
 	movq	%rax, %rdi
 	call	schedule_tail			/* rdi: 'prev' task parameter */
 
@@ -405,6 +406,9 @@ END(spurious_entries_start)
  * Requires kernel GSBASE.
  *
  * The invariant is that, if irq_count != -1, then the IRQ stack is in use.
+ *
+ * irq_pipeline: caller must ensure that hard irqs are off upon
+ * ENTER/LEAVE_IRQ_STACK so that irq_count is always consistent.
  */
 .macro ENTER_IRQ_STACK regs=1 old_rsp save_ret=0
 	DEBUG_ENTRY_ASSERT_IRQS_OFF
@@ -575,8 +579,14 @@ ENTRY(interrupt_entry)
 
 1:
 	ENTER_IRQ_STACK old_rsp=%rdi save_ret=1
-	/* We entered an interrupt context - irqs are off: */
+	/*
+	 * We entered an interrupt context - irqs are off unless
+	 * pipelining is enabled, in which case we defer tracing until
+	 * sync_current_irq_stage() is called from the inband stage.
+	 */
+#ifndef CONFIG_IRQ_PIPELINE
 	TRACE_IRQS_OFF
+#endif
 
 	ret
 END(interrupt_entry)
@@ -604,7 +614,17 @@ common_interrupt:
 	addq	$-0x80, (%rsp)			/* Adjust vector to [-256, -1] range */
 	call	interrupt_entry
 	UNWIND_HINT_REGS indirect=1
+#ifdef CONFIG_IRQ_PIPELINE
+	call	handle_arch_irq_pipelined
+	testl	%eax, %eax
+	jnz	ret_from_intr
+	LEAVE_IRQ_STACK
+	testb	$3, CS(%rsp)
+	jz	retint_kernel_early
+	jmp	retint_user_early
+#else	
 	call	do_IRQ	/* rdi points to pt_regs */
+#endif	
 	/* 0(%rsp): old RSP */
 ret_from_intr:
 	DISABLE_INTERRUPTS(CLBR_ANY)
@@ -619,6 +639,7 @@ ret_from_intr:
 GLOBAL(retint_user)
 	mov	%rsp,%rdi
 	call	prepare_exit_to_usermode
+retint_user_early:
 	TRACE_IRQS_IRETQ
 
 GLOBAL(swapgs_restore_regs_and_return_to_usermode)
@@ -676,7 +697,8 @@ retint_kernel:
 #endif
 	/*
 	 * The iretq could re-enable interrupts:
-	 */
+	*/
+retint_kernel_early:
 	TRACE_IRQS_IRETQ
 
 GLOBAL(restore_regs_and_return_to_kernel)
@@ -794,7 +816,29 @@ _ASM_NOKPROBE(common_interrupt)
 
 /*
  * APIC interrupts.
- */
+*/
+#ifdef CONFIG_IRQ_PIPELINE
+.macro apicinterrupt2 num sym
+ENTRY(\sym)
+	UNWIND_HINT_IRET_REGS
+	pushq	$~(\num)
+.Lcommon_\sym:
+	call	interrupt_entry
+	UNWIND_HINT_REGS indirect=1
+	call	handle_arch_irq_pipelined /* rdi points to pt_regs */
+	testl	%eax, %eax
+	jnz	ret_from_intr
+	LEAVE_IRQ_STACK
+	testb	$3, CS(%rsp)
+	jz	retint_kernel_early
+	jmp	retint_user_early
+END(\sym)
+.endm
+.macro apicinterrupt3 num sym do_sym
+apicinterrupt2 \num \sym
+_ASM_NOKPROBE(\sym)
+.endm
+#else	
 .macro apicinterrupt3 num sym do_sym
 ENTRY(\sym)
 	UNWIND_HINT_IRET_REGS
@@ -807,7 +851,8 @@ ENTRY(\sym)
 END(\sym)
 _ASM_NOKPROBE(\sym)
 .endm
-
+#endif
+	
 /* Make sure APIC interrupt handlers end up in the irqentry section: */
 #define PUSH_SECTION_IRQENTRY	.pushsection .irqentry.text, "ax"
 #define POP_SECTION_IRQENTRY	.popsection
@@ -852,8 +897,15 @@ apicinterrupt THERMAL_APIC_VECTOR		thermal_interrupt		smp_thermal_interrupt
 apicinterrupt CALL_FUNCTION_SINGLE_VECTOR	call_function_single_interrupt	smp_call_function_single_interrupt
 apicinterrupt CALL_FUNCTION_VECTOR		call_function_interrupt		smp_call_function_interrupt
 apicinterrupt RESCHEDULE_VECTOR			reschedule_interrupt		smp_reschedule_interrupt
+#ifdef CONFIG_IRQ_PIPELINE
+apicinterrupt2 RESCHEDULE_OOB_VECTOR		reschedule_oob_interrupt
+#endif
 #endif
 
+#ifdef CONFIG_IRQ_PIPELINE
+apicinterrupt2 TIMER_OOB_VECTOR			timer_oob_interrupt
+#endif
+	
 apicinterrupt ERROR_APIC_VECTOR			error_interrupt			smp_error_interrupt
 apicinterrupt SPURIOUS_APIC_VECTOR		spurious_interrupt		smp_spurious_interrupt
 
@@ -1078,9 +1130,13 @@ EXPORT_SYMBOL(native_load_gs_index)
 ENTRY(do_softirq_own_stack)
 	pushq	%rbp
 	mov	%rsp, %rbp
+	DISABLE_INTERRUPTS_IF_PIPELINED
 	ENTER_IRQ_STACK regs=0 old_rsp=%r11
+	ENABLE_INTERRUPTS_IF_PIPELINED
 	call	__do_softirq
+	DISABLE_INTERRUPTS_IF_PIPELINED
 	LEAVE_IRQ_STACK regs=0
+	ENABLE_INTERRUPTS_IF_PIPELINED
 	leaveq
 	ret
 ENDPROC(do_softirq_own_stack)
@@ -1370,6 +1426,17 @@ ENTRY(error_exit)
 	UNWIND_HINT_REGS
 	DISABLE_INTERRUPTS(CLBR_ANY)
 	TRACE_IRQS_OFF
+#ifdef CONFIG_IRQ_PIPELINE
+	testl	$oob_preempt_mask, PER_CPU_VAR(__preempt_count)
+	jnz	1f
+	testl	$inband_stall_mask, PER_CPU_VAR(irq_pipeline + inband_stage_status)
+	jz	2f
+1:
+	testb	$3, CS(%rsp)
+	jz	retint_kernel_early
+	jmp	retint_user_early
+2:
+#endif
 	testb	$3, CS(%rsp)
 	jz	retint_kernel
 	jmp	retint_user
diff --git a/arch/x86/entry/thunk_64.S b/arch/x86/entry/thunk_64.S
index ea5c4167086c..30ba1139568a 100644
--- a/arch/x86/entry/thunk_64.S
+++ b/arch/x86/entry/thunk_64.S
@@ -39,6 +39,7 @@
 
 #ifdef CONFIG_TRACE_IRQFLAGS
 	THUNK trace_hardirqs_on_thunk,trace_hardirqs_on_caller,1
+	THUNK trace_hardirqs_on_pipelined_thunk,trace_hardirqs_on_pipelined,1
 	THUNK trace_hardirqs_off_thunk,trace_hardirqs_off_caller,1
 #endif
 
diff --git a/arch/x86/include/asm/apic.h b/arch/x86/include/asm/apic.h
index 2ebc17d9c72c..82a200da3733 100644
--- a/arch/x86/include/asm/apic.h
+++ b/arch/x86/include/asm/apic.h
@@ -443,7 +443,7 @@ static inline void apic_set_eoi_write(void (*eoi_write)(u32 reg, u32 v)) {}
 
 extern void apic_ack_irq(struct irq_data *data);
 
-static inline void ack_APIC_irq(void)
+static inline void __ack_APIC_irq(void)
 {
 	/*
 	 * ack_APIC_irq() actually gets compiled as a single instruction
@@ -452,6 +452,12 @@ static inline void ack_APIC_irq(void)
 	apic_eoi();
 }
 
+static inline void ack_APIC_irq(void)
+{
+	if (!irqs_pipelined())
+		__ack_APIC_irq();
+}
+
 static inline unsigned default_get_apic_id(unsigned long x)
 {
 	unsigned int ver = GET_APIC_VERSION(apic_read(APIC_LVR));
@@ -509,12 +515,12 @@ static inline bool apic_id_is_primary_thread(unsigned int id) { return false; }
 static inline void apic_smt_update(void) { }
 #endif
 
-extern void irq_enter(void);
-extern void irq_exit(void);
+extern void irq_enter_if_inband(void);
+extern void irq_exit_if_inband(void);
 
 static inline void entering_irq(void)
 {
-	irq_enter();
+	irq_enter_if_inband();
 	kvm_set_cpu_l1tf_flush_l1d();
 }
 
@@ -526,20 +532,20 @@ static inline void entering_ack_irq(void)
 
 static inline void ipi_entering_ack_irq(void)
 {
-	irq_enter();
+	irq_enter_if_inband();
 	ack_APIC_irq();
 	kvm_set_cpu_l1tf_flush_l1d();
 }
 
 static inline void exiting_irq(void)
 {
-	irq_exit();
+	irq_exit_if_inband();
 }
 
 static inline void exiting_ack_irq(void)
 {
 	ack_APIC_irq();
-	irq_exit();
+	irq_exit_if_inband();
 }
 
 extern void ioapic_zap_locks(void);
diff --git a/arch/x86/include/asm/dovetail.h b/arch/x86/include/asm/dovetail.h
new file mode 100644
index 000000000000..e996daa3992c
--- /dev/null
+++ b/arch/x86/include/asm/dovetail.h
@@ -0,0 +1,32 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2019 Philippe Gerum.
+ */
+#ifndef _ASM_X86_DOVETAIL_H
+#define _ASM_X86_DOVETAIL_H
+
+#ifndef __ASSEMBLY__
+
+#include <asm/fpu/api.h>
+
+static inline
+void arch_dovetail_switch_prepare(bool leave_inband)
+{
+	if (leave_inband)
+		fpu__suspend_inband();
+}
+
+static inline
+void arch_dovetail_switch_finish(bool enter_inband)
+{
+	if (enter_inband)
+		fpu__resume_inband();
+	else if (!(current->flags & PF_KTHREAD) &&
+		test_thread_flag(TIF_NEED_FPU_LOAD))
+		switch_fpu_return();
+}
+
+#endif
+
+#endif /* _ASM_X86_DOVETAIL_H */
diff --git a/arch/x86/include/asm/fpu/api.h b/arch/x86/include/asm/fpu/api.h
index b774c52e5411..b33731917a0e 100644
--- a/arch/x86/include/asm/fpu/api.h
+++ b/arch/x86/include/asm/fpu/api.h
@@ -30,16 +30,25 @@ extern void fpregs_mark_activate(void);
  * fpu->state and set TIF_NEED_FPU_LOAD leaving CPU's FPU registers in
  * a random state.
  */
-static inline void fpregs_lock(void)
+static inline unsigned long fpregs_lock(void)
 {
-	preempt_disable();
-	local_bh_disable();
+	if (IS_ENABLED(CONFIG_IRQ_PIPELINE)) {
+		return hard_preempt_disable();
+	} else {
+		preempt_disable();
+		local_bh_disable();
+		return 0;
+	}
 }
 
-static inline void fpregs_unlock(void)
+static inline void fpregs_unlock(unsigned long flags)
 {
-	local_bh_enable();
-	preempt_enable();
+	if (IS_ENABLED(CONFIG_IRQ_PIPELINE)) {
+		hard_preempt_enable(flags);
+	} else {
+		local_bh_enable();
+		preempt_enable();
+	}
 }
 
 #ifdef CONFIG_X86_DEBUG_FPU
@@ -53,6 +62,10 @@ static inline void fpregs_assert_state_consistent(void) { }
  */
 extern void switch_fpu_return(void);
 
+/* For Dovetail context switching. */
+void fpu__suspend_inband(void);
+void fpu__resume_inband(void);
+
 /*
  * Query the presence of one or more xfeatures. Works on any legacy CPU as well.
  *
diff --git a/arch/x86/include/asm/fpu/internal.h b/arch/x86/include/asm/fpu/internal.h
index 4c95c365058a..7c0d75228f81 100644
--- a/arch/x86/include/asm/fpu/internal.h
+++ b/arch/x86/include/asm/fpu/internal.h
@@ -15,6 +15,7 @@
 #include <linux/sched.h>
 #include <linux/slab.h>
 #include <linux/mm.h>
+#include <linux/dovetail.h>
 
 #include <asm/user.h>
 #include <asm/fpu/api.h>
@@ -547,6 +548,32 @@ static inline void __fpregs_load_activate(void)
 	clear_thread_flag(TIF_NEED_FPU_LOAD);
 }
 
+#ifdef CONFIG_DOVETAIL
+
+static inline void oob_fpu_set_preempt(struct fpu *fpu)
+{
+	fpu->preempted = 1;
+}
+
+static inline void oob_fpu_clear_preempt(struct fpu *fpu)
+{
+	fpu->preempted = 0;
+}
+
+static inline bool oob_fpu_preempted(struct fpu *old_fpu)
+{
+	return old_fpu->preempted;
+}
+
+#else
+
+static inline bool oob_fpu_preempted(struct fpu *old_fpu)
+{
+	return false;
+}
+
+#endif	/* !CONFIG_DOVETAIL */
+
 /*
  * FPU state switching for scheduling.
  *
@@ -571,7 +598,9 @@ static inline void __fpregs_load_activate(void)
  */
 static inline void switch_fpu_prepare(struct fpu *old_fpu, int cpu)
 {
-	if (static_cpu_has(X86_FEATURE_FPU) && !(current->flags & PF_KTHREAD)) {
+	if (static_cpu_has(X86_FEATURE_FPU) &&
+		!(current->flags & PF_KTHREAD) &&
+		!oob_fpu_preempted(old_fpu)) {
 		if (!copy_fpregs_to_fpstate(old_fpu))
 			old_fpu->last_cpu = -1;
 		else
diff --git a/arch/x86/include/asm/fpu/types.h b/arch/x86/include/asm/fpu/types.h
index f098f6cab94b..a95c861a38cc 100644
--- a/arch/x86/include/asm/fpu/types.h
+++ b/arch/x86/include/asm/fpu/types.h
@@ -293,6 +293,18 @@ struct fpu {
 	 */
 	unsigned int			last_cpu;
 
+#ifdef CONFIG_DOVETAIL
+	/*
+	 * @preempted:
+	 *
+	 * When Dovetail is enabled, this flag is set for the inband
+	 * task context saved when entering a kernel_fpu_begin/end()
+	 * section before the latter got preempted by an out-of-band
+	 * task.
+	 */
+	unsigned char			preempted : 1;
+#endif
+
 	/*
 	 * @avx512_timestamp:
 	 *
diff --git a/arch/x86/include/asm/hw_irq.h b/arch/x86/include/asm/hw_irq.h
index 4154bc5f6a4e..beae9ae335ac 100644
--- a/arch/x86/include/asm/hw_irq.h
+++ b/arch/x86/include/asm/hw_irq.h
@@ -50,6 +50,11 @@ extern asmlinkage void deferred_error_interrupt(void);
 extern asmlinkage void call_function_interrupt(void);
 extern asmlinkage void call_function_single_interrupt(void);
 
+#ifdef CONFIG_IRQ_PIPELINE
+extern asmlinkage void reschedule_oob_interrupt(void);
+extern asmlinkage void timer_oob_interrupt(void);
+#endif
+
 #ifdef	CONFIG_X86_LOCAL_APIC
 struct irq_data;
 struct pci_dev;
diff --git a/arch/x86/include/asm/i8259.h b/arch/x86/include/asm/i8259.h
index 89789e8c80f6..facf1bc68de6 100644
--- a/arch/x86/include/asm/i8259.h
+++ b/arch/x86/include/asm/i8259.h
@@ -26,7 +26,7 @@ extern unsigned int cached_irq_mask;
 #define SLAVE_ICW4_DEFAULT	0x01
 #define PIC_ICW4_AEOI		2
 
-extern raw_spinlock_t i8259A_lock;
+extern hard_spinlock_t i8259A_lock;
 
 /* the PIC may need a careful delay on some platforms, hence specific calls */
 static inline unsigned char inb_pic(unsigned int port)
diff --git a/arch/x86/include/asm/irq_pipeline.h b/arch/x86/include/asm/irq_pipeline.h
new file mode 100644
index 000000000000..163bd20dda51
--- /dev/null
+++ b/arch/x86/include/asm/irq_pipeline.h
@@ -0,0 +1,149 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2019 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#ifndef _ASM_X86_IRQ_PIPELINE_H
+#define _ASM_X86_IRQ_PIPELINE_H
+
+#include <asm-generic/irq_pipeline.h>
+
+#ifdef CONFIG_IRQ_PIPELINE
+
+#include <asm/ptrace.h>
+
+#define FIRST_SYSTEM_IRQ	NR_IRQS
+#define TIMER_OOB_IPI		apicm_vector_irq(TIMER_OOB_VECTOR)
+#define RESCHEDULE_OOB_IPI	apicm_vector_irq(RESCHEDULE_OOB_VECTOR)
+#define apicm_irq_vector(__irq) ((__irq) - FIRST_SYSTEM_IRQ + FIRST_SYSTEM_VECTOR)
+#define apicm_vector_irq(__vec) ((__vec) - FIRST_SYSTEM_VECTOR + FIRST_SYSTEM_IRQ)
+
+#define X86_EFLAGS_SS_BIT	31
+
+static inline notrace
+unsigned long arch_irqs_virtual_to_native_flags(int stalled)
+{
+	return (!stalled) << X86_EFLAGS_IF_BIT;
+}
+
+static inline notrace
+unsigned long arch_irqs_native_to_virtual_flags(unsigned long flags)
+{
+	return hard_irqs_disabled_flags(flags) << X86_EFLAGS_SS_BIT;
+}
+
+#ifndef CONFIG_PARAVIRT_XXL
+
+static inline notrace unsigned long arch_local_irq_save(void)
+{
+	int stalled = inband_irq_save();
+	barrier();
+	return arch_irqs_virtual_to_native_flags(stalled);
+}
+
+static inline notrace void arch_local_irq_enable(void)
+{
+	barrier();
+	inband_irq_enable();
+}
+
+static inline notrace void arch_local_irq_disable(void)
+{
+	inband_irq_disable();
+	barrier();
+}
+
+static inline notrace unsigned long arch_local_save_flags(void)
+{
+	int stalled = inband_irqs_disabled();
+	barrier();
+	return arch_irqs_virtual_to_native_flags(stalled);
+}
+
+static inline notrace void arch_local_irq_restore(unsigned long flags)
+{
+	inband_irq_restore(native_irqs_disabled_flags(flags));
+	barrier();
+}
+
+#endif /* !CONFIG_PARAVIRT_XXL */
+
+static inline
+void arch_save_timer_regs(struct pt_regs *dst,
+			  struct pt_regs *src, bool oob_context)
+{
+	dst->flags = src->flags;
+	dst->cs = src->cs;
+	dst->ip = src->ip;
+	dst->bp = src->bp;
+	dst->ss = src->ss;
+	dst->sp = src->sp;
+	if (oob_context)
+		dst->flags &= ~X86_EFLAGS_IF;
+}
+
+static inline bool arch_steal_pipelined_tick(struct pt_regs *regs)
+{
+	return !(regs->flags & X86_EFLAGS_IF);
+}
+
+static inline int arch_enable_oob_stage(void)
+{
+	return 0;
+}
+
+unsigned long pipelined_fault_entry(int trapnr, struct pt_regs *regs);
+
+void pipelined_fault_exit(unsigned long combo);
+
+void handle_arch_irq(struct pt_regs *regs);
+
+#else /* !CONFIG_IRQ_PIPELINE */
+
+struct pt_regs;
+
+#ifndef CONFIG_PARAVIRT_XXL
+
+static inline notrace unsigned long arch_local_save_flags(void)
+{
+	return native_save_fl();
+}
+
+static inline notrace void arch_local_irq_restore(unsigned long flags)
+{
+	native_restore_fl(flags);
+}
+
+static inline notrace void arch_local_irq_disable(void)
+{
+	native_irq_disable();
+}
+
+static inline notrace void arch_local_irq_enable(void)
+{
+	native_irq_enable();
+}
+
+/*
+ * For spinlocks, etc:
+ */
+static inline notrace unsigned long arch_local_irq_save(void)
+{
+	unsigned long flags = arch_local_save_flags();
+	arch_local_irq_disable();
+	return flags;
+}
+
+#endif /* !CONFIG_PARAVIRT_XXL */
+
+static inline
+unsigned long pipelined_fault_entry(int trapnr, struct pt_regs *regs)
+{
+	return 0;
+}
+
+static inline void pipelined_fault_exit(unsigned long combo) { }
+
+#endif /* !CONFIG_IRQ_PIPELINE */
+
+#endif /* _ASM_X86_IRQ_PIPELINE_H */
diff --git a/arch/x86/include/asm/irq_vectors.h b/arch/x86/include/asm/irq_vectors.h
index 889f8b1b5b7f..1e51dc4850a6 100644
--- a/arch/x86/include/asm/irq_vectors.h
+++ b/arch/x86/include/asm/irq_vectors.h
@@ -106,10 +106,19 @@
 
 #define LOCAL_TIMER_VECTOR		0xec
 
+#ifdef CONFIG_IRQ_PIPELINE
+#define TIMER_OOB_VECTOR		0xeb
+#define RESCHEDULE_OOB_VECTOR		0xea
+#define FIRST_SYSTEM_APIC_VECTOR	RESCHEDULE_OOB_VECTOR
+#define NR_APIC_VECTORS	        	(NR_VECTORS - FIRST_SYSTEM_VECTOR)
+#else
+#define FIRST_SYSTEM_APIC_VECTOR	LOCAL_TIMER_VECTOR
+#endif
+
 #define NR_VECTORS			 256
 
 #ifdef CONFIG_X86_LOCAL_APIC
-#define FIRST_SYSTEM_VECTOR		LOCAL_TIMER_VECTOR
+#define FIRST_SYSTEM_VECTOR		FIRST_SYSTEM_APIC_VECTOR
 #else
 #define FIRST_SYSTEM_VECTOR		NR_VECTORS
 #endif
diff --git a/arch/x86/include/asm/irqflags.h b/arch/x86/include/asm/irqflags.h
index 8a0e56e1dcc9..9471ceaea11e 100644
--- a/arch/x86/include/asm/irqflags.h
+++ b/arch/x86/include/asm/irqflags.h
@@ -35,6 +35,11 @@ extern inline unsigned long native_save_fl(void)
 	return flags;
 }
 
+static inline unsigned long native_save_flags(void)
+{
+	return native_save_fl();
+}
+
 extern inline void native_restore_fl(unsigned long flags);
 extern inline void native_restore_fl(unsigned long flags)
 {
@@ -54,6 +59,38 @@ static inline void native_irq_enable(void)
 	asm volatile("sti": : :"memory");
 }
 
+static inline void native_irq_sync(void)
+{
+	asm volatile("sti ; nop ; cli": : :"memory");
+}
+
+static inline unsigned long native_irq_save(void)
+{
+	unsigned long flags;
+
+	flags = native_save_flags();
+
+	native_irq_disable();
+
+	return flags;
+}
+
+static inline void native_irq_restore(unsigned long flags)
+{
+	return native_restore_fl(flags);
+}
+
+static inline int native_irqs_disabled_flags(unsigned long flags)
+{
+	return !(flags & X86_EFLAGS_IF);
+}
+
+static inline bool native_irqs_disabled(void)
+{
+	unsigned long flags = native_save_flags();
+	return native_irqs_disabled_flags(flags);
+}
+
 static inline __cpuidle void native_safe_halt(void)
 {
 	mds_idle_clear_cpu_buffers();
@@ -73,26 +110,7 @@ static inline __cpuidle void native_halt(void)
 #else
 #ifndef __ASSEMBLY__
 #include <linux/types.h>
-
-static inline notrace unsigned long arch_local_save_flags(void)
-{
-	return native_save_fl();
-}
-
-static inline notrace void arch_local_irq_restore(unsigned long flags)
-{
-	native_restore_fl(flags);
-}
-
-static inline notrace void arch_local_irq_disable(void)
-{
-	native_irq_disable();
-}
-
-static inline notrace void arch_local_irq_enable(void)
-{
-	native_irq_enable();
-}
+#include <asm/irq_pipeline.h>
 
 /*
  * Used in the idle loop; sti takes one instruction cycle
@@ -112,15 +130,6 @@ static inline __cpuidle void halt(void)
 	native_halt();
 }
 
-/*
- * For spinlocks, etc:
- */
-static inline notrace unsigned long arch_local_irq_save(void)
-{
-	unsigned long flags = arch_local_save_flags();
-	arch_local_irq_disable();
-	return flags;
-}
 #else
 
 #define ENABLE_INTERRUPTS(x)	sti
@@ -161,7 +170,7 @@ static inline notrace unsigned long arch_local_irq_save(void)
 #ifndef __ASSEMBLY__
 static inline int arch_irqs_disabled_flags(unsigned long flags)
 {
-	return !(flags & X86_EFLAGS_IF);
+	return native_irqs_disabled_flags(flags);
 }
 
 static inline int arch_irqs_disabled(void)
@@ -176,8 +185,14 @@ static inline int arch_irqs_disabled(void)
 #ifdef CONFIG_TRACE_IRQFLAGS
 #  define TRACE_IRQS_ON		call trace_hardirqs_on_thunk;
 #  define TRACE_IRQS_OFF	call trace_hardirqs_off_thunk;
+#ifdef CONFIG_IRQ_PIPELINE
+#  define TRACE_IRQS_ON_PIPELINED	call trace_hardirqs_on_pipelined_thunk;
+#else
+#  define TRACE_IRQS_ON_PIPELINED	TRACE_IRQS_ON
+#endif
 #else
 #  define TRACE_IRQS_ON
+#  define TRACE_IRQS_ON_PIPELINED
 #  define TRACE_IRQS_OFF
 #endif
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
@@ -204,6 +219,13 @@ static inline int arch_irqs_disabled(void)
 #  define LOCKDEP_SYS_EXIT
 #  define LOCKDEP_SYS_EXIT_IRQ
 #endif
+#ifdef CONFIG_IRQ_PIPELINE
+#define ENABLE_INTERRUPTS_IF_PIPELINED	ENABLE_INTERRUPTS(CLBR_ANY)
+#define DISABLE_INTERRUPTS_IF_PIPELINED	DISABLE_INTERRUPTS(CLBR_ANY)
+#else
+#define ENABLE_INTERRUPTS_IF_PIPELINED
+#define DISABLE_INTERRUPTS_IF_PIPELINED
+#endif
 #endif /* __ASSEMBLY__ */
 
 #endif
diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h
index 16ae821483c8..32c3c7607ac3 100644
--- a/arch/x86/include/asm/mmu_context.h
+++ b/arch/x86/include/asm/mmu_context.h
@@ -214,6 +214,13 @@ extern void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 			       struct task_struct *tsk);
 #define switch_mm_irqs_off switch_mm_irqs_off
 
+static inline void
+switch_oob_mm(struct mm_struct *prev, struct mm_struct *next,
+	      struct task_struct *tsk)
+{
+	switch_mm_irqs_off(prev, next, tsk);
+}
+
 #define activate_mm(prev, next)			\
 do {						\
 	paravirt_activate_mm((prev), (next));	\
@@ -377,10 +384,13 @@ typedef struct {
 static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 {
 	temp_mm_state_t temp_state;
+	unsigned long flags;
 
 	lockdep_assert_irqs_disabled();
 	temp_state.mm = this_cpu_read(cpu_tlbstate.loaded_mm);
+	protect_inband_mm(flags);
 	switch_mm_irqs_off(NULL, mm, current);
+	unprotect_inband_mm(flags);
 
 	/*
 	 * If breakpoints are enabled, disable them while the temporary mm is
@@ -401,8 +411,12 @@ static inline temp_mm_state_t use_temporary_mm(struct mm_struct *mm)
 
 static inline void unuse_temporary_mm(temp_mm_state_t prev_state)
 {
+	unsigned long flags;
+
 	lockdep_assert_irqs_disabled();
+	protect_inband_mm(flags);
 	switch_mm_irqs_off(NULL, prev_state.mm, current);
+	unprotect_inband_mm(flags);
 
 	/*
 	 * Restore the breakpoints if they were disabled before the temporary mm
diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index 0bc530c4eb13..159c3b001c54 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -136,6 +136,7 @@ static inline u32 read_pkru(void)
 static inline void write_pkru(u32 pkru)
 {
 	struct pkru_state *pk;
+	unsigned long flags;
 
 	if (!boot_cpu_has(X86_FEATURE_OSPKE))
 		return;
@@ -147,11 +148,11 @@ static inline void write_pkru(u32 pkru)
 	 * written to the CPU. The FPU restore on return to userland would
 	 * otherwise load the previous value again.
 	 */
-	fpregs_lock();
+	flags = fpregs_lock();
 	if (pk)
 		pk->pkru = pkru;
 	__write_pkru(pkru);
-	fpregs_unlock();
+	fpregs_unlock(flags);
 }
 
 static inline int pte_young(pte_t pte)
diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h
index 54f5d54280f6..ea61ffc62b3f 100644
--- a/arch/x86/include/asm/processor.h
+++ b/arch/x86/include/asm/processor.h
@@ -514,6 +514,13 @@ static inline void arch_thread_struct_whitelist(unsigned long *offset,
  * have to worry about atomic accesses.
  */
 #define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/
+#define TS_OOB			0x0004	/* Thread is running out-of-band */
+#define TS_DOVETAIL		0x0008  /* Dovetail notifier enabled */
+#define TS_OFFSTAGE		0x0010	/* Thread is in-flight to OOB context */
+
+#define _TLF_OOB		TS_OOB
+#define _TLF_DOVETAIL		TS_DOVETAIL
+#define _TLF_OFFSTAGE		TS_OFFSTAGE
 
 /*
  * Set IOPL bits in EFLAGS from given mask
diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index f9453536f9bb..40590cdb4b78 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -52,10 +52,12 @@
 struct task_struct;
 #include <asm/cpufeature.h>
 #include <linux/atomic.h>
+#include <dovetail/thread_info.h>
 
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	u32			status;		/* thread synchronous flags */
+	struct oob_thread_state	oob_state;	/* co-kernel thread state */
 };
 
 #define INIT_THREAD_INFO(tsk)			\
@@ -63,6 +65,8 @@ struct thread_info {
 	.flags		= 0,			\
 }
 
+#define ti_local_flags(__ti)	((__ti)->status)
+
 #else /* !__ASSEMBLY__ */
 
 #include <asm/asm-offsets.h>
@@ -98,6 +102,7 @@ struct thread_info {
 #define TIF_IO_BITMAP		22	/* uses I/O bitmap */
 #define TIF_FORCED_TF		24	/* true if TF in eflags artificially */
 #define TIF_BLOCKSTEP		25	/* set when we want DEBUGCTLMSR_BTF */
+#define TIF_MAYDAY		26	/* emergency trap pending */
 #define TIF_LAZY_MMU_UPDATES	27	/* task is updating the mmu lazily */
 #define TIF_SYSCALL_TRACEPOINT	28	/* syscall tracepoint instrumentation */
 #define TIF_ADDR32		29	/* 32-bit address space on 64 bits */
@@ -126,6 +131,7 @@ struct thread_info {
 #define _TIF_POLLING_NRFLAG	(1 << TIF_POLLING_NRFLAG)
 #define _TIF_IO_BITMAP		(1 << TIF_IO_BITMAP)
 #define _TIF_FORCED_TF		(1 << TIF_FORCED_TF)
+#define _TIF_MAYDAY		(1 << TIF_MAYDAY)
 #define _TIF_BLOCKSTEP		(1 << TIF_BLOCKSTEP)
 #define _TIF_LAZY_MMU_UPDATES	(1 << TIF_LAZY_MMU_UPDATES)
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h
index 6f66d841262d..c7e774476dec 100644
--- a/arch/x86/include/asm/tlbflush.h
+++ b/arch/x86/include/asm/tlbflush.h
@@ -284,7 +284,10 @@ static inline void cr4_init_shadow(void)
 
 static inline void __cr4_set(unsigned long cr4)
 {
-	lockdep_assert_irqs_disabled();
+	if (irqs_pipelined())
+		check_hard_irqs_disabled();
+	else
+		lockdep_assert_irqs_disabled();
 	this_cpu_write(cpu_tlbstate.cr4, cr4);
 	__write_cr4(cr4);
 }
@@ -292,21 +295,25 @@ static inline void __cr4_set(unsigned long cr4)
 /* Set in this cpu's CR4. */
 static inline void cr4_set_bits_irqsoff(unsigned long mask)
 {
-	unsigned long cr4;
+	unsigned long cr4, flags;
 
+	flags = hard_local_irq_save();
 	cr4 = this_cpu_read(cpu_tlbstate.cr4);
 	if ((cr4 | mask) != cr4)
 		__cr4_set(cr4 | mask);
+	hard_local_irq_restore(flags);
 }
 
 /* Clear in this cpu's CR4. */
 static inline void cr4_clear_bits_irqsoff(unsigned long mask)
 {
-	unsigned long cr4;
+	unsigned long cr4, flags;
 
+	flags = hard_local_irq_save();
 	cr4 = this_cpu_read(cpu_tlbstate.cr4);
 	if ((cr4 & ~mask) != cr4)
 		__cr4_set(cr4 & ~mask);
+	hard_local_irq_restore(flags);
 }
 
 /* Set in this cpu's CR4. */
@@ -314,9 +321,9 @@ static inline void cr4_set_bits(unsigned long mask)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	cr4_set_bits_irqsoff(mask);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /* Clear in this cpu's CR4. */
@@ -324,17 +331,19 @@ static inline void cr4_clear_bits(unsigned long mask)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	cr4_clear_bits_irqsoff(mask);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static inline void cr4_toggle_bits_irqsoff(unsigned long mask)
 {
-	unsigned long cr4;
+	unsigned long cr4, flags;
 
+	flags = hard_local_irq_save();
 	cr4 = this_cpu_read(cpu_tlbstate.cr4);
 	__cr4_set(cr4 ^ mask);
+	hard_local_irq_restore(flags);
 }
 
 /* Read the CR4 shadow. */
@@ -401,6 +410,8 @@ static inline void invalidate_user_asid(u16 asid)
  */
 static inline void __native_flush_tlb(void)
 {
+	unsigned long flags;
+
 	/*
 	 * Preemption or interrupts must be disabled to protect the access
 	 * to the per CPU variable and to prevent being preempted between
@@ -408,10 +419,14 @@ static inline void __native_flush_tlb(void)
 	 */
 	WARN_ON_ONCE(preemptible());
 
+	flags = hard_cond_local_irq_save();
+
 	invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));
 
 	/* If current->mm == NULL then the read_cr3() "borrows" an mm */
 	native_write_cr3(__native_read_cr3());
+
+	hard_cond_local_irq_restore(flags);
 }
 
 /*
@@ -437,7 +452,7 @@ static inline void __native_flush_tlb_global(void)
 	 * from interrupts. (Use the raw variant because this code can
 	 * be called from deep inside debugging code.)
 	 */
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	cr4 = this_cpu_read(cpu_tlbstate.cr4);
 	/* toggle PGE */
@@ -445,7 +460,7 @@ static inline void __native_flush_tlb_global(void)
 	/* write old PGE again and flush TLBs */
 	native_write_cr4(cr4);
 
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /*
diff --git a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h
index 61d93f062a36..8ae74e0bcf00 100644
--- a/arch/x86/include/asm/uaccess.h
+++ b/arch/x86/include/asm/uaccess.h
@@ -68,7 +68,7 @@ static inline bool __chk_range_not_ok(unsigned long addr, unsigned long size, un
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 static inline bool pagefault_disabled(void);
 # define WARN_ON_IN_IRQ()	\
-	WARN_ON_ONCE(!in_task() && !pagefault_disabled())
+	WARN_ON_ONCE(running_inband() && !in_task() && !pagefault_disabled())
 #else
 # define WARN_ON_IN_IRQ()
 #endif
diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
index 3578ad248bc9..b4539cf53605 100644
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -118,6 +118,7 @@ obj-$(CONFIG_PARAVIRT_CLOCK)	+= pvclock.o
 obj-$(CONFIG_X86_PMEM_LEGACY_DEVICE) += pmem.o
 
 obj-$(CONFIG_JAILHOUSE_GUEST)	+= jailhouse.o
+obj-$(CONFIG_IRQ_PIPELINE)	+= irq_pipeline.o
 
 obj-$(CONFIG_EISA)		+= eisa.o
 obj-$(CONFIG_PCSPKR_PLATFORM)	+= pcspeaker.o
diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c
index 2b0faf86da1b..e4411af83d7c 100644
--- a/arch/x86/kernel/apic/apic.c
+++ b/arch/x86/kernel/apic/apic.c
@@ -31,6 +31,7 @@
 #include <linux/i8253.h>
 #include <linux/dmar.h>
 #include <linux/init.h>
+#include <linux/irq.h>
 #include <linux/cpu.h>
 #include <linux/dmi.h>
 #include <linux/smp.h>
@@ -271,10 +272,10 @@ void native_apic_icr_write(u32 low, u32 id)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	apic_write(APIC_ICR2, SET_APIC_DEST_FIELD(id));
 	apic_write(APIC_ICR, low);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 u64 native_apic_icr_read(void)
@@ -330,6 +331,9 @@ int lapic_get_maxlvt(void)
 static void __setup_APIC_LVTT(unsigned int clocks, int oneshot, int irqen)
 {
 	unsigned int lvtt_value, tmp_value;
+	unsigned long flags;
+
+	flags = hard_cond_local_irq_save();
 
 	lvtt_value = LOCAL_TIMER_VECTOR;
 	if (!oneshot)
@@ -354,6 +358,7 @@ static void __setup_APIC_LVTT(unsigned int clocks, int oneshot, int irqen)
 		asm volatile("mfence" : : : "memory");
 
 		printk_once(KERN_DEBUG "TSC deadline timer enabled\n");
+		hard_cond_local_irq_restore(flags);
 		return;
 	}
 
@@ -367,6 +372,8 @@ static void __setup_APIC_LVTT(unsigned int clocks, int oneshot, int irqen)
 
 	if (!oneshot)
 		apic_write(APIC_TMICT, clocks / APIC_DIVISOR);
+
+	hard_cond_local_irq_restore(flags);
 }
 
 /*
@@ -472,25 +479,31 @@ static int lapic_next_event(unsigned long delta,
 static int lapic_next_deadline(unsigned long delta,
 			       struct clock_event_device *evt)
 {
+	unsigned long flags;
 	u64 tsc;
 
+	flags = hard_local_irq_save();
 	tsc = rdtsc();
 	wrmsrl(MSR_IA32_TSC_DEADLINE, tsc + (((u64) delta) * TSC_DIVISOR));
+	hard_local_irq_restore(flags);
 	return 0;
 }
 
 static int lapic_timer_shutdown(struct clock_event_device *evt)
 {
+	unsigned long flags;
 	unsigned int v;
 
 	/* Lapic used as dummy for broadcast ? */
 	if (evt->features & CLOCK_EVT_FEAT_DUMMY)
 		return 0;
 
+	flags = hard_local_irq_save();
 	v = apic_read(APIC_LVTT);
 	v |= (APIC_LVT_MASKED | LOCAL_TIMER_VECTOR);
 	apic_write(APIC_LVTT, v);
 	apic_write(APIC_TMICT, 0);
+	hard_local_irq_restore(flags);
 	return 0;
 }
 
@@ -525,6 +538,32 @@ static void lapic_timer_broadcast(const struct cpumask *mask)
 #endif
 }
 
+static DEFINE_PER_CPU(struct clock_event_device, lapic_events);
+
+#ifdef CONFIG_IRQ_PIPELINE
+
+#define LAPIC_TIMER_IRQ  apicm_vector_irq(LOCAL_TIMER_VECTOR)
+
+static irqreturn_t lapic_oob_handler(int irq, void *dev_id)
+{
+	struct clock_event_device *evt = this_cpu_ptr(&lapic_events);
+
+	trace_local_timer_entry(LOCAL_TIMER_VECTOR);
+	clockevents_handle_event(evt);
+	trace_local_timer_exit(LOCAL_TIMER_VECTOR);
+
+	return IRQ_HANDLED;
+}
+
+static struct irqaction lapic_oob_action = {
+	.handler = lapic_oob_handler,
+	.name = "Out-of-band LAPIC timer interrupt",
+	.flags = IRQF_TIMER | IRQF_PERCPU,
+};
+
+#else
+#define LAPIC_TIMER_IRQ  -1
+#endif
 
 /*
  * The local apic timer can be used for any function which is CPU local.
@@ -532,8 +571,8 @@ static void lapic_timer_broadcast(const struct cpumask *mask)
 static struct clock_event_device lapic_clockevent = {
 	.name				= "lapic",
 	.features			= CLOCK_EVT_FEAT_PERIODIC |
-					  CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_C3STOP
-					  | CLOCK_EVT_FEAT_DUMMY,
+					  CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_C3STOP  |
+					  CLOCK_EVT_FEAT_PIPELINE | CLOCK_EVT_FEAT_DUMMY,
 	.shift				= 32,
 	.set_state_shutdown		= lapic_timer_shutdown,
 	.set_state_periodic		= lapic_timer_set_periodic,
@@ -542,9 +581,8 @@ static struct clock_event_device lapic_clockevent = {
 	.set_next_event			= lapic_next_event,
 	.broadcast			= lapic_timer_broadcast,
 	.rating				= 100,
-	.irq				= -1,
+	.irq				= LAPIC_TIMER_IRQ,
 };
-static DEFINE_PER_CPU(struct clock_event_device, lapic_events);
 
 #define DEADLINE_MODEL_MATCH_FUNC(model, func)	\
 	{ X86_VENDOR_INTEL, 6, model, X86_FEATURE_ANY, (unsigned long)&func }
@@ -1068,6 +1106,9 @@ void __init setup_boot_APIC_clock(void)
 	/* Setup the lapic or request the broadcast */
 	setup_APIC_timer();
 	amd_e400_c1e_apic_setup();
+#ifdef CONFIG_IRQ_PIPELINE
+	setup_percpu_irq(LAPIC_TIMER_IRQ, &lapic_oob_action);
+#endif
 }
 
 void setup_secondary_APIC_clock(void)
@@ -1541,7 +1582,7 @@ static bool apic_check_and_ack(union apic_ir *irr, union apic_ir *isr)
 		 * per set bit.
 		 */
 		for_each_set_bit(bit, isr->map, APIC_IR_BITS)
-			ack_APIC_irq();
+			__ack_APIC_irq();
 		return true;
 	}
 
@@ -2176,7 +2217,7 @@ __visible void __irq_entry smp_spurious_interrupt(struct pt_regs *regs)
 	if (v & (1 << (vector & 0x1f))) {
 		pr_info("Spurious interrupt (vector 0x%02x) on CPU#%d. Acked\n",
 			vector, smp_processor_id());
-		ack_APIC_irq();
+		__ack_APIC_irq();
 	} else {
 		pr_info("Spurious interrupt (vector 0x%02x) on CPU#%d. Not pending!\n",
 			vector, smp_processor_id());
@@ -2210,7 +2251,7 @@ __visible void __irq_entry smp_error_interrupt(struct pt_regs *regs)
 	if (lapic_get_maxlvt() > 3)	/* Due to the Pentium erratum 3AP. */
 		apic_write(APIC_ESR, 0);
 	v = apic_read(APIC_ESR);
-	ack_APIC_irq();
+	__ack_APIC_irq();
 	atomic_inc(&irq_err_count);
 
 	apic_printk(APIC_DEBUG, KERN_DEBUG "APIC error on CPU%d: %02x",
diff --git a/arch/x86/kernel/apic/apic_flat_64.c b/arch/x86/kernel/apic/apic_flat_64.c
index 7862b152a052..d3762181070f 100644
--- a/arch/x86/kernel/apic/apic_flat_64.c
+++ b/arch/x86/kernel/apic/apic_flat_64.c
@@ -52,9 +52,9 @@ static void _flat_send_IPI_mask(unsigned long mask, int vector)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	__default_send_IPI_dest_field(mask, vector, apic->dest_logical);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static void flat_send_IPI_mask(const struct cpumask *cpumask, int vector)
diff --git a/arch/x86/kernel/apic/apic_numachip.c b/arch/x86/kernel/apic/apic_numachip.c
index cdf45b4700f2..040ec0bd868e 100644
--- a/arch/x86/kernel/apic/apic_numachip.c
+++ b/arch/x86/kernel/apic/apic_numachip.c
@@ -103,10 +103,10 @@ static void numachip_send_IPI_one(int cpu, int vector)
 	if (!((apicid ^ local_apicid) >> NUMACHIP_LAPIC_BITS)) {
 		unsigned long flags;
 
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 		__default_send_IPI_dest_field(apicid, vector,
 			APIC_DEST_PHYSICAL);
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 		preempt_enable();
 		return;
 	}
diff --git a/arch/x86/kernel/apic/io_apic.c b/arch/x86/kernel/apic/io_apic.c
index d6af97fd170a..3c1a4cc5b238 100644
--- a/arch/x86/kernel/apic/io_apic.c
+++ b/arch/x86/kernel/apic/io_apic.c
@@ -78,7 +78,7 @@
 #define for_each_irq_pin(entry, head) \
 	list_for_each_entry(entry, &head, list)
 
-static DEFINE_RAW_SPINLOCK(ioapic_lock);
+static DEFINE_HARD_SPINLOCK(ioapic_lock);
 static DEFINE_MUTEX(ioapic_mutex);
 static unsigned int ioapic_dynirq_base;
 static int ioapic_initialized;
@@ -1637,7 +1637,7 @@ static int __init timer_irq_works(void)
 		return 1;
 
 	local_save_flags(flags);
-	local_irq_enable();
+	local_irq_enable_full();
 
 	if (boot_cpu_has(X86_FEATURE_TSC))
 		delay_with_tsc();
@@ -1645,6 +1645,8 @@ static int __init timer_irq_works(void)
 		delay_without_tsc();
 
 	local_irq_restore(flags);
+	if (raw_irqs_disabled_flags(flags))
+		hard_local_irq_disable();
 
 	/*
 	 * Expect a few ticks at least, to be sure some possible
@@ -1828,7 +1830,7 @@ static void ioapic_ack_level(struct irq_data *irq_data)
 	 * We must acknowledge the irq before we move it or the acknowledge will
 	 * not propagate properly.
 	 */
-	ack_APIC_irq();
+	__ack_APIC_irq();
 
 	/*
 	 * Tail end of clearing remote IRR bit (either by delivering the EOI
@@ -1948,7 +1950,7 @@ static struct irq_chip ioapic_chip __read_mostly = {
 	.irq_set_affinity	= ioapic_set_affinity,
 	.irq_retrigger		= irq_chip_retrigger_hierarchy,
 	.irq_get_irqchip_state	= ioapic_irq_get_chip_state,
-	.flags			= IRQCHIP_SKIP_SET_WAKE,
+	.flags			= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct irq_chip ioapic_ir_chip __read_mostly = {
@@ -1961,7 +1963,7 @@ static struct irq_chip ioapic_ir_chip __read_mostly = {
 	.irq_set_affinity	= ioapic_set_affinity,
 	.irq_retrigger		= irq_chip_retrigger_hierarchy,
 	.irq_get_irqchip_state	= ioapic_irq_get_chip_state,
-	.flags			= IRQCHIP_SKIP_SET_WAKE,
+	.flags			= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 static inline void init_IO_APIC_traps(void)
@@ -2008,7 +2010,7 @@ static void unmask_lapic_irq(struct irq_data *data)
 
 static void ack_lapic_irq(struct irq_data *data)
 {
-	ack_APIC_irq();
+	__ack_APIC_irq();
 }
 
 static struct irq_chip lapic_chip __read_mostly = {
@@ -2016,6 +2018,7 @@ static struct irq_chip lapic_chip __read_mostly = {
 	.irq_mask	= mask_lapic_irq,
 	.irq_unmask	= unmask_lapic_irq,
 	.irq_ack	= ack_lapic_irq,
+	.flags		= IRQCHIP_PIPELINE_SAFE,
 };
 
 static void lapic_register_intr(int irq)
@@ -2133,7 +2136,7 @@ static inline void __init check_timer(void)
 	if (!global_clock_event)
 		return;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	/*
 	 * get/set the timer IRQ vector:
@@ -2201,7 +2204,7 @@ static inline void __init check_timer(void)
 			goto out;
 		}
 		panic_if_irq_remap("timer doesn't work through Interrupt-remapped IO-APIC");
-		local_irq_disable();
+		hard_local_irq_disable();
 		clear_IO_APIC_pin(apic1, pin1);
 		if (!no_pin1)
 			apic_printk(APIC_QUIET, KERN_ERR "..MP-BIOS bug: "
@@ -2225,7 +2228,7 @@ static inline void __init check_timer(void)
 		/*
 		 * Cleanup, just in case ...
 		 */
-		local_irq_disable();
+		hard_local_irq_disable();
 		legacy_pic->mask(0);
 		clear_IO_APIC_pin(apic2, pin2);
 		apic_printk(APIC_QUIET, KERN_INFO "....... failed.\n");
@@ -2242,7 +2245,7 @@ static inline void __init check_timer(void)
 		apic_printk(APIC_QUIET, KERN_INFO "..... works.\n");
 		goto out;
 	}
-	local_irq_disable();
+	hard_local_irq_disable();
 	legacy_pic->mask(0);
 	apic_write(APIC_LVT0, APIC_LVT_MASKED | APIC_DM_FIXED | cfg->vector);
 	apic_printk(APIC_QUIET, KERN_INFO "..... failed.\n");
@@ -2260,7 +2263,7 @@ static inline void __init check_timer(void)
 		apic_printk(APIC_QUIET, KERN_INFO "..... works.\n");
 		goto out;
 	}
-	local_irq_disable();
+	hard_local_irq_disable();
 	apic_printk(APIC_QUIET, KERN_INFO "..... failed :(.\n");
 	if (apic_is_x2apic_enabled())
 		apic_printk(APIC_QUIET, KERN_INFO
@@ -2269,7 +2272,7 @@ static inline void __init check_timer(void)
 	panic("IO-APIC + timer doesn't work!  Boot with apic=debug and send a "
 		"report.  Then try booting with the 'noapic' option.\n");
 out:
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /*
@@ -3010,13 +3013,13 @@ int mp_irqdomain_alloc(struct irq_domain *domain, unsigned int virq,
 	cfg = irqd_cfg(irq_data);
 	add_pin_to_irq_node(data, ioapic_alloc_attr_node(info), ioapic, pin);
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	if (info->ioapic_entry)
 		mp_setup_entry(cfg, data, info->ioapic_entry);
 	mp_register_handler(virq, data->trigger);
 	if (virq < nr_legacy_irqs())
 		legacy_pic->mask(virq);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 
 	apic_printk(APIC_VERBOSE, KERN_DEBUG
 		    "IOAPIC[%d]: Set routing entry (%d-%d -> 0x%x -> IRQ %d Mode:%i Active:%i Dest:%d)\n",
diff --git a/arch/x86/kernel/apic/ipi.c b/arch/x86/kernel/apic/ipi.c
index 6ca0f91372fd..d501cb4a9678 100644
--- a/arch/x86/kernel/apic/ipi.c
+++ b/arch/x86/kernel/apic/ipi.c
@@ -116,8 +116,10 @@ void __default_send_IPI_shortcut(unsigned int shortcut, int vector)
 	 * cli/sti.  Otherwise we use an even cheaper single atomic write
 	 * to the APIC.
 	 */
+	unsigned long flags;
 	unsigned int cfg;
 
+	flags = hard_cond_local_irq_save();
 	/*
 	 * Wait for idle.
 	 */
@@ -136,6 +138,8 @@ void __default_send_IPI_shortcut(unsigned int shortcut, int vector)
 	 * Send the IPI. The write to APIC_ICR fires this off.
 	 */
 	native_apic_mem_write(APIC_ICR, cfg);
+
+	hard_cond_local_irq_restore(flags);
 }
 
 /*
@@ -144,8 +148,10 @@ void __default_send_IPI_shortcut(unsigned int shortcut, int vector)
  */
 void __default_send_IPI_dest_field(unsigned int mask, int vector, unsigned int dest)
 {
+	unsigned long flags;
 	unsigned long cfg;
 
+	flags = hard_cond_local_irq_save();
 	/*
 	 * Wait for idle.
 	 */
@@ -169,16 +175,18 @@ void __default_send_IPI_dest_field(unsigned int mask, int vector, unsigned int d
 	 * Send the IPI. The write to APIC_ICR fires this off.
 	 */
 	native_apic_mem_write(APIC_ICR, cfg);
+
+	hard_cond_local_irq_restore(flags);
 }
 
 void default_send_IPI_single_phys(int cpu, int vector)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	__default_send_IPI_dest_field(per_cpu(x86_cpu_to_apicid, cpu),
 				      vector, APIC_DEST_PHYSICAL);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 void default_send_IPI_mask_sequence_phys(const struct cpumask *mask, int vector)
@@ -191,12 +199,12 @@ void default_send_IPI_mask_sequence_phys(const struct cpumask *mask, int vector)
 	 * to an arbitrary mask, so I do a unicast to each CPU instead.
 	 * - mbligh
 	 */
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	for_each_cpu(query_cpu, mask) {
 		__default_send_IPI_dest_field(per_cpu(x86_cpu_to_apicid,
 				query_cpu), vector, APIC_DEST_PHYSICAL);
 	}
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 void default_send_IPI_mask_allbutself_phys(const struct cpumask *mask,
@@ -208,14 +216,14 @@ void default_send_IPI_mask_allbutself_phys(const struct cpumask *mask,
 
 	/* See Hack comment above */
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	for_each_cpu(query_cpu, mask) {
 		if (query_cpu == this_cpu)
 			continue;
 		__default_send_IPI_dest_field(per_cpu(x86_cpu_to_apicid,
 				 query_cpu), vector, APIC_DEST_PHYSICAL);
 	}
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /*
@@ -255,12 +263,12 @@ void default_send_IPI_mask_sequence_logical(const struct cpumask *mask,
 	 * should be modified to do 1 message per cluster ID - mbligh
 	 */
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	for_each_cpu(query_cpu, mask)
 		__default_send_IPI_dest_field(
 			early_per_cpu(x86_cpu_to_logical_apicid, query_cpu),
 			vector, apic->dest_logical);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 void default_send_IPI_mask_allbutself_logical(const struct cpumask *mask,
@@ -272,7 +280,7 @@ void default_send_IPI_mask_allbutself_logical(const struct cpumask *mask,
 
 	/* See Hack comment above */
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	for_each_cpu(query_cpu, mask) {
 		if (query_cpu == this_cpu)
 			continue;
@@ -280,7 +288,7 @@ void default_send_IPI_mask_allbutself_logical(const struct cpumask *mask,
 			early_per_cpu(x86_cpu_to_logical_apicid, query_cpu),
 			vector, apic->dest_logical);
 		}
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /*
@@ -294,10 +302,10 @@ void default_send_IPI_mask_logical(const struct cpumask *cpumask, int vector)
 	if (!mask)
 		return;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	WARN_ON(mask & ~cpumask_bits(cpu_online_mask)[0]);
 	__default_send_IPI_dest_field(mask, vector, apic->dest_logical);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /* must come after the send_IPI functions above for inlining */
diff --git a/arch/x86/kernel/apic/msi.c b/arch/x86/kernel/apic/msi.c
index 7f7533462474..997040f37a69 100644
--- a/arch/x86/kernel/apic/msi.c
+++ b/arch/x86/kernel/apic/msi.c
@@ -58,7 +58,7 @@ static struct irq_chip pci_msi_controller = {
 	.irq_ack		= irq_chip_ack_parent,
 	.irq_retrigger		= irq_chip_retrigger_hierarchy,
 	.irq_compose_msi_msg	= irq_msi_compose_msg,
-	.flags			= IRQCHIP_SKIP_SET_WAKE,
+	.flags			= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 int native_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)
@@ -156,7 +156,7 @@ static struct irq_chip pci_msi_ir_controller = {
 	.irq_ack		= irq_chip_ack_parent,
 	.irq_retrigger		= irq_chip_retrigger_hierarchy,
 	.irq_set_vcpu_affinity	= irq_chip_set_vcpu_affinity_parent,
-	.flags			= IRQCHIP_SKIP_SET_WAKE,
+	.flags			= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct msi_domain_info pci_msi_ir_domain_info = {
@@ -198,7 +198,7 @@ static struct irq_chip dmar_msi_controller = {
 	.irq_retrigger		= irq_chip_retrigger_hierarchy,
 	.irq_compose_msi_msg	= irq_msi_compose_msg,
 	.irq_write_msi_msg	= dmar_msi_write_msg,
-	.flags			= IRQCHIP_SKIP_SET_WAKE,
+	.flags			= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 static irq_hw_number_t dmar_msi_get_hwirq(struct msi_domain_info *info,
@@ -295,7 +295,7 @@ static struct irq_chip hpet_msi_controller __ro_after_init = {
 	.irq_retrigger = irq_chip_retrigger_hierarchy,
 	.irq_compose_msi_msg = irq_msi_compose_msg,
 	.irq_write_msi_msg = hpet_msi_write_msg,
-	.flags = IRQCHIP_SKIP_SET_WAKE,
+	.flags = IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 static irq_hw_number_t hpet_msi_get_hwirq(struct msi_domain_info *info,
diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c
index 2c5676b0a6e7..2af0ebd4222b 100644
--- a/arch/x86/kernel/apic/vector.c
+++ b/arch/x86/kernel/apic/vector.c
@@ -39,7 +39,7 @@ struct apic_chip_data {
 
 struct irq_domain *x86_vector_domain;
 EXPORT_SYMBOL_GPL(x86_vector_domain);
-static DEFINE_RAW_SPINLOCK(vector_lock);
+static DEFINE_HARD_SPINLOCK(vector_lock);
 static cpumask_var_t vector_searchmask;
 static struct irq_chip lapic_controller;
 static struct irq_matrix *vector_matrix;
@@ -727,6 +727,10 @@ static struct irq_desc *__setup_vector_irq(int vector)
 {
 	int isairq = vector - ISA_IRQ_VECTOR(0);
 
+	/* Copy the cleanup vector if irqs are pipelined. */
+	if (IS_ENABLED(CONFIG_SMP) &&
+		vector == IRQ_MOVE_CLEANUP_VECTOR)
+		return irq_to_desc(IRQ_MOVE_CLEANUP_VECTOR); /* 1:1 mapping */
 	/* Check whether the irq is in the legacy space */
 	if (isairq < 0 || isairq >= nr_legacy_irqs())
 		return VECTOR_UNUSED;
@@ -761,9 +765,11 @@ void lapic_online(void)
 
 void lapic_offline(void)
 {
-	lock_vector_lock();
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&vector_lock, flags);
 	irq_matrix_offline(vector_matrix);
-	unlock_vector_lock();
+	raw_spin_unlock_irqrestore(&vector_lock, flags);
 }
 
 static int apic_set_affinity(struct irq_data *irqd,
@@ -772,6 +778,8 @@ static int apic_set_affinity(struct irq_data *irqd,
 	struct apic_chip_data *apicd = apic_chip_data(irqd);
 	int err;
 
+	WARN_ON_ONCE(irqs_pipelined() && !hard_irqs_disabled());
+
 	/*
 	 * Core code can call here for inactive interrupts. For inactive
 	 * interrupts which use managed or reservation mode there is no
@@ -813,7 +821,7 @@ static int apic_retrigger_irq(struct irq_data *irqd)
 void apic_ack_irq(struct irq_data *irqd)
 {
 	irq_move_irq(irqd);
-	ack_APIC_irq();
+	__ack_APIC_irq();
 }
 
 void apic_ack_edge(struct irq_data *irqd)
@@ -858,10 +866,11 @@ asmlinkage __visible void __irq_entry smp_irq_move_cleanup_interrupt(void)
 	struct hlist_head *clhead = this_cpu_ptr(&cleanup_list);
 	struct apic_chip_data *apicd;
 	struct hlist_node *tmp;
+	unsigned long flags;
 
 	entering_ack_irq();
 	/* Prevent vectors vanishing under us */
-	raw_spin_lock(&vector_lock);
+	raw_spin_lock_irqsave(&vector_lock, flags);
 
 	hlist_for_each_entry_safe(apicd, tmp, clhead, clist) {
 		unsigned int irr, vector = apicd->prev_vector;
@@ -883,15 +892,16 @@ asmlinkage __visible void __irq_entry smp_irq_move_cleanup_interrupt(void)
 		free_moved_vector(apicd);
 	}
 
-	raw_spin_unlock(&vector_lock);
+	raw_spin_unlock_irqrestore(&vector_lock, flags);
 	exiting_irq();
 }
 
 static void __send_cleanup_vector(struct apic_chip_data *apicd)
 {
+	unsigned long flags;
 	unsigned int cpu;
 
-	raw_spin_lock(&vector_lock);
+	raw_spin_lock_irqsave(&vector_lock, flags);
 	apicd->move_in_progress = 0;
 	cpu = apicd->prev_cpu;
 	if (cpu_online(cpu)) {
@@ -900,7 +910,7 @@ static void __send_cleanup_vector(struct apic_chip_data *apicd)
 	} else {
 		apicd->prev_vector = 0;
 	}
-	raw_spin_unlock(&vector_lock);
+	raw_spin_unlock_irqrestore(&vector_lock, flags);
 }
 
 void send_cleanup_vector(struct irq_cfg *cfg)
@@ -938,6 +948,8 @@ void irq_force_complete_move(struct irq_desc *desc)
 	struct irq_data *irqd;
 	unsigned int vector;
 
+	WARN_ON_ONCE(irqs_pipelined() && !hard_irqs_disabled());
+
 	/*
 	 * The function is called for all descriptors regardless of which
 	 * irqdomain they belong to. For example if an IRQ is provided by
@@ -1028,9 +1040,10 @@ void irq_force_complete_move(struct irq_desc *desc)
 int lapic_can_unplug_cpu(void)
 {
 	unsigned int rsvd, avl, tomove, cpu = smp_processor_id();
+	unsigned long flags;
 	int ret = 0;
 
-	raw_spin_lock(&vector_lock);
+	raw_spin_lock_irqsave(&vector_lock, flags);
 	tomove = irq_matrix_allocated(vector_matrix);
 	avl = irq_matrix_available(vector_matrix, true);
 	if (avl < tomove) {
@@ -1045,7 +1058,7 @@ int lapic_can_unplug_cpu(void)
 			rsvd, avl);
 	}
 out:
-	raw_spin_unlock(&vector_lock);
+	raw_spin_unlock_irqrestore(&vector_lock, flags);
 	return ret;
 }
 #endif /* HOTPLUG_CPU */
diff --git a/arch/x86/kernel/apic/x2apic_cluster.c b/arch/x86/kernel/apic/x2apic_cluster.c
index b0889c48a2ac..0e1126a7320c 100644
--- a/arch/x86/kernel/apic/x2apic_cluster.c
+++ b/arch/x86/kernel/apic/x2apic_cluster.c
@@ -42,7 +42,7 @@ __x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)
 	u32 dest;
 
 	x2apic_wrmsr_fence();
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	tmpmsk = this_cpu_cpumask_var_ptr(ipi_mask);
 	cpumask_copy(tmpmsk, mask);
@@ -66,7 +66,7 @@ __x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)
 		cpumask_andnot(tmpmsk, tmpmsk, &cmsk->mask);
 	}
 
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static void x2apic_send_IPI_mask(const struct cpumask *mask, int vector)
diff --git a/arch/x86/kernel/apic/x2apic_phys.c b/arch/x86/kernel/apic/x2apic_phys.c
index bc9693841353..2d6c4e33ba33 100644
--- a/arch/x86/kernel/apic/x2apic_phys.c
+++ b/arch/x86/kernel/apic/x2apic_phys.c
@@ -50,7 +50,7 @@ __x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)
 
 	x2apic_wrmsr_fence();
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	this_cpu = smp_processor_id();
 	for_each_cpu(query_cpu, mask) {
@@ -59,7 +59,7 @@ __x2apic_send_IPI_mask(const struct cpumask *mask, int vector, int apic_dest)
 		__x2apic_send_IPI_dest(per_cpu(x86_cpu_to_apicid, query_cpu),
 				       vector, APIC_DEST_PHYSICAL);
 	}
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static void x2apic_send_IPI_mask(const struct cpumask *mask, int vector)
diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c
index 5c7ee3df4d0b..6528c28224e5 100644
--- a/arch/x86/kernel/asm-offsets.c
+++ b/arch/x86/kernel/asm-offsets.c
@@ -12,6 +12,7 @@
 #include <linux/hardirq.h>
 #include <linux/suspend.h>
 #include <linux/kbuild.h>
+#include <linux/irqstage.h>
 #include <asm/processor.h>
 #include <asm/thread_info.h>
 #include <asm/sigframe.h>
@@ -38,8 +39,16 @@ static void __used common(void)
 #endif
 
 	BLANK();
+	OFFSET(TASK_TI_status, task_struct, thread_info.status);
 	OFFSET(TASK_addr_limit, task_struct, thread.addr_limit);
 
+#ifdef CONFIG_IRQ_PIPELINE
+	BLANK();
+	DEFINE(oob_preempt_mask, STAGE_MASK);
+	DEFINE(inband_stall_mask, BIT(STAGE_STALL_BIT));
+	OFFSET(inband_stage_status, irq_pipeline_data, stages[0].status);
+#endif
+
 	BLANK();
 	OFFSET(crypto_tfm_ctx_offset, crypto_tfm, __crt_ctx);
 
diff --git a/arch/x86/kernel/cpu/mce/core.c b/arch/x86/kernel/cpu/mce/core.c
index 743370ee4983..7e145855e92f 100644
--- a/arch/x86/kernel/cpu/mce/core.c
+++ b/arch/x86/kernel/cpu/mce/core.c
@@ -1345,11 +1345,11 @@ void do_machine_check(struct pt_regs *regs, long error_code)
 	/* Fault was in user mode and we need to take some action */
 	if ((m.cs & 3) == 3) {
 		ist_begin_non_atomic(regs);
-		local_irq_enable();
+		hard_local_irq_enable();
 
 		if (kill_it || do_memory_failure(&m))
 			force_sig(SIGBUS);
-		local_irq_disable();
+		hard_local_irq_disable();
 		ist_end_non_atomic();
 	} else {
 		if (!fixup_exception(regs, X86_TRAP_MC, error_code, 0))
diff --git a/arch/x86/kernel/cpu/mtrr/generic.c b/arch/x86/kernel/cpu/mtrr/generic.c
index aa5c064a6a22..7f656c9e98ec 100644
--- a/arch/x86/kernel/cpu/mtrr/generic.c
+++ b/arch/x86/kernel/cpu/mtrr/generic.c
@@ -450,13 +450,13 @@ void __init mtrr_bp_pat_init(void)
 {
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	prepare_set();
 
 	pat_init();
 
 	post_set();
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /* Grab all of the MTRR state for this CPU into *state */
@@ -797,7 +797,7 @@ static void generic_set_all(void)
 	unsigned long mask, count;
 	unsigned long flags;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	prepare_set();
 
 	/* Actually set the state */
@@ -807,7 +807,7 @@ static void generic_set_all(void)
 	pat_init();
 
 	post_set();
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 
 	/* Use the atomic bitops to update the global mask */
 	for (count = 0; count < sizeof(mask) * 8; ++count) {
@@ -836,7 +836,7 @@ static void generic_set_mtrr(unsigned int reg, unsigned long base,
 
 	vr = &mtrr_state.var_ranges[reg];
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	prepare_set();
 
 	if (size == 0) {
@@ -857,7 +857,7 @@ static void generic_set_mtrr(unsigned int reg, unsigned long base,
 	}
 
 	post_set();
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 int generic_validate_add_page(unsigned long base, unsigned long size,
diff --git a/arch/x86/kernel/dumpstack.c b/arch/x86/kernel/dumpstack.c
index e07424e19274..223791c55ed5 100644
--- a/arch/x86/kernel/dumpstack.c
+++ b/arch/x86/kernel/dumpstack.c
@@ -7,6 +7,7 @@
 #include <linux/uaccess.h>
 #include <linux/utsname.h>
 #include <linux/hardirq.h>
+#include <linux/irq_pipeline.h>
 #include <linux/kdebug.h>
 #include <linux/module.h>
 #include <linux/ptrace.h>
@@ -369,6 +370,8 @@ int __die(const char *str, struct pt_regs *regs, long err)
 {
 	const char *pr = "";
 
+	irq_pipeline_oops();
+
 	/* Save the regs of the first oops for the executive summary later. */
 	if (!die_counter)
 		exec_summary_regs = *regs;
@@ -377,13 +380,14 @@ int __die(const char *str, struct pt_regs *regs, long err)
 		pr = IS_ENABLED(CONFIG_PREEMPT_RT) ? " PREEMPT_RT" : " PREEMPT";
 
 	printk(KERN_DEFAULT
-	       "%s: %04lx [#%d]%s%s%s%s%s\n", str, err & 0xffff, ++die_counter,
+	       "%s: %04lx [#%d]%s%s%s%s%s%s\n", str, err & 0xffff, ++die_counter,
 	       pr,
 	       IS_ENABLED(CONFIG_SMP)     ? " SMP"             : "",
 	       debug_pagealloc_enabled()  ? " DEBUG_PAGEALLOC" : "",
 	       IS_ENABLED(CONFIG_KASAN)   ? " KASAN"           : "",
 	       IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION) ?
-	       (boot_cpu_has(X86_FEATURE_PTI) ? " PTI" : " NOPTI") : "");
+	       (boot_cpu_has(X86_FEATURE_PTI) ? " PTI" : " NOPTI") : "",
+	       irqs_pipelined()           ? " IRQ_PIPELINE"    : "");
 
 	show_regs(regs);
 	print_modules();
diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
index 12c70840980e..51b7261dc32c 100644
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@ -15,6 +15,7 @@
 
 #include <linux/hardirq.h>
 #include <linux/pkeys.h>
+#include <linux/cpuhotplug.h>
 
 #define CREATE_TRACE_POINTS
 #include <asm/trace/fpu.h>
@@ -76,19 +77,24 @@ static bool interrupted_user_mode(void)
  */
 bool irq_fpu_usable(void)
 {
-	return !in_interrupt() ||
-		interrupted_user_mode() ||
-		interrupted_kernel_fpu_idle();
+	return running_inband() &&
+		(!in_interrupt() ||
+			interrupted_user_mode() ||
+			interrupted_kernel_fpu_idle());
 }
 EXPORT_SYMBOL(irq_fpu_usable);
 
 void kernel_fpu_begin(void)
 {
+	unsigned long flags;
+
 	preempt_disable();
 
 	WARN_ON_FPU(!irq_fpu_usable());
 	WARN_ON_FPU(this_cpu_read(in_kernel_fpu));
 
+	flags = hard_cond_local_irq_save();
+
 	this_cpu_write(in_kernel_fpu, true);
 
 	if (!(current->flags & PF_KTHREAD) &&
@@ -100,7 +106,10 @@ void kernel_fpu_begin(void)
 		 */
 		copy_fpregs_to_fpstate(&current->thread.fpu);
 	}
+
 	__cpu_invalidate_fpregs_state();
+
+	hard_cond_local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(kernel_fpu_begin);
 
@@ -120,9 +129,11 @@ EXPORT_SYMBOL_GPL(kernel_fpu_end);
  */
 void fpu__save(struct fpu *fpu)
 {
+	unsigned long flags;
+
 	WARN_ON_FPU(fpu != &current->thread.fpu);
 
-	fpregs_lock();
+	flags = fpregs_lock();
 	trace_x86_fpu_before_save(fpu);
 
 	if (!test_thread_flag(TIF_NEED_FPU_LOAD)) {
@@ -132,7 +143,7 @@ void fpu__save(struct fpu *fpu)
 	}
 
 	trace_x86_fpu_after_save(fpu);
-	fpregs_unlock();
+	fpregs_unlock(flags);
 }
 
 /*
@@ -168,6 +179,7 @@ int fpu__copy(struct task_struct *dst, struct task_struct *src)
 {
 	struct fpu *dst_fpu = &dst->thread.fpu;
 	struct fpu *src_fpu = &src->thread.fpu;
+	unsigned long flags;
 
 	dst_fpu->last_cpu = -1;
 
@@ -190,14 +202,14 @@ int fpu__copy(struct task_struct *dst, struct task_struct *src)
 	 * ( The function 'fails' in the FNSAVE case, which destroys
 	 *   register contents so we have to load them back. )
 	 */
-	fpregs_lock();
+	flags = fpregs_lock();
 	if (test_thread_flag(TIF_NEED_FPU_LOAD))
 		memcpy(&dst_fpu->state, &src_fpu->state, fpu_kernel_xstate_size);
 
 	else if (!copy_fpregs_to_fpstate(dst_fpu))
 		copy_kernel_to_fpregs(&dst_fpu->state);
 
-	fpregs_unlock();
+	fpregs_unlock(flags);
 
 	set_tsk_thread_flag(dst, TIF_NEED_FPU_LOAD);
 
@@ -275,7 +287,9 @@ void fpu__prepare_write(struct fpu *fpu)
  */
 void fpu__drop(struct fpu *fpu)
 {
-	preempt_disable();
+	unsigned long flags;
+
+	flags = hard_preempt_disable();
 
 	if (fpu == &current->thread.fpu) {
 		/* Ignore delayed exceptions from user space */
@@ -287,7 +301,7 @@ void fpu__drop(struct fpu *fpu)
 
 	trace_x86_fpu_dropped(fpu);
 
-	preempt_enable();
+	hard_preempt_enable(flags);
 }
 
 /*
@@ -296,7 +310,9 @@ void fpu__drop(struct fpu *fpu)
  */
 static inline void copy_init_fpstate_to_fpregs(void)
 {
-	fpregs_lock();
+	unsigned long flags;
+
+	flags = fpregs_lock();
 
 	if (use_xsave())
 		copy_kernel_to_xregs(&init_fpstate.xsave, -1);
@@ -309,7 +325,7 @@ static inline void copy_init_fpstate_to_fpregs(void)
 		copy_init_pkru_to_fpregs();
 
 	fpregs_mark_activate();
-	fpregs_unlock();
+	fpregs_unlock(flags);
 }
 
 /*
@@ -320,16 +336,24 @@ static inline void copy_init_fpstate_to_fpregs(void)
  */
 void fpu__clear(struct fpu *fpu)
 {
+	unsigned long flags;
+
 	WARN_ON_FPU(fpu != &current->thread.fpu); /* Almost certainly an anomaly */
 
 	fpu__drop(fpu);
 
 	/*
-	 * Make sure fpstate is cleared and initialized.
+	 * Make sure fpstate is cleared and initialized. When the
+	 * pipeline is enabled, make sure to prevent out-of-band code
+	 * from preempting while the fpstate is under construction.
 	 */
+	flags = hard_cond_local_irq_save();
+
 	fpu__initialize(fpu);
 	if (static_cpu_has(X86_FEATURE_FPU))
 		copy_init_fpstate_to_fpregs();
+
+	hard_cond_local_irq_restore(flags);
 }
 
 /*
@@ -337,10 +361,14 @@ void fpu__clear(struct fpu *fpu)
  */
 void switch_fpu_return(void)
 {
+	unsigned long flags;
+
 	if (!static_cpu_has(X86_FEATURE_FPU))
 		return;
 
+	flags = hard_cond_local_irq_save();
 	__fpregs_load_activate();
+	hard_cond_local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(switch_fpu_return);
 
@@ -440,3 +468,70 @@ int fpu__exception_code(struct fpu *fpu, int trap_nr)
 	 */
 	return 0;
 }
+
+#ifdef CONFIG_DOVETAIL
+
+/*
+ * Holds the in-kernel fpu state when preempted by a task running on
+ * the out-of-band stage.
+ */
+static DEFINE_PER_CPU(struct fpu *, in_kernel_fpstate);
+
+static int fpu__init_kernel_fpstate(unsigned int cpu)
+{
+	struct fpu *fpu;
+
+	fpu = kzalloc(sizeof(*fpu) + fpu_kernel_xstate_size, GFP_KERNEL);
+	if (fpu == NULL)
+		return -ENOMEM;
+
+	this_cpu_write(in_kernel_fpstate, fpu);
+	fpstate_init(&fpu->state);
+
+	return 0;
+}
+
+static int fpu__drop_kernel_fpstate(unsigned int cpu)
+{
+	struct fpu *fpu = this_cpu_read(in_kernel_fpstate);
+
+	kfree(fpu);
+
+	return 0;
+}
+
+void fpu__suspend_inband(void)
+{
+	struct fpu *kfpu = this_cpu_read(in_kernel_fpstate);
+	struct task_struct *tsk = current;
+
+	if (kernel_fpu_disabled()) {
+		copy_fpregs_to_fpstate(kfpu);
+		__cpu_invalidate_fpregs_state();
+		oob_fpu_set_preempt(&tsk->thread.fpu);
+	}
+}
+
+void fpu__resume_inband(void)
+{
+	struct fpu *kfpu = this_cpu_read(in_kernel_fpstate);
+	struct task_struct *tsk = current;
+
+	if (oob_fpu_preempted(&tsk->thread.fpu)) {
+		copy_kernel_to_fpregs(&kfpu->state);
+		__cpu_invalidate_fpregs_state();
+		oob_fpu_clear_preempt(&tsk->thread.fpu);
+	} else if (!(tsk->flags & PF_KTHREAD) &&
+		test_thread_flag(TIF_NEED_FPU_LOAD))
+		switch_fpu_return();
+}
+
+static void __init fpu__init_dovetail(void)
+{
+	cpuhp_setup_state(CPUHP_AP_ONLINE_DYN,
+			"platform/x86/dovetail:online",
+			fpu__init_kernel_fpstate, fpu__drop_kernel_fpstate);
+}
+core_initcall(fpu__init_dovetail);
+
+#endif
diff --git a/arch/x86/kernel/fpu/signal.c b/arch/x86/kernel/fpu/signal.c
index 0071b794ed19..528f88eee96f 100644
--- a/arch/x86/kernel/fpu/signal.c
+++ b/arch/x86/kernel/fpu/signal.c
@@ -61,11 +61,12 @@ static inline int save_fsave_header(struct task_struct *tsk, void __user *buf)
 		struct xregs_state *xsave = &tsk->thread.fpu.state.xsave;
 		struct user_i387_ia32_struct env;
 		struct _fpstate_32 __user *fp = buf;
+		unsigned long flags;
 
-		fpregs_lock();
+		flags = fpregs_lock();
 		if (!test_thread_flag(TIF_NEED_FPU_LOAD))
 			copy_fxregs_to_kernel(&tsk->thread.fpu);
-		fpregs_unlock();
+		fpregs_unlock(flags);
 
 		convert_from_fxsr(&env, tsk);
 
@@ -165,6 +166,7 @@ int copy_fpstate_to_sigframe(void __user *buf, void __user *buf_fx, int size)
 {
 	struct task_struct *tsk = current;
 	int ia32_fxstate = (buf != buf_fx);
+	unsigned long flags;
 	int ret;
 
 	ia32_fxstate &= (IS_ENABLED(CONFIG_X86_32) ||
@@ -185,14 +187,14 @@ int copy_fpstate_to_sigframe(void __user *buf, void __user *buf_fx, int size)
 	 * userland's stack frame which will likely succeed. If it does not,
 	 * resolve the fault in the user memory and try again.
 	 */
-	fpregs_lock();
+	flags = fpregs_lock();
 	if (test_thread_flag(TIF_NEED_FPU_LOAD))
 		__fpregs_load_activate();
 
 	pagefault_disable();
 	ret = copy_fpregs_to_sigframe(buf_fx);
 	pagefault_enable();
-	fpregs_unlock();
+	fpregs_unlock(flags);
 
 	if (ret) {
 		if (!fault_in_pages_writeable(buf_fx, fpu_user_xstate_size))
@@ -277,6 +279,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
 	struct task_struct *tsk = current;
 	struct fpu *fpu = &tsk->thread.fpu;
 	struct user_i387_ia32_struct env;
+	unsigned long flags;
 	u64 xfeatures = 0;
 	int fx_only = 0;
 	int ret = 0;
@@ -343,16 +346,16 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
 		 * going through the kernel buffer with the enabled pagefault
 		 * handler.
 		 */
-		fpregs_lock();
+		flags = fpregs_lock();
 		pagefault_disable();
 		ret = copy_user_to_fpregs_zeroing(buf_fx, xfeatures, fx_only);
 		pagefault_enable();
 		if (!ret) {
 			fpregs_mark_activate();
-			fpregs_unlock();
+			fpregs_unlock(flags);
 			return 0;
 		}
-		fpregs_unlock();
+		fpregs_unlock(flags);
 	}
 
 
@@ -372,7 +375,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
 
 		sanitize_restored_xstate(&fpu->state, envp, xfeatures, fx_only);
 
-		fpregs_lock();
+		flags = fpregs_lock();
 		if (unlikely(init_bv))
 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
 		ret = copy_kernel_to_xregs_err(&fpu->state.xsave, xfeatures);
@@ -386,7 +389,7 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
 
 		sanitize_restored_xstate(&fpu->state, envp, xfeatures, fx_only);
 
-		fpregs_lock();
+		flags = fpregs_lock();
 		if (use_xsave()) {
 			u64 init_bv = xfeatures_mask & ~XFEATURE_MASK_FPSSE;
 			copy_kernel_to_xregs(&init_fpstate.xsave, init_bv);
@@ -398,12 +401,12 @@ static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)
 		if (ret)
 			goto err_out;
 
-		fpregs_lock();
+		flags = fpregs_lock();
 		ret = copy_kernel_to_fregs_err(&fpu->state.fsave);
 	}
 	if (!ret)
 		fpregs_mark_activate();
-	fpregs_unlock();
+	fpregs_unlock(flags);
 
 err_out:
 	if (ret)
diff --git a/arch/x86/kernel/hpet.c b/arch/x86/kernel/hpet.c
index c6f791bc481e..aa0ec1ed1a72 100644
--- a/arch/x86/kernel/hpet.c
+++ b/arch/x86/kernel/hpet.c
@@ -405,7 +405,7 @@ static void hpet_init_clockevent(struct hpet_channel *hc, unsigned int rating)
 	evt->set_next_event	= hpet_clkevt_set_next_event;
 	evt->set_state_shutdown	= hpet_clkevt_set_state_shutdown;
 
-	evt->features = CLOCK_EVT_FEAT_ONESHOT;
+	evt->features = CLOCK_EVT_FEAT_ONESHOT|CLOCK_EVT_FEAT_PIPELINE;
 	if (hc->boot_cfg & HPET_TN_PERIODIC) {
 		evt->features		|= CLOCK_EVT_FEAT_PERIODIC;
 		evt->set_state_periodic	= hpet_clkevt_set_state_periodic;
@@ -518,7 +518,7 @@ static irqreturn_t hpet_msi_interrupt_handler(int irq, void *data)
 		return IRQ_HANDLED;
 	}
 
-	evt->event_handler(evt);
+	clockevents_handle_event(evt);
 	return IRQ_HANDLED;
 }
 
diff --git a/arch/x86/kernel/i8259.c b/arch/x86/kernel/i8259.c
index 519649ddf100..a2d8a2589d23 100644
--- a/arch/x86/kernel/i8259.c
+++ b/arch/x86/kernel/i8259.c
@@ -33,7 +33,7 @@
 static void init_8259A(int auto_eoi);
 
 static int i8259A_auto_eoi;
-DEFINE_RAW_SPINLOCK(i8259A_lock);
+DEFINE_HARD_SPINLOCK(i8259A_lock);
 
 /*
  * 8259A PIC functions to handle ISA devices:
@@ -227,6 +227,7 @@ struct irq_chip i8259A_chip = {
 	.irq_disable	= disable_8259A_irq,
 	.irq_unmask	= enable_8259A_irq,
 	.irq_mask_ack	= mask_and_ack_8259A,
+	.flags		= IRQCHIP_PIPELINE_SAFE,
 };
 
 static char irq_trigger[2];
diff --git a/arch/x86/kernel/idt.c b/arch/x86/kernel/idt.c
index 87ef69a72c52..7158c35785e5 100644
--- a/arch/x86/kernel/idt.c
+++ b/arch/x86/kernel/idt.c
@@ -114,6 +114,9 @@ static const __initconst struct idt_data apic_idts[] = {
 	INTG(CALL_FUNCTION_SINGLE_VECTOR, call_function_single_interrupt),
 	INTG(IRQ_MOVE_CLEANUP_VECTOR,	irq_move_cleanup_interrupt),
 	INTG(REBOOT_VECTOR,		reboot_interrupt),
+#ifdef CONFIG_IRQ_PIPELINE
+	INTG(RESCHEDULE_OOB_VECTOR,	reschedule_oob_interrupt),
+#endif
 #endif
 
 #ifdef CONFIG_X86_THERMAL_VECTOR
@@ -145,6 +148,9 @@ static const __initconst struct idt_data apic_idts[] = {
 	INTG(SPURIOUS_APIC_VECTOR,	spurious_interrupt),
 	INTG(ERROR_APIC_VECTOR,		error_interrupt),
 #endif
+#ifdef CONFIG_IRQ_PIPELINE
+	INTG(TIMER_OOB_VECTOR,		timer_oob_interrupt),
+#endif
 };
 
 #ifdef CONFIG_X86_64
diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c
index 21efee32e2b1..41a29f9ce801 100644
--- a/arch/x86/kernel/irq.c
+++ b/arch/x86/kernel/irq.c
@@ -4,6 +4,7 @@
  */
 #include <linux/cpu.h>
 #include <linux/interrupt.h>
+#include <linux/irq_pipeline.h>
 #include <linux/kernel_stat.h>
 #include <linux/of.h>
 #include <linux/seq_file.h>
@@ -49,7 +50,7 @@ void ack_bad_irq(unsigned int irq)
 	 * completely.
 	 * But only ack when the APIC is enabled -AK
 	 */
-	ack_APIC_irq();
+	__ack_APIC_irq();
 }
 
 #define irq_stats(x)		(&per_cpu(irq_stat, x))
diff --git a/arch/x86/kernel/irq_pipeline.c b/arch/x86/kernel/irq_pipeline.c
new file mode 100644
index 000000000000..452a28698949
--- /dev/null
+++ b/arch/x86/kernel/irq_pipeline.c
@@ -0,0 +1,276 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2019 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <linux/irq.h>
+#include <linux/irq_pipeline.h>
+#include <asm/irqdomain.h>
+#include <asm/apic.h>
+#include <asm/traps.h>
+#include <asm/irq_work.h>
+#include <asm/mshyperv.h>
+
+static struct irq_domain *sipic_domain;
+
+static void sipic_irq_noop(struct irq_data *data) { }
+
+static unsigned int sipic_irq_noop_ret(struct irq_data *data)
+{
+	return 0;
+}
+
+static struct irq_chip sipic_chip = {
+	.name		= "SIPIC",
+	.irq_startup	= sipic_irq_noop_ret,
+	.irq_shutdown	= sipic_irq_noop,
+	.irq_enable	= sipic_irq_noop,
+	.irq_disable	= sipic_irq_noop,
+	.flags		= IRQCHIP_PIPELINE_SAFE | IRQCHIP_SKIP_SET_WAKE,
+};
+
+void handle_apic_irq(struct irq_desc *desc)
+{
+	unsigned int irq = irq_desc_get_irq(desc);
+	struct pt_regs *regs = get_irq_regs(); /* from generic_pipeline_irq() */
+
+	if (WARN_ON_ONCE(irq_pipeline_debug() && !on_pipeline_entry()))
+		return;
+
+	switch (apicm_irq_vector(irq)) {
+	case SPURIOUS_APIC_VECTOR:
+	case ERROR_APIC_VECTOR:
+		/*
+		 * No ack for error events, which should never
+		 * happen. If they do, the situation is messy, leave
+		 * the decision to acknowledge or not to the in-band
+		 * handler.
+		 */
+		break;
+	case THERMAL_APIC_VECTOR:
+		/*
+		 * MCE events are non-maskable, their in-band handlers
+		 * have to be OOB-compatible by construction, so we
+		 * can run them immediately.
+		 */
+		smp_thermal_interrupt(regs);
+		__ack_APIC_irq();
+		return;
+	case THRESHOLD_APIC_VECTOR:
+		smp_threshold_interrupt(regs);
+		__ack_APIC_irq();
+		return;
+	default:
+		__ack_APIC_irq();
+	}
+
+	handle_oob_irq(desc);
+}
+
+void irq_pipeline_send_remote(unsigned int ipi,
+			      const struct cpumask *cpumask)
+{
+	apic->send_IPI_mask_allbutself(cpumask,	apicm_irq_vector(ipi));
+}
+EXPORT_SYMBOL_GPL(irq_pipeline_send_remote);
+
+void uv_bau_message_interrupt(struct pt_regs *regs);
+
+static void do_apic_irq(unsigned int irq, struct pt_regs *regs)
+{
+	int vector = apicm_irq_vector(irq);
+
+	switch (vector) {
+	case SPURIOUS_APIC_VECTOR:
+		smp_spurious_interrupt(regs);
+		break;
+	case ERROR_APIC_VECTOR:
+		smp_error_interrupt(regs);
+		break;
+#ifdef CONFIG_SMP
+	case RESCHEDULE_VECTOR:
+		smp_reschedule_interrupt(regs);
+		break;
+	case CALL_FUNCTION_VECTOR:
+		smp_call_function_interrupt(regs);
+		break;
+	case CALL_FUNCTION_SINGLE_VECTOR:
+		smp_call_function_single_interrupt(regs);
+		break;
+	case REBOOT_VECTOR:
+		smp_reboot_interrupt();
+		break;
+#endif
+	case X86_PLATFORM_IPI_VECTOR:
+		smp_x86_platform_ipi(regs);
+		break;
+	case IRQ_WORK_VECTOR:
+		smp_irq_work_interrupt(regs);
+		break;
+#ifdef CONFIG_X86_UV
+	case UV_BAU_MESSAGE:
+		uv_bau_message_interrupt(regs);
+		break;
+#endif
+#ifdef CONFIG_X86_MCE_AMD
+	case DEFERRED_ERROR_VECTOR:
+		smp_deferred_error_interrupt(regs);
+		break;
+#endif
+#ifdef CONFIG_HAVE_KVM
+	case POSTED_INTR_VECTOR:
+		smp_kvm_posted_intr_ipi(regs);
+		break;
+	case POSTED_INTR_WAKEUP_VECTOR:
+		smp_kvm_posted_intr_wakeup_ipi(regs);
+		break;
+	case POSTED_INTR_NESTED_VECTOR:
+		smp_kvm_posted_intr_nested_ipi(regs);
+		break;
+#endif
+#ifdef CONFIG_HYPERV
+	case HYPERVISOR_CALLBACK_VECTOR:
+		hyperv_vector_handler(regs);
+		break;
+	case HYPERV_REENLIGHTENMENT_VECTOR:
+		hyperv_reenlightenment_intr(regs);
+		break;
+	case HYPERV_STIMER0_VECTOR:
+		hv_stimer0_vector_handler(regs);
+		break;
+#endif
+	case LOCAL_TIMER_VECTOR:
+		smp_apic_timer_interrupt(regs);
+		break;
+	case THERMAL_APIC_VECTOR:
+	case THRESHOLD_APIC_VECTOR:
+		/*
+		 * MCE have been dealt with immediatly on entry to the
+		 * pipeline (see handle_apic_irq()).
+		 */
+		break;
+	default:
+		printk_once(KERN_ERR "irq_pipeline: unexpected event"
+			" on vector #%.2x (irq=%u)", vector, irq);
+	}
+}
+
+void arch_do_IRQ_pipelined(struct irq_desc *desc)
+{
+	struct pt_regs *regs = raw_cpu_ptr(&irq_pipeline.tick_regs);
+	struct pt_regs *old_regs = set_irq_regs(regs);
+	unsigned int irq = irq_desc_get_irq(desc);
+
+	if (desc->irq_data.domain == sipic_domain) {
+		do_apic_irq(irq, regs);
+		return;
+	}
+
+	entering_irq();
+	generic_handle_irq_desc(desc);
+	exiting_irq();
+
+	set_irq_regs(old_regs);
+}
+
+void handle_arch_irq(struct pt_regs *regs)
+{
+	unsigned int irq, vector = ~regs->orig_ax;
+	struct irq_desc *desc;
+
+	if (vector >= FIRST_SYSTEM_VECTOR)
+		irq = apicm_vector_irq(vector);
+	else {
+		desc = __this_cpu_read(vector_irq[vector]);
+		if (unlikely(desc == NULL)) {
+			pr_err("IRQ pipeline: unhandled vector %#.2x\n", vector);
+			return;
+		}
+		irq = irq_desc_get_irq(desc);
+	}
+
+	generic_pipeline_irq(irq, regs);
+}
+
+__visible unsigned int __irq_entry
+handle_arch_irq_pipelined(struct pt_regs *regs)
+{
+	return handle_irq_pipelined(regs);
+}
+
+static int sipic_irq_map(struct irq_domain *d, unsigned int irq,
+			irq_hw_number_t hwirq)
+{
+	irq_set_percpu_devid(irq);
+	irq_set_chip_and_handler(irq, &sipic_chip, handle_apic_irq);
+
+	return 0;
+}
+
+static struct irq_domain_ops sipic_domain_ops = {
+	.map	= sipic_irq_map,
+};
+
+static void create_x86_apic_domain(void)
+{
+	sipic_domain = irq_domain_add_simple(NULL, NR_APIC_VECTORS,
+					     FIRST_SYSTEM_IRQ,
+					     &sipic_domain_ops, NULL);
+}
+
+#ifdef CONFIG_SMP
+
+void handle_irq_move_cleanup(struct irq_desc *desc)
+{
+	if (on_pipeline_entry()) {
+		/* First there on receipt from hardware. */
+		__ack_APIC_irq();
+		handle_oob_irq(desc);
+	} else /* Next there on inband delivery. */
+		smp_irq_move_cleanup_interrupt();
+}
+
+static void smp_setup(void)
+{
+	int irq;
+
+	/*
+	 * The IRQ cleanup event must be pipelined to the inband
+	 * stage, so we need a valid IRQ descriptor for it. Since we
+	 * still are in the early boot stage on CPU0, we ask for a 1:1
+	 * mapping between the vector number and IRQ number, to make
+	 * things easier for us later on.
+	 */
+	irq = irq_alloc_desc_at(IRQ_MOVE_CLEANUP_VECTOR, 0);
+	WARN_ON(IRQ_MOVE_CLEANUP_VECTOR != irq);
+	/*
+	 * Set up the vector_irq[] mapping array for the boot CPU,
+	 * other CPUs will copy this entry when their APIC is going
+	 * online (see lapic_online()).
+	 */
+	per_cpu(vector_irq, 0)[irq] = irq_to_desc(irq);
+
+	irq_set_chip_and_handler(irq, &dummy_irq_chip,
+				handle_irq_move_cleanup);
+}
+
+#else
+
+static void smp_setup(void) { }
+
+#endif
+
+void __init arch_irq_pipeline_init(void)
+{
+	/*
+	 * Create an IRQ domain for mapping APIC system interrupts
+	 * (in-band and out-of-band), with fixed sirq numbers starting
+	 * from FIRST_SYSTEM_IRQ. Upon receipt of a system interrupt,
+	 * the corresponding sirq is injected into the pipeline.
+	 */
+	create_x86_apic_domain();
+
+	smp_setup();
+}
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 5e94c4354d4e..14a8c7b52eb6 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -116,8 +116,17 @@ void exit_thread(struct task_struct *tsk)
 	if (bp) {
 		struct tss_struct *tss = &per_cpu(cpu_tss_rw, get_cpu());
 
-		t->io_bitmap_ptr = NULL;
+		/*
+		 * irq_pipeline: the oob stage may preempt. Make sure
+		 * TIF_IO_BITMAP always denotes a valid I/O bitmap
+		 * when set, by clearing it _before_ the I/O bitmap
+		 * pointer. No cache coherence issue ahead as CPU
+		 * migration is currently locked for the in-band
+		 * stage, and we can't migrate to another CPU over the
+		 * oob stage.
+		 */
 		clear_thread_flag(TIF_IO_BITMAP);
+		t->io_bitmap_ptr = NULL;
 		/*
 		 * Careful, clear this in the TSS too:
 		 */
@@ -482,9 +491,9 @@ void speculation_ctrl_update(unsigned long tif)
 	unsigned long flags;
 
 	/* Forced update. Make sure all relevant TIF flags are different */
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	__speculation_ctrl_update(~tif, tif);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /* Called from seccomp/prctl update */
@@ -578,6 +587,8 @@ void __cpuidle default_idle(void)
 {
 	trace_cpu_idle_rcuidle(1, smp_processor_id());
 	safe_halt();
+	if (irqs_pipelined())
+		local_irq_enable();
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 #if defined(CONFIG_APM_MODULE) || defined(CONFIG_HALTPOLL_CPUIDLE_MODULE)
@@ -597,7 +608,7 @@ bool xen_set_default_idle(void)
 
 void stop_this_cpu(void *dummy)
 {
-	local_irq_disable();
+	hard_local_irq_disable();
 	/*
 	 * Remove this CPU:
 	 */
@@ -692,13 +703,15 @@ static __cpuidle void mwait_idle(void)
 		}
 
 		__monitor((void *)&current_thread_info()->flags, 0, 0);
-		if (!need_resched())
+		if (!need_resched()) {
 			__sti_mwait(0, 0);
-		else
-			local_irq_enable();
+			if (irqs_pipelined())
+				local_irq_enable();
+		} else
+			local_irq_enable_full();
 		trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 	} else {
-		local_irq_enable();
+		local_irq_enable_full();
 	}
 	__current_clr_polling();
 }
diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c
index af64519b2695..dd7b4941eb42 100644
--- a/arch/x86/kernel/process_64.c
+++ b/arch/x86/kernel/process_64.c
@@ -512,6 +512,8 @@ __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
 	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &&
 		     this_cpu_read(irq_count) != -1);
 
+	WARN_ON_ONCE(dovetail_debug() && !hard_irqs_disabled());
+
 	if (!test_thread_flag(TIF_NEED_FPU_LOAD))
 		switch_fpu_prepare(prev_fpu, cpu);
 
@@ -702,6 +704,7 @@ static long prctl_map_vdso(const struct vdso_image *image, unsigned long addr)
 
 long do_arch_prctl_64(struct task_struct *task, int option, unsigned long arg2)
 {
+	unsigned long flags;
 	int ret = 0;
 
 	switch (option) {
@@ -709,7 +712,7 @@ long do_arch_prctl_64(struct task_struct *task, int option, unsigned long arg2)
 		if (unlikely(arg2 >= TASK_SIZE_MAX))
 			return -EPERM;
 
-		preempt_disable();
+		flags = hard_preempt_disable();
 		/*
 		 * ARCH_SET_GS has always overwritten the index
 		 * and the base. Zero is the most sensible value
@@ -730,7 +733,7 @@ long do_arch_prctl_64(struct task_struct *task, int option, unsigned long arg2)
 			task->thread.gsindex = 0;
 			x86_gsbase_write_task(task, arg2);
 		}
-		preempt_enable();
+		hard_preempt_enable(flags);
 		break;
 	}
 	case ARCH_SET_FS: {
@@ -741,7 +744,7 @@ long do_arch_prctl_64(struct task_struct *task, int option, unsigned long arg2)
 		if (unlikely(arg2 >= TASK_SIZE_MAX))
 			return -EPERM;
 
-		preempt_disable();
+		flags = hard_preempt_disable();
 		/*
 		 * Set the selector to 0 for the same reason
 		 * as %gs above.
@@ -759,7 +762,7 @@ long do_arch_prctl_64(struct task_struct *task, int option, unsigned long arg2)
 			task->thread.fsindex = 0;
 			x86_fsbase_write_task(task, arg2);
 		}
-		preempt_enable();
+		hard_preempt_enable(flags);
 		break;
 	}
 	case ARCH_GET_FS: {
diff --git a/arch/x86/kernel/smp.c b/arch/x86/kernel/smp.c
index b8d4e9c3c070..5a75d586e5cb 100644
--- a/arch/x86/kernel/smp.c
+++ b/arch/x86/kernel/smp.c
@@ -213,10 +213,10 @@ static void native_stop_other_cpus(int wait)
 			udelay(1);
 	}
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	disable_local_APIC();
 	mcheck_cpu_clear(this_cpu_ptr(&cpu_info));
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 /*
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 69881b2d446c..6dd327510268 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -253,7 +253,7 @@ static void notrace start_secondary(void *unused)
 	x86_platform.nmi_init();
 
 	/* enable local interrupts */
-	local_irq_enable();
+	local_irq_enable_full();
 
 	/* to prevent fake stack check failure in clock setup */
 	boot_init_stack_canary();
@@ -1128,7 +1128,6 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
 	int apicid = apic->cpu_present_to_apicid(cpu);
 	int cpu0_nmi_registered = 0;
-	unsigned long flags;
 	int err, ret = 0;
 
 	lockdep_assert_irqs_enabled();
@@ -1179,9 +1178,9 @@ int native_cpu_up(unsigned int cpu, struct task_struct *tidle)
 	 * Check TSC synchronization with the AP (keep irqs disabled
 	 * while doing so):
 	 */
-	local_irq_save(flags);
+	local_irq_disable_full();
 	check_tsc_sync_source(cpu);
-	local_irq_restore(flags);
+	local_irq_enable_full();
 
 	while (!cpu_online(cpu)) {
 		cpu_relax();
@@ -1635,7 +1634,7 @@ void play_dead_common(void)
 	/*
 	 * With physical CPU hotplug, we should halt the cpu
 	 */
-	local_irq_disable();
+	local_irq_disable_full();
 }
 
 static bool wakeup_cpu0(void)
diff --git a/arch/x86/kernel/time.c b/arch/x86/kernel/time.c
index 7ce29cee9f9e..1326d5dbe967 100644
--- a/arch/x86/kernel/time.c
+++ b/arch/x86/kernel/time.c
@@ -58,7 +58,7 @@ EXPORT_SYMBOL(profile_pc);
  */
 static irqreturn_t timer_interrupt(int irq, void *dev_id)
 {
-	global_clock_event->event_handler(global_clock_event);
+	clockevents_handle_event(global_clock_event);
 	return IRQ_HANDLED;
 }
 
diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c
index 4bb0f8447112..77e1fbc14a01 100644
--- a/arch/x86/kernel/traps.c
+++ b/arch/x86/kernel/traps.c
@@ -14,6 +14,7 @@
 
 #include <linux/context_tracking.h>
 #include <linux/interrupt.h>
+#include <linux/irq_pipeline.h>
 #include <linux/kallsyms.h>
 #include <linux/spinlock.h>
 #include <linux/kprobes.h>
@@ -74,16 +75,66 @@
 
 DECLARE_BITMAP(system_vectors, NR_VECTORS);
 
+#ifdef CONFIG_IRQ_PIPELINE
+
+unsigned long pipelined_fault_entry(int trapnr, struct pt_regs *regs)
+{
+	unsigned long flags;
+	int nosync = 1;
+
+	/*
+	 * NOTE: having entered the IST context when the companion
+	 * core is notified that an exception has been taken from
+	 * out-of-band context is not an issue. At best that core
+	 * could plan for a deferred switch to inband mode, which by
+	 * definition cannot involve immediate schedule().
+	 */
+	oob_trap_notify(trapnr, regs);
+
+	flags = hard_local_irq_save();
+
+	if (hard_irqs_disabled_flags(flags)) {
+		nosync = test_and_set_stage_bit(STAGE_STALL_BIT,
+					this_inband_staged());
+		trace_hardirqs_off();
+	}
+	if (oob_stage_present())
+		hard_local_irq_enable();
+
+	return irqs_merge_flags(flags, nosync);
+}
+
+void pipelined_fault_exit(unsigned long combo)
+{
+	unsigned long flags;
+	int nosync;
+
+	WARN_ON_ONCE(irq_pipeline_debug() &&
+		oob_stage_present() && hard_irqs_disabled());
+
+	flags = irqs_split_flags(combo, &nosync);
+	if (!nosync) {
+		hard_local_irq_disable();
+		trace_hardirqs_on();
+		clear_stage_bit(STAGE_STALL_BIT, this_inband_staged());
+		if (!hard_irqs_disabled_flags(flags))
+			hard_local_irq_enable();
+	} else if (hard_irqs_disabled_flags(flags))
+		hard_local_irq_disable();
+}
+
+#endif	/* CONFIG_IRQ_PIPELINE */
+
 static inline void cond_local_irq_enable(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
-		local_irq_enable();
+		local_irq_enable_full();
 }
 
 static inline void cond_local_irq_disable(struct pt_regs *regs)
 {
 	if (regs->flags & X86_EFLAGS_IF)
-		local_irq_disable();
+		hard_local_irq_disable();
 }
 
 /*
@@ -263,6 +314,8 @@ NOKPROBE_SYMBOL(do_trap);
 static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 	unsigned long trapnr, int signr, int sicode, void __user *addr)
 {
+	unsigned long flags;
+
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
 	/*
@@ -272,11 +325,15 @@ static void do_error_trap(struct pt_regs *regs, long error_code, char *str,
 	if (!user_mode(regs) && fixup_bug(regs, trapnr))
 		return;
 
+	flags = pipelined_fault_entry(trapnr, regs);
+
 	if (notify_die(DIE_TRAP, str, regs, error_code, trapnr, signr) !=
 			NOTIFY_STOP) {
 		cond_local_irq_enable(regs);
 		do_trap(trapnr, signr, str, regs, error_code, sicode, addr);
 	}
+
+	pipelined_fault_exit(flags);
 }
 
 #define IP ((void __user *)uprobe_get_trap_addr(regs))
@@ -431,6 +488,9 @@ dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code, unsign
 dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 {
 	const struct mpx_bndcsr *bndcsr;
+	unsigned long flags;
+
+	flags = pipelined_fault_entry(X86_TRAP_BR, regs);
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	if (notify_die(DIE_TRAP, "bounds", regs, error_code,
@@ -501,7 +561,7 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 		die("bounds", regs, error_code);
 	}
 
-	return;
+	goto out;
 
 exit_trap:
 	/*
@@ -512,6 +572,8 @@ dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)
 	 * time..
 	 */
 	do_trap(X86_TRAP_BR, SIGSEGV, "bounds", regs, error_code, 0, NULL);
+out:
+	pipelined_fault_exit(flags);
 }
 
 dotraplinkage void
@@ -519,25 +581,28 @@ do_general_protection(struct pt_regs *regs, long error_code)
 {
 	const char *desc = "general protection fault";
 	struct task_struct *tsk;
+	unsigned long flags;
+
+	flags = pipelined_fault_entry(X86_TRAP_GP, regs);
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 	cond_local_irq_enable(regs);
 
 	if (static_cpu_has(X86_FEATURE_UMIP)) {
 		if (user_mode(regs) && fixup_umip_exception(regs))
-			return;
+			goto out;
 	}
 
 	if (v8086_mode(regs)) {
-		local_irq_enable();
+		hard_local_irq_enable();
 		handle_vm86_fault((struct kernel_vm86_regs *) regs, error_code);
-		return;
+		goto out;
 	}
 
 	tsk = current;
 	if (!user_mode(regs)) {
 		if (fixup_exception(regs, X86_TRAP_GP, error_code, 0))
-			return;
+			goto out;
 
 		tsk->thread.error_code = error_code;
 		tsk->thread.trap_nr = X86_TRAP_GP;
@@ -549,12 +614,12 @@ do_general_protection(struct pt_regs *regs, long error_code)
 		 */
 		if (!preemptible() && kprobe_running() &&
 		    kprobe_fault_handler(regs, X86_TRAP_GP))
-			return;
+			goto out;
 
 		if (notify_die(DIE_GPF, desc, regs, error_code,
 			       X86_TRAP_GP, SIGSEGV) != NOTIFY_STOP)
 			die(desc, regs, error_code);
-		return;
+		goto out;
 	}
 
 	tsk->thread.error_code = error_code;
@@ -563,11 +628,15 @@ do_general_protection(struct pt_regs *regs, long error_code)
 	show_signal(tsk, SIGSEGV, "", desc, regs, error_code);
 
 	force_sig(SIGSEGV);
+out:
+	pipelined_fault_exit(flags);
 }
 NOKPROBE_SYMBOL(do_general_protection);
 
 dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 {
+	unsigned long flags;
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 	/*
 	 * ftrace must be first, everything else may cause a recursive crash.
@@ -600,13 +669,17 @@ dotraplinkage void notrace do_int3(struct pt_regs *regs, long error_code)
 		goto exit;
 #endif
 
+	flags = pipelined_fault_entry(X86_TRAP_BP, regs);
+
 	if (notify_die(DIE_INT3, "int3", regs, error_code, X86_TRAP_BP,
 			SIGTRAP) == NOTIFY_STOP)
-		goto exit;
+		goto exit_fault;
 
 	cond_local_irq_enable(regs);
 	do_trap(X86_TRAP_BP, SIGTRAP, "int3", regs, error_code, 0, NULL);
 	cond_local_irq_disable(regs);
+exit_fault:
+	pipelined_fault_exit(flags);
 
 exit:
 	ist_exit(regs);
@@ -709,8 +782,8 @@ static bool is_sysenter_singlestep(struct pt_regs *regs)
 dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 {
 	struct task_struct *tsk = current;
+	unsigned long dr6, flags;
 	int user_icebp = 0;
-	unsigned long dr6;
 	int si_code;
 
 	ist_enter(regs);
@@ -743,7 +816,7 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 		     is_sysenter_singlestep(regs))) {
 		dr6 &= ~DR_STEP;
 		if (!dr6)
-			goto exit;
+			goto out;
 		/*
 		 * else we might have gotten a single-step trap and hit a
 		 * watchpoint at the same time, in which case we should fall
@@ -764,9 +837,11 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 
 #ifdef CONFIG_KPROBES
 	if (kprobe_debug_handler(regs))
-		goto exit;
+		goto out;
 #endif
 
+	flags = pipelined_fault_entry(X86_TRAP_DB, regs);
+
 	if (notify_die(DIE_DEBUG, "debug", regs, (long)&dr6, error_code,
 							SIGTRAP) == NOTIFY_STOP)
 		goto exit;
@@ -806,6 +881,8 @@ dotraplinkage void do_debug(struct pt_regs *regs, long error_code)
 	debug_stack_usage_dec();
 
 exit:
+	pipelined_fault_exit(flags);
+out:
 	ist_exit(regs);
 }
 NOKPROBE_SYMBOL(do_debug);
@@ -819,15 +896,18 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 {
 	struct task_struct *task = current;
 	struct fpu *fpu = &task->thread.fpu;
+	unsigned long flags;
 	int si_code;
 	char *str = (trapnr == X86_TRAP_MF) ? "fpu exception" :
 						"simd exception";
 
+	flags = pipelined_fault_entry(trapnr, regs);
+
 	cond_local_irq_enable(regs);
 
 	if (!user_mode(regs)) {
 		if (fixup_exception(regs, trapnr, error_code, 0))
-			return;
+			goto out;
 
 		task->thread.error_code = error_code;
 		task->thread.trap_nr = trapnr;
@@ -835,7 +915,7 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 		if (notify_die(DIE_TRAP, str, regs, error_code,
 					trapnr, SIGFPE) != NOTIFY_STOP)
 			die(str, regs, error_code);
-		return;
+		goto out;
 	}
 
 	/*
@@ -849,10 +929,12 @@ static void math_error(struct pt_regs *regs, int error_code, int trapnr)
 	si_code = fpu__exception_code(fpu, trapnr);
 	/* Retry when we get spurious exceptions: */
 	if (!si_code)
-		return;
+		goto out;
 
 	force_sig_fault(SIGFPE, si_code,
 			(void __user *)uprobe_get_trap_addr(regs));
+out:
+	pipelined_fault_exit(flags);
 }
 
 dotraplinkage void do_coprocessor_error(struct pt_regs *regs, long error_code)
@@ -877,7 +959,7 @@ do_spurious_interrupt_bug(struct pt_regs *regs, long error_code)
 dotraplinkage void
 do_device_not_available(struct pt_regs *regs, long error_code)
 {
-	unsigned long cr0 = read_cr0();
+	unsigned long cr0 = read_cr0(), flags __maybe_unused;
 
 	RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
 
@@ -885,10 +967,14 @@ do_device_not_available(struct pt_regs *regs, long error_code)
 	if (!boot_cpu_has(X86_FEATURE_FPU) && (cr0 & X86_CR0_EM)) {
 		struct math_emu_info info = { };
 
+		flags = pipelined_fault_entry(X86_TRAP_NM, regs);
+
 		cond_local_irq_enable(regs);
 
 		info.regs = regs;
 		math_emulate(&info);
+
+		pipelined_fault_exit(flags);
 		return;
 	}
 #endif
diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index 7e322e2daaf5..07ee274ba8a9 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -124,8 +124,11 @@ static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long ts
 {
 	unsigned long long ns_now;
 	struct cyc2ns_data data;
+	unsigned long flags;
 	struct cyc2ns *c2n;
 
+	flags = hard_cond_local_irq_save();
+
 	ns_now = cycles_2_ns(tsc_now);
 
 	/*
@@ -156,6 +159,8 @@ static void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long ts
 	c2n->data[0] = data;
 	raw_write_seqcount_latch(&c2n->seq);
 	c2n->data[1] = data;
+
+	hard_cond_local_irq_restore(flags);
 }
 
 static void set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)
@@ -752,11 +757,11 @@ static unsigned long pit_hpet_ptimer_calibrate_cpu(void)
 		 * calibration, which will take at least 50ms, and
 		 * read the end value.
 		 */
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 		tsc1 = tsc_read_refs(&ref1, hpet);
 		tsc_pit_khz = pit_calibrate_tsc(latch, ms, loopmin);
 		tsc2 = tsc_read_refs(&ref2, hpet);
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 
 		/* Pick the lowest PIT TSC calibration so far */
 		tsc_pit_min = min(tsc_pit_min, tsc_pit_khz);
@@ -865,9 +870,9 @@ unsigned long native_calibrate_cpu_early(void)
 	if (!fast_calibrate)
 		fast_calibrate = cpu_khz_from_msr();
 	if (!fast_calibrate) {
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 		fast_calibrate = quick_pit_calibrate();
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 	}
 	return fast_calibrate;
 }
@@ -935,7 +940,7 @@ void tsc_restore_sched_clock_state(void)
 	if (!sched_clock_stable())
 		return;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	/*
 	 * We're coming out of suspend, there's no concurrency yet; don't
@@ -953,7 +958,7 @@ void tsc_restore_sched_clock_state(void)
 		per_cpu(cyc2ns.data[1].cyc2ns_offset, cpu) = offset;
 	}
 
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 #ifdef CONFIG_CPU_FREQ
diff --git a/arch/x86/kernel/tsc_sync.c b/arch/x86/kernel/tsc_sync.c
index ec534f978867..91e9d0f04f56 100644
--- a/arch/x86/kernel/tsc_sync.c
+++ b/arch/x86/kernel/tsc_sync.c
@@ -327,6 +327,8 @@ void check_tsc_sync_source(int cpu)
 		atomic_set(&test_runs, 1);
 	else
 		atomic_set(&test_runs, 3);
+
+	hard_cond_local_irq_disable();
 retry:
 	/*
 	 * Wait for the target to start or to skip the test:
@@ -408,6 +410,8 @@ void check_tsc_sync_target(void)
 	if (unsynchronized_tsc())
 		return;
 
+	hard_cond_local_irq_disable();
+
 	/*
 	 * Store, verify and sanitize the TSC adjust register. If
 	 * successful skip the test.
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 5d530521f11d..643b065bf759 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -8439,7 +8439,9 @@ static int complete_emulated_mmio(struct kvm_vcpu *vcpu)
 /* Swap (qemu) user FPU context for the guest FPU context. */
 static void kvm_load_guest_fpu(struct kvm_vcpu *vcpu)
 {
-	fpregs_lock();
+	unsigned long flags;
+
+	flags = fpregs_lock();
 
 	copy_fpregs_to_fpstate(vcpu->arch.user_fpu);
 	/* PKRU is separately restored in kvm_x86_ops->run.  */
@@ -8447,7 +8449,7 @@ static void kvm_load_guest_fpu(struct kvm_vcpu *vcpu)
 				~XFEATURE_MASK_PKRU);
 
 	fpregs_mark_activate();
-	fpregs_unlock();
+	fpregs_unlock(flags);
 
 	trace_kvm_fpu(1);
 }
@@ -8455,13 +8457,15 @@ static void kvm_load_guest_fpu(struct kvm_vcpu *vcpu)
 /* When vcpu_run ends, restore user space FPU context. */
 static void kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 {
-	fpregs_lock();
+	unsigned long flags;
+
+	flags = fpregs_lock();
 
 	copy_fpregs_to_fpstate(vcpu->arch.guest_fpu);
 	copy_kernel_to_fpregs(&vcpu->arch.user_fpu->state);
 
 	fpregs_mark_activate();
-	fpregs_unlock();
+	fpregs_unlock(flags);
 
 	++vcpu->stat.fpu_reload;
 	trace_kvm_fpu(0);
diff --git a/arch/x86/lib/usercopy.c b/arch/x86/lib/usercopy.c
index 3f435d7fca5e..1cdb806bd689 100644
--- a/arch/x86/lib/usercopy.c
+++ b/arch/x86/lib/usercopy.c
@@ -18,7 +18,7 @@ copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
 {
 	unsigned long ret;
 
-	if (__range_not_ok(from, n, TASK_SIZE))
+	if (running_oob() || __range_not_ok(from, n, TASK_SIZE))
 		return n;
 
 	if (!nmi_uaccess_okay())
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 9ceacd1156db..0b086a92c0e0 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -15,6 +15,7 @@
 #include <linux/hugetlb.h>		/* hstate_index_to_shift	*/
 #include <linux/prefetch.h>		/* prefetchw			*/
 #include <linux/context_tracking.h>	/* exception_enter(), ...	*/
+#include <linux/irq_pipeline.h>		/* pipelined_fault_entry(), ...	*/
 #include <linux/uaccess.h>		/* faulthandler_disabled()	*/
 #include <linux/efi.h>			/* efi_recover_from_page_fault()*/
 #include <linux/mm_types.h>
@@ -708,7 +709,7 @@ no_context(struct pt_regs *regs, unsigned long error_code,
 	   unsigned long address, int signal, int si_code)
 {
 	struct task_struct *tsk = current;
-	unsigned long flags;
+	unsigned long flags, entry_flags = 0;
 	int sig;
 
 	if (user_mode(regs)) {
@@ -727,7 +728,7 @@ no_context(struct pt_regs *regs, unsigned long error_code,
 		 * the below recursive fault logic only apply to a faults from
 		 * task context.
 		 */
-		if (in_interrupt())
+		if (running_inband() && in_interrupt())
 			return;
 
 		/*
@@ -737,10 +738,19 @@ no_context(struct pt_regs *regs, unsigned long error_code,
 		 * faulting through the emulate_vsyscall() logic.
 		 */
 		if (current->thread.sig_on_uaccess_err && signal) {
+			/*
+			 * If !user_mode(regs), we did not notify the
+			 * pipeline about this fault, do this now
+			 * before we (re-enter inband code.
+			 */
+			entry_flags = pipelined_fault_entry(X86_TRAP_PF, regs);
+
 			set_signal_archinfo(address, error_code);
 
 			/* XXX: hwpoison faults will set the wrong code. */
 			force_sig_fault(signal, si_code, (void __user *)address);
+
+			pipelined_fault_exit(entry_flags);
 		}
 
 		/*
@@ -797,6 +807,14 @@ no_context(struct pt_regs *regs, unsigned long error_code,
 	if (is_errata93(regs, address))
 		return;
 
+	/*
+	 * The odds we can survive to this PF over an out-of-band
+	 * context are pretty high, but if a companion core is
+	 * present, we want to let it know so it might try desperate
+	 * fixups as well.
+	 */
+	entry_flags = pipelined_fault_entry(X86_TRAP_PF, regs);
+
 	/*
 	 * Buggy firmware could access regions which might page fault, try to
 	 * recover from such faults.
@@ -824,6 +842,9 @@ no_context(struct pt_regs *regs, unsigned long error_code,
 	printk(KERN_DEFAULT "CR2: %016lx\n", address);
 
 	oops_end(flags, regs, sig);
+
+	if (!user_mode(regs))
+		pipelined_fault_exit(entry_flags);
 }
 
 /*
@@ -873,7 +894,7 @@ __bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,
 		/*
 		 * It's possible to have interrupts off here:
 		 */
-		local_irq_enable();
+		local_irq_enable_full();
 
 		/*
 		 * Valid to do another page fault here because this one came
@@ -1335,11 +1356,11 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * potential system fault or CPU buglet:
 	 */
 	if (user_mode(regs)) {
-		local_irq_enable();
+		local_irq_enable_full();
 		flags |= FAULT_FLAG_USER;
 	} else {
 		if (regs->flags & X86_EFLAGS_IF)
-			local_irq_enable();
+			local_irq_enable_full();
 	}
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
@@ -1494,6 +1515,8 @@ static noinline void
 __do_page_fault(struct pt_regs *regs, unsigned long hw_error_code,
 		unsigned long address)
 {
+	unsigned long flags;
+
 	prefetchw(&current->mm->mmap_sem);
 
 	if (unlikely(kmmio_fault(regs, address)))
@@ -1502,8 +1525,11 @@ __do_page_fault(struct pt_regs *regs, unsigned long hw_error_code,
 	/* Was the fault on kernel-controlled part of the address space? */
 	if (unlikely(fault_in_kernel_space(address)))
 		do_kern_addr_fault(regs, hw_error_code, address);
-	else
+	else {
+		flags = pipelined_fault_entry(X86_TRAP_PF, regs);
 		do_user_addr_fault(regs, hw_error_code, address);
+		pipelined_fault_exit(flags);
+	}
 }
 NOKPROBE_SYMBOL(__do_page_fault);
 
@@ -1531,3 +1557,41 @@ do_page_fault(struct pt_regs *regs, unsigned long error_code, unsigned long addr
 	exception_exit(prev_state);
 }
 NOKPROBE_SYMBOL(do_page_fault);
+
+#ifdef CONFIG_DOVETAIL
+
+void arch_advertise_page_mapping(unsigned long start, unsigned long end)
+{
+	unsigned long next, addr = start;
+	pgd_t *pgd, *pgd_ref;
+	struct page *page;
+
+	/*
+	 * APEI may create temporary mappings in interrupt context -
+	 * nothing we can and need to propagate globally.
+	 */
+	if (in_interrupt())
+		return;
+
+	if (!(start >= VMALLOC_START && start < VMALLOC_END))
+		return;
+
+	do {
+		next = pgd_addr_end(addr, end);
+		pgd_ref = pgd_offset_k(addr);
+		if (pgd_none(*pgd_ref))
+			continue;
+		spin_lock(&pgd_lock);
+		list_for_each_entry(page, &pgd_list, lru) {
+			pgd = page_address(page) + pgd_index(addr);
+			if (pgd_none(*pgd))
+				set_pgd(pgd, *pgd_ref);
+		}
+		spin_unlock(&pgd_lock);
+		addr = next;
+	} while (addr != end);
+
+	arch_flush_lazy_mmu_mode();
+}
+
+#endif
diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c
index e6a9edc5baaf..af56ddfe366b 100644
--- a/arch/x86/mm/tlb.c
+++ b/arch/x86/mm/tlb.c
@@ -154,10 +154,12 @@ EXPORT_SYMBOL_GPL(leave_mm);
 void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	       struct task_struct *tsk)
 {
-	unsigned long flags;
+	unsigned long flags, _flags;
 
 	local_irq_save(flags);
+	protect_inband_mm(_flags);
 	switch_mm_irqs_off(prev, next, tsk);
+	unprotect_inband_mm(_flags);
 	local_irq_restore(flags);
 }
 
@@ -293,7 +295,9 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 	 */
 
 	/* We don't want flush_tlb_func_* to run concurrently with us. */
-	if (IS_ENABLED(CONFIG_PROVE_LOCKING))
+	if (IS_ENABLED(CONFIG_DOVETAIL))
+		WARN_ON_ONCE(!hard_irqs_disabled());
+	else if (IS_ENABLED(CONFIG_PROVE_LOCKING))
 		WARN_ON_ONCE(!irqs_disabled());
 
 	/*
@@ -531,15 +535,23 @@ static void flush_tlb_func_common(const struct flush_tlb_info *f,
 	 *                   wants us to catch up to.
 	 */
 	struct mm_struct *loaded_mm = this_cpu_read(cpu_tlbstate.loaded_mm);
-	u32 loaded_mm_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);
-	u64 mm_tlb_gen = atomic64_read(&loaded_mm->context.tlb_gen);
-	u64 local_tlb_gen = this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen);
+	u64 mm_tlb_gen, local_tlb_gen;
+	u32 loaded_mm_asid;
+	unsigned long flags;
 
 	/* This code cannot presently handle being reentered. */
 	VM_WARN_ON(!irqs_disabled());
 
-	if (unlikely(loaded_mm == &init_mm))
+	protect_inband_mm(flags);
+
+	loaded_mm_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);
+	mm_tlb_gen = atomic64_read(&loaded_mm->context.tlb_gen);
+	local_tlb_gen = this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen);
+
+	if (unlikely(loaded_mm == &init_mm)) {
+		unprotect_inband_mm(flags);
 		return;
+	}
 
 	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].ctx_id) !=
 		   loaded_mm->context.ctx_id);
@@ -555,6 +567,7 @@ static void flush_tlb_func_common(const struct flush_tlb_info *f,
 		 * IPIs to lazy TLB mode CPUs.
 		 */
 		switch_mm_irqs_off(NULL, &init_mm, NULL);
+		unprotect_inband_mm(flags);
 		return;
 	}
 
@@ -565,6 +578,7 @@ static void flush_tlb_func_common(const struct flush_tlb_info *f,
 		 * be handled can catch us all the way up, leaving no work for
 		 * the second flush.
 		 */
+		unprotect_inband_mm(flags);
 		trace_tlb_flush(reason, 0);
 		return;
 	}
@@ -572,6 +586,8 @@ static void flush_tlb_func_common(const struct flush_tlb_info *f,
 	WARN_ON_ONCE(local_tlb_gen > mm_tlb_gen);
 	WARN_ON_ONCE(f->new_tlb_gen > mm_tlb_gen);
 
+	unprotect_inband_mm(flags);
+
 	/*
 	 * If we get to this point, we know that our TLB is out of date.
 	 * This does not strictly imply that we need to flush (it's
diff --git a/arch/x86/xen/Kconfig b/arch/x86/xen/Kconfig
index ba5a41828e9d..20514f255127 100644
--- a/arch/x86/xen/Kconfig
+++ b/arch/x86/xen/Kconfig
@@ -5,7 +5,7 @@
 
 config XEN
 	bool "Xen guest support"
-	depends on PARAVIRT
+	depends on PARAVIRT && !IRQ_PIPELINE
 	select PARAVIRT_CLOCK
 	select X86_HV_CALLBACK_VECTOR
 	depends on X86_64 || (X86_32 && X86_PAE)
diff --git a/drivers/base/regmap/internal.h b/drivers/base/regmap/internal.h
index 3d80c4b43f72..d8cfa86b47b0 100644
--- a/drivers/base/regmap/internal.h
+++ b/drivers/base/regmap/internal.h
@@ -50,7 +50,10 @@ struct regmap {
 	union {
 		struct mutex mutex;
 		struct {
-			spinlock_t spinlock;
+			union {
+				spinlock_t spinlock;
+				hard_spinlock_t oob_lock;
+			};
 			unsigned long spinlock_flags;
 		};
 	};
diff --git a/drivers/base/regmap/regmap-irq.c b/drivers/base/regmap/regmap-irq.c
index 3d64c9331a82..0d1e6ccb5bbe 100644
--- a/drivers/base/regmap/regmap-irq.c
+++ b/drivers/base/regmap/regmap-irq.c
@@ -324,6 +324,7 @@ static const struct irq_chip regmap_irq_chip = {
 	.irq_enable		= regmap_irq_enable,
 	.irq_set_type		= regmap_irq_set_type,
 	.irq_set_wake		= regmap_irq_set_wake,
+	.flags			= IRQCHIP_PIPELINE_SAFE,
 };
 
 static inline int read_sub_irq_data(struct regmap_irq_chip_data *data,
diff --git a/drivers/base/regmap/regmap.c b/drivers/base/regmap/regmap.c
index 19f57ccfbe1d..93dff94539b8 100644
--- a/drivers/base/regmap/regmap.c
+++ b/drivers/base/regmap/regmap.c
@@ -14,6 +14,7 @@
 #include <linux/of.h>
 #include <linux/rbtree.h>
 #include <linux/sched.h>
+#include <linux/dovetail.h>
 #include <linux/delay.h>
 #include <linux/log2.h>
 #include <linux/hwspinlock.h>
@@ -519,6 +520,23 @@ __releases(&map->spinlock)
 	spin_unlock_irqrestore(&map->spinlock, map->spinlock_flags);
 }
 
+static void regmap_lock_oob(void *__map)
+__acquires(&map->oob_lock)
+{
+	struct regmap *map = __map;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&map->oob_lock, flags);
+	map->spinlock_flags = flags;
+}
+
+static void regmap_unlock_oob(void *__map)
+__releases(&map->oob_lock)
+{
+	struct regmap *map = __map;
+	raw_spin_unlock_irqrestore(&map->oob_lock, map->spinlock_flags);
+}
+
 static void dev_get_regmap_release(struct device *dev, void *res)
 {
 	/*
@@ -741,17 +759,28 @@ struct regmap *__regmap_init(struct device *dev,
 	} else {
 		if ((bus && bus->fast_io) ||
 		    config->fast_io) {
-			spin_lock_init(&map->spinlock);
-			map->lock = regmap_lock_spinlock;
-			map->unlock = regmap_unlock_spinlock;
-			lockdep_set_class_and_name(&map->spinlock,
-						   lock_key, lock_name);
-		} else {
+			if (dovetailing() && config->oob_io) {
+				raw_spin_lock_init(&map->oob_lock);
+				map->lock = regmap_lock_oob;
+				map->unlock = regmap_unlock_oob;
+				lockdep_set_class_and_name(&map->oob_lock,
+							lock_key, lock_name);
+			} else {
+				spin_lock_init(&map->spinlock);
+				map->lock = regmap_lock_spinlock;
+				map->unlock = regmap_unlock_spinlock;
+				lockdep_set_class_and_name(&map->spinlock,
+							lock_key, lock_name);
+			}
+		} else if (!config->oob_io) {
 			mutex_init(&map->mutex);
 			map->lock = regmap_lock_mutex;
 			map->unlock = regmap_unlock_mutex;
 			lockdep_set_class_and_name(&map->mutex,
 						   lock_key, lock_name);
+		} else {
+			ret = -ENXIO;
+			goto err_name;
 		}
 		map->lock_arg = map;
 	}
diff --git a/drivers/clocksource/Kconfig b/drivers/clocksource/Kconfig
index f35a53ce8988..a03d410dcb99 100644
--- a/drivers/clocksource/Kconfig
+++ b/drivers/clocksource/Kconfig
@@ -31,6 +31,11 @@ config CLKBLD_I8253
 config CLKSRC_MMIO
 	bool
 
+config CLKSRC_VDSO_MAPPED
+        select CLKSRC_MMIO
+	select VDSO
+	bool
+
 config BCM2835_TIMER
 	bool "BCM2835 timer driver" if COMPILE_TEST
 	select CLKSRC_MMIO
@@ -57,6 +62,7 @@ config DIGICOLOR_TIMER
 
 config DW_APB_TIMER
 	bool "DW APB timer driver" if COMPILE_TEST
+	select CLKSRC_VDSO_MAPPED
 	help
 	  Enables the support for the dw_apb timer.
 
@@ -388,6 +394,7 @@ config SUN50I_ERRATUM_UNKNOWN1
 config ARM_GLOBAL_TIMER
 	bool "Support for the ARM global timer" if COMPILE_TEST
 	select TIMER_OF if OF
+	select CLKSRC_VDSO_MAPPED
 	depends on ARM
 	help
 	  This options enables support for the ARM global timer unit
@@ -612,7 +619,7 @@ config H8300_TPU
 config CLKSRC_IMX_GPT
 	bool "Clocksource using i.MX GPT" if COMPILE_TEST
 	depends on (ARM || ARM64) && CLKDEV_LOOKUP
-	select CLKSRC_MMIO
+	select CLKSRC_VDSO_MAPPED
 
 config CLKSRC_IMX_TPM
 	bool "Clocksource using i.MX TPM" if COMPILE_TEST
@@ -633,7 +640,7 @@ config CLKSRC_ST_LPC
 	bool "Low power clocksource found in the LPC" if COMPILE_TEST
 	select TIMER_OF if OF
 	depends on HAS_IOMEM
-	select CLKSRC_MMIO
+	select CLKSRC_VDSO_MAPPED
 	help
 	  Enable this option to use the Low Power controller timer
 	  as clocksource.
diff --git a/drivers/clocksource/arm_arch_timer.c b/drivers/clocksource/arm_arch_timer.c
index 9a5464c625b4..756fbd2c4294 100644
--- a/drivers/clocksource/arm_arch_timer.c
+++ b/drivers/clocksource/arm_arch_timer.c
@@ -21,6 +21,7 @@
 #include <linux/of_address.h>
 #include <linux/io.h>
 #include <linux/slab.h>
+#include <linux/dovetail.h>
 #include <linux/sched/clock.h>
 #include <linux/sched_clock.h>
 #include <linux/acpi.h>
@@ -629,7 +630,7 @@ static __always_inline irqreturn_t timer_handler(const int access,
 	if (ctrl & ARCH_TIMER_CTRL_IT_STAT) {
 		ctrl |= ARCH_TIMER_CTRL_IT_MASK;
 		arch_timer_reg_write(access, ARCH_TIMER_REG_CTRL, ctrl, evt);
-		evt->event_handler(evt);
+		clockevents_handle_event(evt);
 		return IRQ_HANDLED;
 	}
 
@@ -738,7 +739,7 @@ static int arch_timer_set_next_event_phys_mem(unsigned long evt,
 static void __arch_timer_setup(unsigned type,
 			       struct clock_event_device *clk)
 {
-	clk->features = CLOCK_EVT_FEAT_ONESHOT;
+	clk->features = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_PIPELINE;
 
 	if (type == ARCH_TIMER_TYPE_CP15) {
 		typeof(clk->set_next_event) sne;
@@ -840,6 +841,9 @@ static void arch_counter_set_user_access(void)
 	else
 		cntkctl |= ARCH_TIMER_USR_VCT_ACCESS_EN;
 
+	if (dovetailing())
+		cntkctl |= ARCH_TIMER_USR_PT_ACCESS_EN;
+
 	arch_timer_set_cntkctl(cntkctl);
 }
 
@@ -873,6 +877,7 @@ static int arch_timer_starting_cpu(unsigned int cpu)
 	enable_percpu_irq(arch_timer_ppi[arch_timer_uses_ppi], flags);
 
 	if (arch_timer_has_nonsecure_ppi()) {
+		clk->irq = arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI];
 		flags = check_ppi_trigger(arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI]);
 		enable_percpu_irq(arch_timer_ppi[ARCH_TIMER_PHYS_NONSECURE_PPI],
 				  flags);
@@ -957,6 +962,10 @@ struct arch_timer_kvm_info *arch_timer_get_kvm_info(void)
 	return &arch_timer_kvm_info;
 }
 
+void __weak arch_clocksource_arch_timer_init(struct clocksource *cs)
+{
+}
+
 static void __init arch_counter_register(unsigned type)
 {
 	u64 start_count;
@@ -980,6 +989,8 @@ static void __init arch_counter_register(unsigned type)
 
 		arch_timer_read_counter = rd;
 		clocksource_counter.archdata.vdso_direct = vdso_default;
+
+		arch_clocksource_arch_timer_init(&clocksource_counter);
 	} else {
 		arch_timer_read_counter = arch_counter_get_cntvct_mem;
 	}
diff --git a/drivers/clocksource/arm_global_timer.c b/drivers/clocksource/arm_global_timer.c
index 88b2d38a7a61..3273fc6b567e 100644
--- a/drivers/clocksource/arm_global_timer.c
+++ b/drivers/clocksource/arm_global_timer.c
@@ -153,11 +153,11 @@ static irqreturn_t gt_clockevent_interrupt(int irq, void *dev_id)
 	 *	the Global Timer flag _after_ having incremented
 	 *	the Comparator register	value to a higher value.
 	 */
-	if (clockevent_state_oneshot(evt))
+	if (clockevent_is_oob(evt) || clockevent_state_oneshot(evt))
 		gt_compare_set(ULONG_MAX, 0);
 
 	writel_relaxed(GT_INT_STATUS_EVENT_FLAG, gt_base + GT_INT_STATUS);
-	evt->event_handler(evt);
+	clockevents_handle_event(evt);
 
 	return IRQ_HANDLED;
 }
@@ -168,7 +168,7 @@ static int gt_starting_cpu(unsigned int cpu)
 
 	clk->name = "arm_global_timer";
 	clk->features = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT |
-		CLOCK_EVT_FEAT_PERCPU;
+		CLOCK_EVT_FEAT_PERCPU | CLOCK_EVT_FEAT_PIPELINE;
 	clk->set_state_shutdown = gt_clockevent_shutdown;
 	clk->set_state_periodic = gt_clockevent_set_periodic;
 	clk->set_state_oneshot = gt_clockevent_shutdown;
@@ -192,11 +192,6 @@ static int gt_dying_cpu(unsigned int cpu)
 	return 0;
 }
 
-static u64 gt_clocksource_read(struct clocksource *cs)
-{
-	return gt_counter_read();
-}
-
 static void gt_resume(struct clocksource *cs)
 {
 	unsigned long ctrl;
@@ -207,13 +202,15 @@ static void gt_resume(struct clocksource *cs)
 		writel(GT_CONTROL_TIMER_ENABLE, gt_base + GT_CONTROL);
 }
 
-static struct clocksource gt_clocksource = {
-	.name	= "arm_global_timer",
-	.rating	= 300,
-	.read	= gt_clocksource_read,
-	.mask	= CLOCKSOURCE_MASK(64),
-	.flags	= CLOCK_SOURCE_IS_CONTINUOUS,
-	.resume = gt_resume,
+static struct clocksource_user_mmio gt_clocksource = {
+	.mmio.clksrc = {
+		.name	= "arm_global_timer",
+		.rating	= 300,
+		.read	= clocksource_dual_mmio_readl_up,
+		.mask	= CLOCKSOURCE_MASK(64),
+		.flags	= CLOCK_SOURCE_IS_CONTINUOUS,
+		.resume = gt_resume,
+	},
 };
 
 #ifdef CONFIG_CLKSRC_ARM_GLOBAL_TIMER_SCHED_CLOCK
@@ -240,6 +237,8 @@ static void __init gt_delay_timer_init(void)
 
 static int __init gt_clocksource_init(void)
 {
+	struct clocksource_mmio_regs mmr;
+
 	writel(0, gt_base + GT_CONTROL);
 	writel(0, gt_base + GT_COUNTER0);
 	writel(0, gt_base + GT_COUNTER1);
@@ -249,7 +248,13 @@ static int __init gt_clocksource_init(void)
 #ifdef CONFIG_CLKSRC_ARM_GLOBAL_TIMER_SCHED_CLOCK
 	sched_clock_register(gt_sched_clock_read, 64, gt_clk_rate);
 #endif
-	return clocksource_register_hz(&gt_clocksource, gt_clk_rate);
+	mmr.reg_upper = gt_base + GT_COUNTER1;
+	mmr.reg_lower = gt_base + GT_COUNTER0;
+	mmr.bits_upper = 32;
+	mmr.bits_lower = 32;
+	mmr.revmap = NULL;
+
+	return clocksource_user_mmio_init(&gt_clocksource, &mmr, gt_clk_rate);
 }
 
 static int __init global_timer_of_register(struct device_node *np)
@@ -299,8 +304,8 @@ static int __init global_timer_of_register(struct device_node *np)
 		goto out_clk;
 	}
 
-	err = request_percpu_irq(gt_ppi, gt_clockevent_interrupt,
-				 "gt", gt_evt);
+	err = __request_percpu_irq(gt_ppi, gt_clockevent_interrupt,
+				   IRQF_TIMER, "gt", gt_evt);
 	if (err) {
 		pr_warn("global-timer: can't register interrupt %d (%d)\n",
 			gt_ppi, err);
diff --git a/drivers/clocksource/bcm2835_timer.c b/drivers/clocksource/bcm2835_timer.c
index 2b196cbfadb6..6387f2ce98ea 100644
--- a/drivers/clocksource/bcm2835_timer.c
+++ b/drivers/clocksource/bcm2835_timer.c
@@ -54,13 +54,11 @@ static int bcm2835_time_set_next_event(unsigned long event,
 static irqreturn_t bcm2835_time_interrupt(int irq, void *dev_id)
 {
 	struct bcm2835_timer *timer = dev_id;
-	void (*event_handler)(struct clock_event_device *);
+
 	if (readl_relaxed(timer->control) & timer->match_mask) {
 		writel_relaxed(timer->match_mask, timer->control);
 
-		event_handler = READ_ONCE(timer->evt.event_handler);
-		if (event_handler)
-			event_handler(&timer->evt);
+		clockevents_handle_event(&timer->evt);
 		return IRQ_HANDLED;
 	} else {
 		return IRQ_NONE;
@@ -110,9 +108,10 @@ static int __init bcm2835_timer_init(struct device_node *node)
 	timer->match_mask = BIT(DEFAULT_TIMER);
 	timer->evt.name = node->name;
 	timer->evt.rating = 300;
-	timer->evt.features = CLOCK_EVT_FEAT_ONESHOT;
+	timer->evt.features = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_PIPELINE;
 	timer->evt.set_next_event = bcm2835_time_set_next_event;
 	timer->evt.cpumask = cpumask_of(0);
+	timer->evt.irq = irq;
 	timer->act.name = node->name;
 	timer->act.flags = IRQF_TIMER | IRQF_SHARED;
 	timer->act.dev_id = timer;
diff --git a/drivers/clocksource/clksrc_st_lpc.c b/drivers/clocksource/clksrc_st_lpc.c
index 419a886876e4..b30b814d271f 100644
--- a/drivers/clocksource/clksrc_st_lpc.c
+++ b/drivers/clocksource/clksrc_st_lpc.c
@@ -51,7 +51,7 @@ static int __init st_clksrc_init(void)
 
 	sched_clock_register(st_clksrc_sched_clock_read, 32, rate);
 
-	ret = clocksource_mmio_init(ddata.base + LPC_LPT_LSB_OFF,
+	ret = clocksource_user_single_mmio_init(ddata.base + LPC_LPT_LSB_OFF,
 				    "clksrc-st-lpc", rate, 300, 32,
 				    clocksource_mmio_readl_up);
 	if (ret) {
diff --git a/drivers/clocksource/dw_apb_timer.c b/drivers/clocksource/dw_apb_timer.c
index 654766538f93..8bb7c43f6112 100644
--- a/drivers/clocksource/dw_apb_timer.c
+++ b/drivers/clocksource/dw_apb_timer.c
@@ -43,7 +43,7 @@ ced_to_dw_apb_ced(struct clock_event_device *evt)
 static inline struct dw_apb_clocksource *
 clocksource_to_dw_apb_clocksource(struct clocksource *cs)
 {
-	return container_of(cs, struct dw_apb_clocksource, cs);
+	return container_of(cs, struct dw_apb_clocksource, ummio.mmio.clksrc);
 }
 
 static inline u32 apbt_readl(struct dw_apb_timer *timer, unsigned long offs)
@@ -347,18 +347,6 @@ void dw_apb_clocksource_start(struct dw_apb_clocksource *dw_cs)
 	dw_apb_clocksource_read(dw_cs);
 }
 
-static u64 __apbt_read_clocksource(struct clocksource *cs)
-{
-	u32 current_count;
-	struct dw_apb_clocksource *dw_cs =
-		clocksource_to_dw_apb_clocksource(cs);
-
-	current_count = apbt_readl_relaxed(&dw_cs->timer,
-					APBTMR_N_CURRENT_VALUE);
-
-	return (u64)~current_count;
-}
-
 static void apbt_restart_clocksource(struct clocksource *cs)
 {
 	struct dw_apb_clocksource *dw_cs =
@@ -380,7 +368,7 @@ static void apbt_restart_clocksource(struct clocksource *cs)
  * dw_apb_clocksource_register() as the next step.
  */
 struct dw_apb_clocksource *
-dw_apb_clocksource_init(unsigned rating, const char *name, void __iomem *base,
+__init dw_apb_clocksource_init(unsigned rating, const char *name, void __iomem *base,
 			unsigned long freq)
 {
 	struct dw_apb_clocksource *dw_cs = kzalloc(sizeof(*dw_cs), GFP_KERNEL);
@@ -390,12 +378,12 @@ dw_apb_clocksource_init(unsigned rating, const char *name, void __iomem *base,
 
 	dw_cs->timer.base = base;
 	dw_cs->timer.freq = freq;
-	dw_cs->cs.name = name;
-	dw_cs->cs.rating = rating;
-	dw_cs->cs.read = __apbt_read_clocksource;
-	dw_cs->cs.mask = CLOCKSOURCE_MASK(32);
-	dw_cs->cs.flags = CLOCK_SOURCE_IS_CONTINUOUS;
-	dw_cs->cs.resume = apbt_restart_clocksource;
+	dw_cs->ummio.mmio.clksrc.name = name;
+	dw_cs->ummio.mmio.clksrc.rating = rating;
+	dw_cs->ummio.mmio.clksrc.read = clocksource_mmio_readl_down;
+	dw_cs->ummio.mmio.clksrc.mask = CLOCKSOURCE_MASK(32);
+	dw_cs->ummio.mmio.clksrc.flags = CLOCK_SOURCE_IS_CONTINUOUS;
+	dw_cs->ummio.mmio.clksrc.resume = apbt_restart_clocksource;
 
 	return dw_cs;
 }
@@ -405,9 +393,17 @@ dw_apb_clocksource_init(unsigned rating, const char *name, void __iomem *base,
  *
  * @dw_cs:	The clocksource to register.
  */
-void dw_apb_clocksource_register(struct dw_apb_clocksource *dw_cs)
+void __init dw_apb_clocksource_register(struct dw_apb_clocksource *dw_cs)
 {
-	clocksource_register_hz(&dw_cs->cs, dw_cs->timer.freq);
+	struct clocksource_mmio_regs mmr;
+
+	mmr.reg_lower = dw_cs->timer.base + APBTMR_N_CURRENT_VALUE;
+	mmr.bits_lower = 32;
+	mmr.reg_upper = 0;
+	mmr.bits_upper = 0;
+	mmr.revmap = NULL;
+
+	clocksource_user_mmio_init(&dw_cs->ummio, &mmr, dw_cs->timer.freq);
 }
 
 /**
diff --git a/drivers/clocksource/mmio.c b/drivers/clocksource/mmio.c
index 9de751531831..188cde1bbb46 100644
--- a/drivers/clocksource/mmio.c
+++ b/drivers/clocksource/mmio.c
@@ -6,12 +6,31 @@
 #include <linux/errno.h>
 #include <linux/init.h>
 #include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/uaccess.h>
+#include <linux/miscdevice.h>
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/mman.h>
+#include <linux/device.h>
 
-struct clocksource_mmio {
-	void __iomem *reg;
-	struct clocksource clksrc;
+struct clocksource_user_mapping {
+	struct mm_struct *mm;
+	struct clocksource_user_mmio *ucs;
+	void *regs;
+	struct hlist_node link;
+	atomic_t refs;
 };
 
+static struct class *user_mmio_class;
+static dev_t user_mmio_devt;
+
+static DEFINE_SPINLOCK(user_clksrcs_lock);
+static unsigned int user_clksrcs_count;
+static LIST_HEAD(user_clksrcs);
+
 static inline struct clocksource_mmio *to_mmio_clksrc(struct clocksource *c)
 {
 	return container_of(c, struct clocksource_mmio, clksrc);
@@ -37,6 +56,53 @@ u64 clocksource_mmio_readw_down(struct clocksource *c)
 	return ~(u64)readw_relaxed(to_mmio_clksrc(c)->reg) & c->mask;
 }
 
+static inline struct clocksource_user_mmio *
+to_mmio_ucs(struct clocksource *c)
+{
+	return container_of(c, struct clocksource_user_mmio, mmio.clksrc);
+}
+
+u64 clocksource_dual_mmio_readl_up(struct clocksource *c)
+{
+	struct clocksource_user_mmio *ucs = to_mmio_ucs(c);
+	u32 upper, old_upper, lower;
+
+	upper = readl_relaxed(ucs->reg_upper);
+	do {
+		old_upper = upper;
+		lower = readl_relaxed(ucs->mmio.reg);
+		upper = readl_relaxed(ucs->reg_upper);
+	} while (upper != old_upper);
+
+	return (((u64)upper) << ucs->bits_lower) | lower;
+}
+
+u64 clocksource_dual_mmio_readw_up(struct clocksource *c)
+{
+	struct clocksource_user_mmio *ucs = to_mmio_ucs(c);
+	u16 upper, old_upper, lower;
+
+	upper = readw_relaxed(ucs->reg_upper);
+	do {
+		old_upper = upper;
+		lower = readw_relaxed(ucs->mmio.reg);
+		upper = readw_relaxed(ucs->reg_upper);
+	} while (upper != old_upper);
+
+	return (((u64)upper) << ucs->bits_lower) | lower;
+}
+
+static void mmio_base_init(const char *name,int rating, unsigned int bits,
+			   u64 (*read)(struct clocksource *),
+			   struct clocksource *cs)
+{
+	cs->name = name;
+	cs->rating = rating;
+	cs->read = read;
+	cs->mask = CLOCKSOURCE_MASK(bits);
+	cs->flags = CLOCK_SOURCE_IS_CONTINUOUS;
+}
+
 /**
  * clocksource_mmio_init - Initialize a simple mmio based clocksource
  * @base:	Virtual address of the clock readout register
@@ -51,6 +117,7 @@ int __init clocksource_mmio_init(void __iomem *base, const char *name,
 	u64 (*read)(struct clocksource *))
 {
 	struct clocksource_mmio *cs;
+	int err;
 
 	if (bits > 64 || bits < 16)
 		return -EINVAL;
@@ -60,11 +127,432 @@ int __init clocksource_mmio_init(void __iomem *base, const char *name,
 		return -ENOMEM;
 
 	cs->reg = base;
-	cs->clksrc.name = name;
-	cs->clksrc.rating = rating;
-	cs->clksrc.read = read;
-	cs->clksrc.mask = CLOCKSOURCE_MASK(bits);
-	cs->clksrc.flags = CLOCK_SOURCE_IS_CONTINUOUS;
+	mmio_base_init(name, rating, bits, read, &cs->clksrc);
+
+	err = clocksource_register_hz(&cs->clksrc, hz);
+	if (err < 0) {
+		kfree(cs);
+		return err;
+	}
+
+	return err;
+}
+
+static void clksrc_vmopen(struct vm_area_struct *vma)
+{
+	struct clocksource_user_mapping *mapping, *clone;
+	struct clocksource_user_mmio *ucs;
+	unsigned long h_key;
+
+	mapping = vma->vm_private_data;
+
+	if (mapping->mm == vma->vm_mm) {
+		atomic_inc(&mapping->refs);
+	} else if (mapping->mm) {
+		/*
+		 * We must be duplicating the original mm upon fork(),
+		 * clone the parent ucs mapping struct then rehash it
+		 * on the child mm key. If we cannot get memory for
+		 * this, mitigate the issue for users by preventing a
+		 * stale parent mm from being matched later on by a
+		 * process which reused its mm_struct (h_key is based
+		 * on this struct address).
+		 */
+		clone = kmalloc(sizeof(*mapping), GFP_KERNEL);
+		if (clone == NULL) {
+			pr_alert("out-of-memory for UCS mapping!\n");
+			atomic_inc(&mapping->refs);
+			mapping->mm = NULL;
+			return;
+		}
+		ucs = mapping->ucs;
+		clone->mm = vma->vm_mm;
+		clone->ucs = ucs;
+		clone->regs = mapping->regs;
+		atomic_set(&clone->refs, 1);
+		vma->vm_private_data = clone;
+		h_key = (unsigned long)vma->vm_mm / sizeof(*vma->vm_mm);
+		spin_lock(&ucs->lock);
+		hash_add(ucs->mappings, &clone->link, h_key);
+		spin_unlock(&ucs->lock);
+	}
+}
+
+static void clksrc_vmclose(struct vm_area_struct *vma)
+{
+	struct clocksource_user_mapping *mapping;
+
+	mapping = vma->vm_private_data;
+
+	if (atomic_dec_and_test(&mapping->refs)) {
+		spin_lock(&mapping->ucs->lock);
+		hash_del(&mapping->link);
+		spin_unlock(&mapping->ucs->lock);
+		kfree(mapping);
+	}
+}
+
+static const struct vm_operations_struct clksrc_vmops = {
+	.open = clksrc_vmopen,
+	.close = clksrc_vmclose,
+};
+
+static int mmio_ucs_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	unsigned long addr, upper_pfn, lower_pfn;
+	struct clocksource_user_mapping *mapping, *tmp;
+	struct clocksource_user_mmio *ucs;
+	unsigned int bits_upper;
+	unsigned long h_key;
+	pgprot_t prot;
+	size_t pages;
+	int err;
+
+	pages = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
+	if (pages > 2)
+		return -EINVAL;
+
+	vma->vm_private_data = NULL;
+
+	ucs = file->private_data;
+	upper_pfn = ucs->phys_upper >> PAGE_SHIFT;
+	lower_pfn = ucs->phys_lower >> PAGE_SHIFT;
+	bits_upper = fls(ucs->mmio.clksrc.mask) - ucs->bits_lower;
+	if (pages == 2 && (!bits_upper || upper_pfn == lower_pfn))
+		return -EINVAL;
+
+	mapping = kmalloc(sizeof(*mapping), GFP_KERNEL);
+	if (!mapping)
+		return -ENOSPC;
+
+	mapping->mm = vma->vm_mm;
+	mapping->ucs = ucs;
+	mapping->regs = (void *)vma->vm_start;
+	atomic_set(&mapping->refs, 1);
+
+	vma->vm_private_data = mapping;
+	vma->vm_ops = &clksrc_vmops;
+	prot = pgprot_noncached(vma->vm_page_prot);
+	addr = vma->vm_start;
+
+	err = remap_pfn_range(vma, addr, lower_pfn, PAGE_SIZE, prot);
+	if (err < 0)
+		goto fail;
+
+	if (pages > 1) {
+		addr += PAGE_SIZE;
+		err = remap_pfn_range(vma, addr, upper_pfn, PAGE_SIZE, prot);
+		if (err < 0)
+			goto fail;
+	}
+
+	h_key = (unsigned long)vma->vm_mm / sizeof(*vma->vm_mm);
+
+	spin_lock(&ucs->lock);
+	hash_for_each_possible(ucs->mappings, tmp, link, h_key) {
+		if (tmp->mm == vma->vm_mm) {
+			spin_unlock(&ucs->lock);
+			err = -EBUSY;
+			goto fail;
+		}
+	}
+	hash_add(ucs->mappings, &mapping->link, h_key);
+	spin_unlock(&ucs->lock);
+
+	return 0;
+fail:
+	kfree(mapping);
+
+	return err;
+}
+
+static long
+mmio_ucs_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct clocksource_user_mapping *mapping;
+	struct clksrc_user_mmio_info __user *u;
+	struct clksrc_user_mmio_info info;
+	struct clocksource_user_mmio *ucs;
+	unsigned long upper_pfn, lower_pfn;
+	unsigned int bits_upper;
+	void __user *map_base;
+	unsigned long h_key;
+	size_t size;
+
+	u = (struct clksrc_user_mmio_info __user *)arg;
+
+	switch (cmd) {
+	case CLKSRC_USER_MMIO_MAP:
+		break;
+	default:
+		return -ENOTTY;
+	}
+
+	h_key = (unsigned long)current->mm / sizeof(*current->mm);
+
+	ucs = file->private_data;
+	upper_pfn = ucs->phys_upper >> PAGE_SHIFT;
+	lower_pfn = ucs->phys_lower >> PAGE_SHIFT;
+	bits_upper = fls(ucs->mmio.clksrc.mask) - ucs->bits_lower;
+	size = PAGE_SIZE;
+	if (bits_upper && upper_pfn != lower_pfn)
+		size += PAGE_SIZE;
+
+	do {
+		spin_lock(&ucs->lock);
+		hash_for_each_possible(ucs->mappings, mapping, link, h_key) {
+			if (mapping->mm == current->mm) {
+				spin_unlock(&ucs->lock);
+				map_base = mapping->regs;
+				goto found;
+			}
+		}
+		spin_unlock(&ucs->lock);
+
+		map_base = (void *)
+			vm_mmap(file, 0, size, PROT_READ, MAP_SHARED, 0);
+	} while (IS_ERR(map_base) && PTR_ERR(map_base) == -EBUSY);
+
+	if (IS_ERR(map_base))
+		return PTR_ERR(map_base);
+
+found:
+	info.type = ucs->type;
+	info.reg_lower = map_base + offset_in_page(ucs->phys_lower);
+	info.mask_lower = ucs->mmio.clksrc.mask;
+	info.bits_lower = ucs->bits_lower;
+	info.reg_upper = NULL;
+	if (ucs->phys_upper)
+		info.reg_upper = map_base + (size - PAGE_SIZE)
+			+ offset_in_page(ucs->phys_upper);
+	info.mask_upper = ucs->mask_upper;
+
+	return copy_to_user(u, &info, sizeof(*u));
+}
+
+static int mmio_ucs_open(struct inode *inode, struct file *file)
+{
+	struct clocksource_user_mmio *ucs;
+
+	if (file->f_mode & FMODE_WRITE)
+		return -EINVAL;
+
+	ucs = container_of(inode->i_cdev, typeof(*ucs), cdev);
+	file->private_data = ucs;
+
+	return 0;
+}
+
+static const struct file_operations mmio_ucs_fops = {
+	.owner		= THIS_MODULE,
+	.unlocked_ioctl = mmio_ucs_ioctl,
+	.open		= mmio_ucs_open,
+	.mmap		= mmio_ucs_mmap,
+
+};
+
+static int __init
+ucs_create_cdev(struct class *class, struct clocksource_user_mmio *ucs)
+{
+	int err;
+
+	ucs->dev = device_create(class, NULL,
+				MKDEV(MAJOR(user_mmio_devt), ucs->id),
+				ucs, "ucs/%d", ucs->id);
+	if (IS_ERR(ucs->dev))
+		return PTR_ERR(ucs->dev);
+
+	spin_lock_init(&ucs->lock);
+	hash_init(ucs->mappings);
+
+	cdev_init(&ucs->cdev, &mmio_ucs_fops);
+	ucs->cdev.kobj.parent = &ucs->dev->kobj;
+
+	err = cdev_add(&ucs->cdev, ucs->dev->devt, 1);
+	if (err < 0)
+		goto err_device_destroy;
+
+	return 0;
+
+err_device_destroy:
+	device_destroy(class, MKDEV(MAJOR(user_mmio_devt), ucs->id));
+	return err;
+}
+
+static unsigned long default_revmap(void *virt)
+{
+	struct vm_struct *vm;
+
+	vm = find_vm_area(virt);
+	if (!vm)
+		return 0;
+
+	return vm->phys_addr + (virt - vm->addr);
+}
+
+void __weak
+arch_clocksource_user_mmio_init(struct clocksource *cs, unsigned id) { }
+
+int __init clocksource_user_mmio_init(struct clocksource_user_mmio *ucs,
+				      const struct clocksource_mmio_regs *regs,
+				      unsigned long hz)
+{
+	static u64 (*user_types[CLKSRC_MMIO_TYPE_NR])(struct clocksource *) = {
+		[CLKSRC_MMIO_L_UP] = clocksource_mmio_readl_up,
+		[CLKSRC_MMIO_L_DOWN] = clocksource_mmio_readl_down,
+		[CLKSRC_DMMIO_L_UP] = clocksource_dual_mmio_readl_up,
+		[CLKSRC_MMIO_W_UP] = clocksource_mmio_readw_up,
+		[CLKSRC_MMIO_W_DOWN] = clocksource_mmio_readw_down,
+		[CLKSRC_DMMIO_W_UP] = clocksource_dual_mmio_readw_up,
+	};
+	const char *name = ucs->mmio.clksrc.name;
+	unsigned long phys_upper = 0, phys_lower;
+	enum clksrc_user_mmio_type type;
+	unsigned long (*revmap)(void *);
+	int err;
+
+	if (regs->bits_lower > 32 || regs->bits_lower < 16 ||
+	    regs->bits_upper > 32)
+		return -EINVAL;
+
+	for (type = 0; type < ARRAY_SIZE(user_types); type++)
+		if (ucs->mmio.clksrc.read == user_types[type])
+			break;
+
+	if (type == ARRAY_SIZE(user_types))
+		return -EINVAL;
+
+	if (!(ucs->mmio.clksrc.flags & CLOCK_SOURCE_IS_CONTINUOUS))
+		return -EINVAL;
+
+	revmap = regs->revmap;
+	if (!revmap)
+		revmap = default_revmap;
+
+	phys_lower = revmap(regs->reg_lower);
+	if (!phys_lower)
+		return -EINVAL;
+
+	if (regs->bits_upper) {
+		phys_upper = revmap(regs->reg_upper);
+		if (!phys_upper)
+			return -EINVAL;
+	}
+
+	ucs->mmio.reg = regs->reg_lower;
+	ucs->type = type;
+	ucs->bits_lower = regs->bits_lower;
+	ucs->reg_upper = regs->reg_upper;
+	ucs->mask_lower = CLOCKSOURCE_MASK(regs->bits_lower);
+	ucs->mask_upper = CLOCKSOURCE_MASK(regs->bits_upper);
+	ucs->phys_lower = phys_lower;
+	ucs->phys_upper = phys_upper;
+	spin_lock_init(&ucs->lock);
+
+	err = clocksource_register_hz(&ucs->mmio.clksrc, hz);
+	if (err < 0)
+		return err;
+
+	spin_lock(&user_clksrcs_lock);
+
+	ucs->id = user_clksrcs_count++;
+	if (ucs->id < CLKSRC_USER_MMIO_MAX)
+		list_add_tail(&ucs->link, &user_clksrcs);
+
+	spin_unlock(&user_clksrcs_lock);
+
+	if (ucs->id >= CLKSRC_USER_MMIO_MAX) {
+		pr_warn("%s: Too many clocksources\n", name);
+		err = -EAGAIN;
+		goto fail;
+	}
+
+	arch_clocksource_user_mmio_init(&ucs->mmio.clksrc, ucs->id);
+
+	if (user_mmio_class) {
+		err = ucs_create_cdev(user_mmio_class, ucs);
+		if (err < 0) {
+			pr_warn("%s: Failed to add character device\n", name);
+			goto fail;
+		}
+	}
+
+	return 0;
+
+fail:
+	clocksource_unregister(&ucs->mmio.clksrc);
+
+	return err;
+}
+
+int __init clocksource_user_single_mmio_init(
+	void __iomem *base, const char *name,
+	unsigned long hz, int rating, unsigned int bits,
+	u64 (*read)(struct clocksource *))
+{
+	struct clocksource_user_mmio *ucs;
+	struct clocksource_mmio_regs regs;
+	int ret;
+
+	ucs = kzalloc(sizeof(*ucs), GFP_KERNEL);
+	if (!ucs)
+		return -ENOMEM;
+
+	mmio_base_init(name, rating, bits, read, &ucs->mmio.clksrc);
+	regs.reg_lower = base;
+	regs.reg_upper = NULL;
+	regs.bits_lower = bits;
+	regs.bits_upper = 0;
+	regs.revmap = NULL;
+
+	ret = clocksource_user_mmio_init(ucs, &regs, hz);
+	if (ret)
+		kfree(ucs);
+
+	return ret;
+}
+
+static int __init mmio_clksrc_chr_dev_init(void)
+{
+	struct clocksource_user_mmio *ucs;
+	struct class *class;
+	int err;
+
+	class = class_create(THIS_MODULE, "mmio_ucs");
+	if (IS_ERR(class)) {
+		pr_err("couldn't create user mmio clocksources class\n");
+		return PTR_ERR(class);
+	}
+
+	err = alloc_chrdev_region(&user_mmio_devt, 0, CLKSRC_USER_MMIO_MAX,
+				  "mmio_ucs");
+	if (err < 0) {
+		pr_err("failed to allocate user mmio clocksources character devivces region\n");
+		goto err_class_destroy;
+	}
+
+	/*
+	 * Calling list_for_each_entry is safe here: clocksources are always
+	 * added to the list tail, never removed.
+	 */
+	spin_lock(&user_clksrcs_lock);
+	list_for_each_entry(ucs, &user_clksrcs, link) {
+		spin_unlock(&user_clksrcs_lock);
+
+		err = ucs_create_cdev(class, ucs);
+		if (err < 0)
+			pr_err("%s: Failed to add character device\n",
+			       ucs->mmio.clksrc.name);
+
+		spin_lock(&user_clksrcs_lock);
+	}
+	user_mmio_class = class;
+	spin_unlock(&user_clksrcs_lock);
+
+	return 0;
 
-	return clocksource_register_hz(&cs->clksrc, hz);
+err_class_destroy:
+	class_destroy(class);
+	return err;
 }
+device_initcall(mmio_clksrc_chr_dev_init);
diff --git a/drivers/clocksource/timer-imx-gpt.c b/drivers/clocksource/timer-imx-gpt.c
index 706c0d0ff56c..dfb789f58f22 100644
--- a/drivers/clocksource/timer-imx-gpt.c
+++ b/drivers/clocksource/timer-imx-gpt.c
@@ -164,8 +164,8 @@ static int __init mxc_clocksource_init(struct imx_timer *imxtm)
 	sched_clock_reg = reg;
 
 	sched_clock_register(mxc_read_sched_clock, 32, c);
-	return clocksource_mmio_init(reg, "mxc_timer1", c, 200, 32,
-			clocksource_mmio_readl_up);
+	return clocksource_user_single_mmio_init(reg, "mxc_timer1", c, 200, 32,
+					 clocksource_mmio_readl_up);
 }
 
 /* clock event */
@@ -265,7 +265,7 @@ static irqreturn_t mxc_timer_interrupt(int irq, void *dev_id)
 
 	imxtm->gpt->gpt_irq_acknowledge(imxtm);
 
-	ced->event_handler(ced);
+	clockevents_handle_event(ced);
 
 	return IRQ_HANDLED;
 }
@@ -276,7 +276,7 @@ static int __init mxc_clockevent_init(struct imx_timer *imxtm)
 	struct irqaction *act = &imxtm->act;
 
 	ced->name = "mxc_timer1";
-	ced->features = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_DYNIRQ;
+	ced->features = CLOCK_EVT_FEAT_ONESHOT | CLOCK_EVT_FEAT_DYNIRQ | CLOCK_EVT_FEAT_PIPELINE;
 	ced->set_state_shutdown = mxc_shutdown;
 	ced->set_state_oneshot = mxc_set_oneshot;
 	ced->tick_resume = mxc_shutdown;
diff --git a/drivers/cpuidle/cpuidle.c b/drivers/cpuidle/cpuidle.c
index 0895b988fa92..90e198b81f38 100644
--- a/drivers/cpuidle/cpuidle.c
+++ b/drivers/cpuidle/cpuidle.c
@@ -17,6 +17,7 @@
 #include <linux/pm_qos.h>
 #include <linux/cpu.h>
 #include <linux/cpuidle.h>
+#include <linux/irq_pipeline.h>
 #include <linux/ktime.h>
 #include <linux/hrtimer.h>
 #include <linux/module.h>
@@ -203,6 +204,21 @@ int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,
 	bool broadcast = !!(target_state->flags & CPUIDLE_FLAG_TIMER_STOP);
 	ktime_t time_start, time_end;
 
+	/*
+	 * A co-kernel running on the oob stage of the IRQ pipeline
+	 * may deny switching to a deeper C-state. If so, call the
+	 * default idle routine instead. If the co-kernel cannot bear
+	 * with the latency induced by the default idling operation,
+	 * then CPUIDLE is not usable and should be disabled at build
+	 * time. The inband stage is stalled on entry,
+	 * irq_cpuidle_enter() additionally returns with hard irqs
+	 * off.
+	 */
+	if (!irq_cpuidle_enter(dev, target_state)) {
+		default_idle_call();
+		return -EBUSY;
+	}
+
 	/*
 	 * Tell the time framework to switch to a broadcast timer because our
 	 * local timer will be shut down.  If a local timer is used from another
@@ -227,6 +243,7 @@ int cpuidle_enter_state(struct cpuidle_device *dev, struct cpuidle_driver *drv,
 
 	stop_critical_timings();
 	entered_state = target_state->enter(dev, drv, index);
+	hard_cond_local_irq_enable();
 	start_critical_timings();
 
 	sched_clock_idle_wakeup_event();
diff --git a/drivers/cpuidle/poll_state.c b/drivers/cpuidle/poll_state.c
index c8fa5f41dfc4..71b6d9d3ca2e 100644
--- a/drivers/cpuidle/poll_state.c
+++ b/drivers/cpuidle/poll_state.c
@@ -17,7 +17,7 @@ static int __cpuidle poll_idle(struct cpuidle_device *dev,
 
 	dev->poll_time_limit = false;
 
-	local_irq_enable();
+	local_irq_enable_full();
 	if (!current_set_polling_and_test()) {
 		unsigned int loop_count = 0;
 		u64 limit;
diff --git a/drivers/gpio/gpio-mxc.c b/drivers/gpio/gpio-mxc.c
index 7907a8755866..323da64b4153 100644
--- a/drivers/gpio/gpio-mxc.c
+++ b/drivers/gpio/gpio-mxc.c
@@ -359,7 +359,7 @@ static int mxc_gpio_init_gc(struct mxc_gpio_port *port, int irq_base)
 	ct->chip.irq_unmask = irq_gc_mask_set_bit;
 	ct->chip.irq_set_type = gpio_set_irq_type;
 	ct->chip.irq_set_wake = gpio_set_wake_irq;
-	ct->chip.flags = IRQCHIP_MASK_ON_SUSPEND;
+	ct->chip.flags = IRQCHIP_MASK_ON_SUSPEND | IRQCHIP_PIPELINE_SAFE;
 	ct->regs.ack = GPIO_ISR;
 	ct->regs.mask = GPIO_IMR;
 
diff --git a/drivers/gpio/gpio-omap.c b/drivers/gpio/gpio-omap.c
index d0f27084a942..d9859ce4db50 100644
--- a/drivers/gpio/gpio-omap.c
+++ b/drivers/gpio/gpio-omap.c
@@ -54,7 +54,7 @@ struct gpio_bank {
 	u32 saved_datain;
 	u32 level_mask;
 	u32 toggle_mask;
-	raw_spinlock_t lock;
+	hard_spinlock_t lock;
 	raw_spinlock_t wa_lock;
 	struct gpio_chip chip;
 	struct clk *dbck;
@@ -1374,7 +1374,7 @@ static int omap_gpio_probe(struct platform_device *pdev)
 	irqc->irq_bus_lock = omap_gpio_irq_bus_lock,
 	irqc->irq_bus_sync_unlock = gpio_irq_bus_sync_unlock,
 	irqc->name = dev_name(&pdev->dev);
-	irqc->flags = IRQCHIP_MASK_ON_SUSPEND;
+	irqc->flags = IRQCHIP_MASK_ON_SUSPEND | IRQCHIP_PIPELINE_SAFE;
 	irqc->parent_device = dev;
 
 	bank->irq = platform_get_irq(pdev, 0);
diff --git a/drivers/gpio/gpio-pl061.c b/drivers/gpio/gpio-pl061.c
index 722ce5cf861e..72914aba9f78 100644
--- a/drivers/gpio/gpio-pl061.c
+++ b/drivers/gpio/gpio-pl061.c
@@ -47,7 +47,7 @@ struct pl061_context_save_regs {
 #endif
 
 struct pl061 {
-	raw_spinlock_t		lock;
+	hard_spinlock_t		lock;
 
 	void __iomem		*base;
 	struct gpio_chip	gc;
@@ -320,6 +320,7 @@ static int pl061_probe(struct amba_device *adev, const struct amba_id *id)
 	pl061->irq_chip.irq_unmask = pl061_irq_unmask;
 	pl061->irq_chip.irq_set_type = pl061_irq_type;
 	pl061->irq_chip.irq_set_wake = pl061_irq_set_wake;
+	pl061->irq_chip.flags = IRQCHIP_PIPELINE_SAFE;
 
 	writeb(0, pl061->base + GPIOIE); /* disable irqs */
 	irq = adev->irq[0];
diff --git a/drivers/gpio/gpio-zynq.c b/drivers/gpio/gpio-zynq.c
index cd475ff4bcad..7c2d47be5bf9 100644
--- a/drivers/gpio/gpio-zynq.c
+++ b/drivers/gpio/gpio-zynq.c
@@ -583,7 +583,7 @@ static struct irq_chip zynq_gpio_level_irqchip = {
 	.irq_request_resources = zynq_gpio_irq_reqres,
 	.irq_release_resources = zynq_gpio_irq_relres,
 	.flags		= IRQCHIP_EOI_THREADED | IRQCHIP_EOI_IF_HANDLED |
-			  IRQCHIP_MASK_ON_SUSPEND,
+			  IRQCHIP_MASK_ON_SUSPEND | IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct irq_chip zynq_gpio_edge_irqchip = {
@@ -596,7 +596,7 @@ static struct irq_chip zynq_gpio_edge_irqchip = {
 	.irq_set_wake	= zynq_gpio_set_wake,
 	.irq_request_resources = zynq_gpio_irq_reqres,
 	.irq_release_resources = zynq_gpio_irq_relres,
-	.flags		= IRQCHIP_MASK_ON_SUSPEND,
+	.flags		= IRQCHIP_MASK_ON_SUSPEND | IRQCHIP_PIPELINE_SAFE,
 };
 
 static void zynq_gpio_handle_bank_irq(struct zynq_gpio *gpio,
diff --git a/drivers/gpu/drm/msm/disp/dpu1/dpu_mdss.c b/drivers/gpu/drm/msm/disp/dpu1/dpu_mdss.c
index 29705e773a4b..6e87dadba10a 100644
--- a/drivers/gpu/drm/msm/disp/dpu1/dpu_mdss.c
+++ b/drivers/gpu/drm/msm/disp/dpu1/dpu_mdss.c
@@ -123,6 +123,7 @@ static struct irq_chip dpu_mdss_irq_chip = {
 	.name = "dpu_mdss",
 	.irq_mask = dpu_mdss_irq_mask,
 	.irq_unmask = dpu_mdss_irq_unmask,
+	.flags = IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct lock_class_key dpu_mdss_lock_key, dpu_mdss_request_key;
diff --git a/drivers/gpu/drm/msm/disp/mdp5/mdp5_mdss.c b/drivers/gpu/drm/msm/disp/mdp5/mdp5_mdss.c
index 09bd46ad820b..8e0f2be155d8 100644
--- a/drivers/gpu/drm/msm/disp/mdp5/mdp5_mdss.c
+++ b/drivers/gpu/drm/msm/disp/mdp5/mdp5_mdss.c
@@ -91,6 +91,7 @@ static struct irq_chip mdss_hw_irq_chip = {
 	.name		= "mdss",
 	.irq_mask	= mdss_hw_mask_irq,
 	.irq_unmask	= mdss_hw_unmask_irq,
+	.flags		= IRQCHIP_PIPELINE_SAFE,
 };
 
 static int mdss_hw_irqdomain_map(struct irq_domain *d, unsigned int irq,
diff --git a/drivers/gpu/ipu-v3/ipu-common.c b/drivers/gpu/ipu-v3/ipu-common.c
index ee2a025e54cf..6bb61afcf07a 100644
--- a/drivers/gpu/ipu-v3/ipu-common.c
+++ b/drivers/gpu/ipu-v3/ipu-common.c
@@ -1303,6 +1303,7 @@ static int ipu_irq_init(struct ipu_soc *ipu)
 		ct->chip.irq_ack = irq_gc_ack_set_bit;
 		ct->chip.irq_mask = irq_gc_mask_clr_bit;
 		ct->chip.irq_unmask = irq_gc_mask_set_bit;
+		ct->chip.flags = IRQCHIP_PIPELINE_SAFE;
 		ct->regs.ack = IPU_INT_STAT(i / 32);
 		ct->regs.mask = IPU_INT_CTRL(i / 32);
 	}
diff --git a/drivers/iio/industrialio-trigger.c b/drivers/iio/industrialio-trigger.c
index 3908a9a90035..56c2ea6fd30f 100644
--- a/drivers/iio/industrialio-trigger.c
+++ b/drivers/iio/industrialio-trigger.c
@@ -543,6 +543,7 @@ static struct iio_trigger *viio_trigger_alloc(const char *fmt, va_list vargs)
 	trig->subirq_chip.name = trig->name;
 	trig->subirq_chip.irq_mask = &iio_trig_subirqmask;
 	trig->subirq_chip.irq_unmask = &iio_trig_subirqunmask;
+	trig->subirq_chip.flags = IRQCHIP_PIPELINE_SAFE;
 	for (i = 0; i < CONFIG_IIO_CONSUMERS_PER_TRIGGER; i++) {
 		irq_set_chip(trig->subirq_base + i, &trig->subirq_chip);
 		irq_set_handler(trig->subirq_base + i, &handle_simple_irq);
diff --git a/drivers/irqchip/irq-bcm2835.c b/drivers/irqchip/irq-bcm2835.c
index 418245d31921..c409720fdd21 100644
--- a/drivers/irqchip/irq-bcm2835.c
+++ b/drivers/irqchip/irq-bcm2835.c
@@ -101,7 +101,8 @@ static void armctrl_unmask_irq(struct irq_data *d)
 static struct irq_chip armctrl_chip = {
 	.name = "ARMCTRL-level",
 	.irq_mask = armctrl_mask_irq,
-	.irq_unmask = armctrl_unmask_irq
+	.irq_unmask = armctrl_unmask_irq,
+	.flags = IRQCHIP_PIPELINE_SAFE,
 };
 
 static int armctrl_xlate(struct irq_domain *d, struct device_node *ctrlr,
diff --git a/drivers/irqchip/irq-bcm2836.c b/drivers/irqchip/irq-bcm2836.c
index 2038693f074c..49e06a37bc9a 100644
--- a/drivers/irqchip/irq-bcm2836.c
+++ b/drivers/irqchip/irq-bcm2836.c
@@ -57,6 +57,7 @@ static struct irq_chip bcm2836_arm_irqchip_timer = {
 	.name		= "bcm2836-timer",
 	.irq_mask	= bcm2836_arm_irqchip_mask_timer_irq,
 	.irq_unmask	= bcm2836_arm_irqchip_unmask_timer_irq,
+	.flags		= IRQCHIP_PIPELINE_SAFE,
 };
 
 static void bcm2836_arm_irqchip_mask_pmu_irq(struct irq_data *d)
@@ -73,6 +74,7 @@ static struct irq_chip bcm2836_arm_irqchip_pmu = {
 	.name		= "bcm2836-pmu",
 	.irq_mask	= bcm2836_arm_irqchip_mask_pmu_irq,
 	.irq_unmask	= bcm2836_arm_irqchip_unmask_pmu_irq,
+	.flags		= IRQCHIP_PIPELINE_SAFE,
 };
 
 static void bcm2836_arm_irqchip_mask_gpu_irq(struct irq_data *d)
@@ -87,6 +89,7 @@ static struct irq_chip bcm2836_arm_irqchip_gpu = {
 	.name		= "bcm2836-gpu",
 	.irq_mask	= bcm2836_arm_irqchip_mask_gpu_irq,
 	.irq_unmask	= bcm2836_arm_irqchip_unmask_gpu_irq,
+	.flags		= IRQCHIP_PIPELINE_SAFE,
 };
 
 static int bcm2836_map(struct irq_domain *d, unsigned int irq,
diff --git a/drivers/irqchip/irq-gic-v2m.c b/drivers/irqchip/irq-gic-v2m.c
index e88e75c22b6a..7689721b0fb2 100644
--- a/drivers/irqchip/irq-gic-v2m.c
+++ b/drivers/irqchip/irq-gic-v2m.c
@@ -88,6 +88,7 @@ static struct irq_chip gicv2m_msi_irq_chip = {
 	.irq_unmask		= gicv2m_unmask_msi_irq,
 	.irq_eoi		= irq_chip_eoi_parent,
 	.irq_write_msi_msg	= pci_msi_domain_write_msg,
+	.flags			= IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct msi_domain_info gicv2m_msi_domain_info = {
@@ -129,6 +130,7 @@ static struct irq_chip gicv2m_irq_chip = {
 	.irq_eoi		= irq_chip_eoi_parent,
 	.irq_set_affinity	= irq_chip_set_affinity_parent,
 	.irq_compose_msi_msg	= gicv2m_compose_msi_msg,
+	.flags			= IRQCHIP_PIPELINE_SAFE,
 };
 
 static int gicv2m_irq_gic_domain_alloc(struct irq_domain *domain,
@@ -251,6 +253,7 @@ static bool is_msi_spi_valid(u32 base, u32 num)
 
 static struct irq_chip gicv2m_pmsi_irq_chip = {
 	.name			= "pMSI",
+	.flags			= IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct msi_domain_ops gicv2m_pmsi_ops = {
diff --git a/drivers/irqchip/irq-gic-v3.c b/drivers/irqchip/irq-gic-v3.c
index 1edc99335a94..1b07ddd92a17 100644
--- a/drivers/irqchip/irq-gic-v3.c
+++ b/drivers/irqchip/irq-gic-v3.c
@@ -1215,7 +1215,8 @@ static struct irq_chip gic_chip = {
 	.irq_nmi_teardown	= gic_irq_nmi_teardown,
 	.flags			= IRQCHIP_SET_TYPE_MASKED |
 				  IRQCHIP_SKIP_SET_WAKE |
-				  IRQCHIP_MASK_ON_SUSPEND,
+				  IRQCHIP_MASK_ON_SUSPEND |
+				  IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct irq_chip gic_eoimode1_chip = {
@@ -1232,7 +1233,8 @@ static struct irq_chip gic_eoimode1_chip = {
 	.irq_nmi_teardown	= gic_irq_nmi_teardown,
 	.flags			= IRQCHIP_SET_TYPE_MASKED |
 				  IRQCHIP_SKIP_SET_WAKE |
-				  IRQCHIP_MASK_ON_SUSPEND,
+				  IRQCHIP_MASK_ON_SUSPEND |
+				  IRQCHIP_PIPELINE_SAFE,
 };
 
 static int gic_irq_domain_map(struct irq_domain *d, unsigned int irq,
diff --git a/drivers/irqchip/irq-gic.c b/drivers/irqchip/irq-gic.c
index 30ab623343d3..c6a2377069c7 100644
--- a/drivers/irqchip/irq-gic.c
+++ b/drivers/irqchip/irq-gic.c
@@ -90,7 +90,7 @@ struct gic_chip_data {
 
 #ifdef CONFIG_BL_SWITCHER
 
-static DEFINE_RAW_SPINLOCK(cpu_map_lock);
+static DEFINE_HARD_SPINLOCK(cpu_map_lock);
 
 #define gic_lock_irqsave(f)		\
 	raw_spin_lock_irqsave(&cpu_map_lock, (f))
@@ -429,7 +429,8 @@ static const struct irq_chip gic_chip = {
 	.irq_set_irqchip_state	= gic_irq_set_irqchip_state,
 	.flags			= IRQCHIP_SET_TYPE_MASKED |
 				  IRQCHIP_SKIP_SET_WAKE |
-				  IRQCHIP_MASK_ON_SUSPEND,
+				  IRQCHIP_MASK_ON_SUSPEND |
+				  IRQCHIP_PIPELINE_SAFE,
 };
 
 void __init gic_cascade_irq(unsigned int gic_nr, unsigned int irq)
diff --git a/drivers/irqchip/irq-imx-irqsteer.c b/drivers/irqchip/irq-imx-irqsteer.c
index 290531ec3d61..f4d9e59e2072 100644
--- a/drivers/irqchip/irq-imx-irqsteer.c
+++ b/drivers/irqchip/irq-imx-irqsteer.c
@@ -29,7 +29,7 @@ struct irqsteer_data {
 	struct clk		*ipg_clk;
 	int			irq[CHAN_MAX_OUTPUT_INT];
 	int			irq_count;
-	raw_spinlock_t		lock;
+	hard_spinlock_t		lock;
 	int			reg_num;
 	int			channel;
 	struct irq_domain	*domain;
@@ -74,6 +74,7 @@ static struct irq_chip imx_irqsteer_irq_chip = {
 	.name		= "irqsteer",
 	.irq_mask	= imx_irqsteer_irq_mask,
 	.irq_unmask	= imx_irqsteer_irq_unmask,
+	.flags		= IRQCHIP_PIPELINE_SAFE,
 };
 
 static int imx_irqsteer_irq_map(struct irq_domain *h, unsigned int irq,
diff --git a/drivers/mfd/tps65217.c b/drivers/mfd/tps65217.c
index 7566ce4457a0..23095cdc1ced 100644
--- a/drivers/mfd/tps65217.c
+++ b/drivers/mfd/tps65217.c
@@ -84,6 +84,7 @@ static struct irq_chip tps65217_irq_chip = {
 	.irq_bus_sync_unlock	= tps65217_irq_sync_unlock,
 	.irq_enable		= tps65217_irq_enable,
 	.irq_disable		= tps65217_irq_disable,
+	.flags			= IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct mfd_cell tps65217s[] = {
diff --git a/drivers/net/ethernet/freescale/fec_main.c b/drivers/net/ethernet/freescale/fec_main.c
index 4bb30761abfc..4badb4b6e18a 100644
--- a/drivers/net/ethernet/freescale/fec_main.c
+++ b/drivers/net/ethernet/freescale/fec_main.c
@@ -1619,10 +1619,54 @@ fec_enet_interrupt(int irq, void *dev_id)
 	return ret;
 }
 
+static int unmask_events(struct net_device *ndev, unsigned int events)
+{
+	struct fec_enet_private *fep = netdev_priv(ndev);
+	struct fec_enet_priv_rx_q *rxq;
+	int n, queue_id;
+
+	writel(events, fep->hwp + FEC_IMASK);
+	dma_wmb();
+
+	if ((events & FEC_ENET_RXF) == 0)
+		return 0;
+
+	/*
+	 * The FEC sees the unmasking of RXF events in the EIMR while
+	 * RDAR is cleared as an error, and stops issuing RXF
+	 * interrupts as a consequence of this. Since the receive path
+	 * is fully interrupt-driven, this leads to a stall.
+	 *
+	 * Unfortunately, the combination of high network load on the
+	 * receiver side, and delays on the current CPU (kernel
+	 * preemption, IRQ activity) may cause the RX ring to fill up
+	 * again soon after we pulled some/all of the pending frames,
+	 * but _before_ RXF is unmasked in the EIMR, leading to the
+	 * obnoxious RX stall.
+	 *
+	 * To prevent this, we pull exactly one frame from the RX ring
+	 * if RDAR is zero, _after_ the EIMR was updated. RDAR gets
+	 * written to as a consequence of such update, which is enough
+	 * to re-enable the RXF event.
+	 *
+	 * NOTE: a request to unmask RXF implies that we did not
+	 * consume the entire NAPI budget for the current round. This
+	 * means that we may pull at least one more frame without
+	 * exceeding the allowed weight (NAPI_POLL_WEIGHT).
+	 */
+	for (n = 0; n < FEC_ENET_MAX_RX_QS; n++) {
+		queue_id = FEC_ENET_GET_QUQUE(n);
+		rxq = fep->rx_queue[queue_id];
+		if (rxq && readl(rxq->bd.reg_desc_active) == 0)
+			return fec_enet_rx_queue(ndev, 1, queue_id);
+	}
+
+	return 0;
+}
+
 static int fec_enet_rx_napi(struct napi_struct *napi, int budget)
 {
 	struct net_device *ndev = napi->dev;
-	struct fec_enet_private *fep = netdev_priv(ndev);
 	int pkts;
 
 	pkts = fec_enet_rx(ndev, budget);
@@ -1631,8 +1675,10 @@ static int fec_enet_rx_napi(struct napi_struct *napi, int budget)
 
 	if (pkts < budget) {
 		napi_complete_done(napi, pkts);
-		writel(FEC_DEFAULT_IMASK, fep->hwp + FEC_IMASK);
-	}
+		pkts += unmask_events(ndev, FEC_DEFAULT_IMASK);
+	} else
+		unmask_events(ndev, FEC_RX_DISABLED_IMASK);
+
 	return pkts;
 }
 
diff --git a/drivers/pci/controller/dwc/pcie-designware-host.c b/drivers/pci/controller/dwc/pcie-designware-host.c
index 0f36a926059a..46615a6a07cb 100644
--- a/drivers/pci/controller/dwc/pcie-designware-host.c
+++ b/drivers/pci/controller/dwc/pcie-designware-host.c
@@ -66,6 +66,7 @@ static struct irq_chip dw_pcie_msi_irq_chip = {
 	.irq_ack = dw_msi_ack_irq,
 	.irq_mask = dw_msi_mask_irq,
 	.irq_unmask = dw_msi_unmask_irq,
+	.flags = IRQCHIP_PIPELINE_SAFE,
 };
 
 static struct msi_domain_info dw_pcie_msi_domain_info = {
diff --git a/drivers/pinctrl/bcm/pinctrl-bcm2835.c b/drivers/pinctrl/bcm/pinctrl-bcm2835.c
index 0de1a3a96984..3b497500da3c 100644
--- a/drivers/pinctrl/bcm/pinctrl-bcm2835.c
+++ b/drivers/pinctrl/bcm/pinctrl-bcm2835.c
@@ -87,7 +87,7 @@ struct bcm2835_pinctrl {
 	struct gpio_chip gpio_chip;
 	struct pinctrl_gpio_range gpio_range;
 
-	raw_spinlock_t irq_lock[BCM2835_NUM_BANKS];
+	hard_spinlock_t irq_lock[BCM2835_NUM_BANKS];
 };
 
 /* pins are just named GPIO0..GPIO53 */
@@ -616,6 +616,7 @@ static struct irq_chip bcm2835_gpio_irq_chip = {
 	.irq_ack = bcm2835_gpio_irq_ack,
 	.irq_mask = bcm2835_gpio_irq_disable,
 	.irq_unmask = bcm2835_gpio_irq_enable,
+	.flags = IRQCHIP_PIPELINE_SAFE,
 };
 
 static int bcm2835_pctl_get_groups_count(struct pinctrl_dev *pctldev)
diff --git a/drivers/pinctrl/qcom/pinctrl-msm.c b/drivers/pinctrl/qcom/pinctrl-msm.c
index 763da0be10d6..d451ec844bb5 100644
--- a/drivers/pinctrl/qcom/pinctrl-msm.c
+++ b/drivers/pinctrl/qcom/pinctrl-msm.c
@@ -57,7 +57,7 @@ struct msm_pinctrl {
 	struct irq_chip irq_chip;
 	int irq;
 
-	raw_spinlock_t lock;
+	hard_spinlock_t lock;
 
 	DECLARE_BITMAP(dual_edge_irqs, MAX_NR_GPIO);
 	DECLARE_BITMAP(enabled_irqs, MAX_NR_GPIO);
@@ -1027,6 +1027,7 @@ static int msm_gpio_init(struct msm_pinctrl *pctrl)
 	pctrl->irq_chip.irq_set_wake = msm_gpio_irq_set_wake;
 	pctrl->irq_chip.irq_request_resources = msm_gpio_irq_reqres;
 	pctrl->irq_chip.irq_release_resources = msm_gpio_irq_relres;
+	pctrl->irq_chip.flags = IRQCHIP_PIPELINE_SAFE;
 
 	girq = &chip->irq;
 	girq->chip = &pctrl->irq_chip;
diff --git a/drivers/soc/qcom/smp2p.c b/drivers/soc/qcom/smp2p.c
index c7300d54e444..517c0f689feb 100644
--- a/drivers/soc/qcom/smp2p.c
+++ b/drivers/soc/qcom/smp2p.c
@@ -281,6 +281,7 @@ static struct irq_chip smp2p_irq_chip = {
 	.irq_mask       = smp2p_mask_irq,
 	.irq_unmask     = smp2p_unmask_irq,
 	.irq_set_type	= smp2p_set_irq_type,
+	.flags		= IRQCHIP_PIPELINE_SAFE,
 };
 
 static int smp2p_irq_map(struct irq_domain *d,
diff --git a/drivers/spmi/spmi-pmic-arb.c b/drivers/spmi/spmi-pmic-arb.c
index 97acc2ba2912..20c0b7eb904f 100644
--- a/drivers/spmi/spmi-pmic-arb.c
+++ b/drivers/spmi/spmi-pmic-arb.c
@@ -145,7 +145,7 @@ struct spmi_pmic_arb {
 	void __iomem		*cnfg;
 	void __iomem		*core;
 	resource_size_t		core_size;
-	raw_spinlock_t		lock;
+	hard_spinlock_t		lock;
 	u8			channel;
 	int			irq;
 	u8			ee;
@@ -685,7 +685,7 @@ static struct irq_chip pmic_arb_irqchip = {
 	.irq_set_type	= qpnpint_irq_set_type,
 	.irq_set_wake	= qpnpint_irq_set_wake,
 	.irq_get_irqchip_state	= qpnpint_get_irqchip_state,
-	.flags		= IRQCHIP_MASK_ON_SUSPEND,
+	.flags		= IRQCHIP_MASK_ON_SUSPEND|IRQCHIP_PIPELINE_SAFE,
 };
 
 static int qpnpint_irq_domain_translate(struct irq_domain *d,
diff --git a/drivers/tty/serial/8250/8250_core.c b/drivers/tty/serial/8250/8250_core.c
index e682390ce0de..9221053d40a0 100644
--- a/drivers/tty/serial/8250/8250_core.c
+++ b/drivers/tty/serial/8250/8250_core.c
@@ -662,12 +662,57 @@ static int univ8250_console_match(struct console *co, char *name, int idx,
 	return -ENODEV;
 }
 
+#ifdef CONFIG_RAW_PRINTK
+
+static void raw_write_char(struct uart_8250_port *up, int c)
+{
+	unsigned int status, tmout = 10000;
+
+	for (;;) {
+		status = serial_in(up, UART_LSR);
+		up->lsr_saved_flags |= status & LSR_SAVE_FLAGS;
+		if ((status & UART_LSR_THRE) == UART_LSR_THRE)
+			break;
+		if (--tmout == 0)
+			break;
+		cpu_relax();
+	}
+	serial_port_out(&up->port, UART_TX, c);
+}
+
+static void univ8250_console_write_raw(struct console *co, const char *s,
+				       unsigned int count)
+{
+	struct uart_8250_port *up = &serial8250_ports[co->index];
+	unsigned int ier;
+
+        ier = serial_in(up, UART_IER);
+
+        if (up->capabilities & UART_CAP_UUE)
+                serial_out(up, UART_IER, UART_IER_UUE);
+        else
+                serial_out(up, UART_IER, 0);
+
+	while (count-- > 0) {
+		if (*s == '\n')
+			raw_write_char(up, '\r');
+		raw_write_char(up, *s++);
+	}
+
+        serial_out(up, UART_IER, ier);
+}
+
+#endif
+
 static struct console univ8250_console = {
 	.name		= "ttyS",
 	.write		= univ8250_console_write,
 	.device		= uart_console_device,
 	.setup		= univ8250_console_setup,
 	.match		= univ8250_console_match,
+#ifdef CONFIG_RAW_PRINTK
+	.write_raw	= univ8250_console_write_raw,
+#endif
 	.flags		= CON_PRINTBUFFER | CON_ANYTIME,
 	.index		= -1,
 	.data		= &serial8250_reg,
diff --git a/drivers/tty/serial/amba-pl011.c b/drivers/tty/serial/amba-pl011.c
index 3a7d1a66f79c..02816673390e 100644
--- a/drivers/tty/serial/amba-pl011.c
+++ b/drivers/tty/serial/amba-pl011.c
@@ -1880,6 +1880,8 @@ static void pl011_shutdown(struct uart_port *port)
 
 	pl011_disable_uart(uap);
 
+	if (IS_ENABLED(CONFIG_RAW_PRINTK))
+		clk_disable(uap->clk);
 	/*
 	 * Shut down the clock producer
 	 */
@@ -2206,6 +2208,37 @@ static void pl011_console_putchar(struct uart_port *port, int ch)
 	pl011_write(ch, uap, REG_DR);
 }
 
+#ifdef CONFIG_RAW_PRINTK
+
+static void
+pl011_console_write_raw(struct console *co, const char *s, unsigned int count)
+{
+	struct uart_amba_port *uap = amba_ports[co->index];
+	unsigned int old_cr = 0, new_cr;
+
+	if (!uap->vendor->always_enabled) {
+		old_cr = pl011_read(uap, REG_CR);
+		new_cr = old_cr & ~UART011_CR_CTSEN;
+		new_cr |= UART01x_CR_UARTEN | UART011_CR_TXE;
+		pl011_write(new_cr, uap, REG_CR);
+	}
+
+	while (count-- > 0) {
+		if (*s == '\n')
+			pl011_console_putchar(&uap->port, '\r');
+		pl011_console_putchar(&uap->port, *s++);
+	}
+
+	while ((pl011_read(uap, REG_FR) ^ uap->vendor->inv_fr)
+		& uap->vendor->fr_busy)
+		cpu_relax();
+
+	if (!uap->vendor->always_enabled)
+		pl011_write(old_cr, uap, REG_CR);
+}
+
+#endif  /* !CONFIG_RAW_PRINTK */
+
 static void
 pl011_console_write(struct console *co, const char *s, unsigned int count)
 {
@@ -2336,6 +2369,9 @@ static int __init pl011_console_setup(struct console *co, char *options)
 			pl011_console_get_options(uap, &baud, &parity, &bits);
 	}
 
+	if (IS_ENABLED(CONFIG_RAW_PRINTK))
+		clk_enable(uap->clk);
+
 	return uart_set_options(&uap->port, co, baud, parity, bits, flow);
 }
 
@@ -2406,6 +2442,9 @@ static struct console amba_console = {
 	.device		= uart_console_device,
 	.setup		= pl011_console_setup,
 	.match		= pl011_console_match,
+#ifdef CONFIG_RAW_PRINTK
+	.write_raw	= pl011_console_write_raw,
+#endif
 	.flags		= CON_PRINTBUFFER | CON_ANYTIME,
 	.index		= -1,
 	.data		= &amba_reg,
diff --git a/drivers/tty/serial/imx.c b/drivers/tty/serial/imx.c
index 5e08f2657b90..db9c47eafa1d 100644
--- a/drivers/tty/serial/imx.c
+++ b/drivers/tty/serial/imx.c
@@ -1904,30 +1904,33 @@ static void imx_uart_console_putchar(struct uart_port *port, int ch)
  * Interrupts are disabled on entering
  */
 static void
-imx_uart_console_write(struct console *co, const char *s, unsigned int count)
+__imx_uart_console_write(struct console *co, const char *s, unsigned int count, int locked)
 {
 	struct imx_port *sport = imx_uart_ports[co->index];
 	struct imx_port_ucrs old_ucr;
 	unsigned int ucr1;
 	unsigned long flags = 0;
-	int locked = 1;
 	int retval;
 
-	retval = clk_enable(sport->clk_per);
-	if (retval)
-		return;
-	retval = clk_enable(sport->clk_ipg);
-	if (retval) {
-		clk_disable(sport->clk_per);
-		return;
+	if (!IS_ENABLED(CONFIG_RAW_PRINTK)) {
+		retval = clk_enable(sport->clk_per);
+		if (retval)
+			return;
+		retval = clk_enable(sport->clk_ipg);
+		if (retval) {
+			clk_disable(sport->clk_per);
+			return;
+		}
 	}
 
-	if (sport->port.sysrq)
-		locked = 0;
-	else if (oops_in_progress)
-		locked = spin_trylock_irqsave(&sport->port.lock, flags);
-	else
-		spin_lock_irqsave(&sport->port.lock, flags);
+	if (locked) {
+		if (sport->port.sysrq)
+			locked = 0;
+		else if (oops_in_progress)
+			locked = spin_trylock_irqsave(&sport->port.lock, flags);
+		else
+			spin_lock_irqsave(&sport->port.lock, flags);
+	}
 
 	/*
 	 *	First, save UCR1/2/3 and then disable interrupts
@@ -1957,10 +1960,26 @@ imx_uart_console_write(struct console *co, const char *s, unsigned int count)
 	if (locked)
 		spin_unlock_irqrestore(&sport->port.lock, flags);
 
-	clk_disable(sport->clk_ipg);
-	clk_disable(sport->clk_per);
+	if (!IS_ENABLED(CONFIG_RAW_PRINTK)) {
+		clk_disable(sport->clk_ipg);
+		clk_disable(sport->clk_per);
+	}
+}
+
+static void
+imx_uart_console_write(struct console *co, const char *s, unsigned int count)
+{
+	return __imx_uart_console_write(co, s, count, 1);
 }
 
+#ifdef CONFIG_RAW_PRINTK
+static void
+imx_uart_console_write_raw(struct console *co, const char *s, unsigned int count)
+{
+	return __imx_uart_console_write(co, s, count, 0);
+}
+#endif
+
 /*
  * If the port was already initialised (eg, by a boot loader),
  * try to determine the current setup.
@@ -2068,6 +2087,10 @@ imx_uart_console_setup(struct console *co, char *options)
 	retval = clk_prepare(sport->clk_per);
 	if (retval)
 		clk_unprepare(sport->clk_ipg);
+	else if (IS_ENABLED(CONFIG_RAW_PRINTK)) {
+		clk_enable(sport->clk_ipg);
+		clk_enable(sport->clk_per);
+	}
 
 error_console:
 	return retval;
@@ -2077,6 +2100,9 @@ static struct uart_driver imx_uart_uart_driver;
 static struct console imx_uart_console = {
 	.name		= DEV_NAME,
 	.write		= imx_uart_console_write,
+#ifdef CONFIG_RAW_PRINTK
+	.write_raw	= imx_uart_console_write_raw,
+#endif
 	.device		= uart_console_device,
 	.setup		= imx_uart_console_setup,
 	.flags		= CON_PRINTBUFFER,
diff --git a/drivers/tty/serial/st-asc.c b/drivers/tty/serial/st-asc.c
index 7971997cdead..e80561f15eb5 100644
--- a/drivers/tty/serial/st-asc.c
+++ b/drivers/tty/serial/st-asc.c
@@ -909,6 +909,29 @@ static void asc_console_write(struct console *co, const char *s, unsigned count)
 		spin_unlock_irqrestore(&port->lock, flags);
 }
 
+#ifdef CONFIG_RAW_PRINTK
+
+static void asc_console_write_raw(struct console *co,
+				  const char *s, unsigned int count)
+{
+	struct uart_port *port = &asc_ports[co->index].port;
+	unsigned long timeout = 1000000;
+	u32 intenable;
+
+	intenable = asc_in(port, ASC_INTEN);
+	asc_out(port, ASC_INTEN, 0);
+	(void)asc_in(port, ASC_INTEN);	/* Defeat bus write posting */
+
+	uart_console_write(port, s, count, asc_console_putchar);
+
+	while (timeout-- && !asc_txfifo_is_empty(port))
+		cpu_relax();	/* wait shorter */
+
+	asc_out(port, ASC_INTEN, intenable);
+}
+
+#endif
+
 static int asc_console_setup(struct console *co, char *options)
 {
 	struct asc_port *ascport;
@@ -941,6 +964,9 @@ static struct console asc_console = {
 	.name		= ASC_SERIAL_NAME,
 	.device		= uart_console_device,
 	.write		= asc_console_write,
+#ifdef CONFIG_RAW_PRINTK
+	.write_raw	= asc_console_write_raw,
+#endif
 	.setup		= asc_console_setup,
 	.flags		= CON_PRINTBUFFER,
 	.index		= -1,
diff --git a/fs/exec.c b/fs/exec.c
index 555e93c7dec8..42fe0016f110 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -1011,6 +1011,7 @@ static int exec_mmap(struct mm_struct *mm)
 {
 	struct task_struct *tsk;
 	struct mm_struct *old_mm, *active_mm;
+	unsigned long flags;
 
 	/* Notify parent that we're no longer interested in the old VM */
 	tsk = current;
@@ -1035,8 +1036,10 @@ static int exec_mmap(struct mm_struct *mm)
 	active_mm = tsk->active_mm;
 	membarrier_exec_mmap(mm);
 	tsk->mm = mm;
+	protect_inband_mm(flags);
 	tsk->active_mm = mm;
 	activate_mm(active_mm, mm);
+	unprotect_inband_mm(flags);
 	tsk->mm->vmacache_seqnum = 0;
 	vmacache_flush(tsk);
 	task_unlock(tsk);
diff --git a/fs/file.c b/fs/file.c
index 3da91a112bab..d4fbe05210ab 100644
--- a/fs/file.c
+++ b/fs/file.c
@@ -385,6 +385,7 @@ static struct fdtable *close_files(struct files_struct * files)
 			if (set & 1) {
 				struct file * file = xchg(&fdt->fd[i], NULL);
 				if (file) {
+					uninstall_inband_fd(i, file, files);
 					filp_close(file, files);
 					cond_resched();
 				}
@@ -597,6 +598,7 @@ void __fd_install(struct files_struct *files, unsigned int fd,
 		fdt = files_fdtable(files);
 		BUG_ON(fdt->fd[fd] != NULL);
 		rcu_assign_pointer(fdt->fd[fd], file);
+		install_inband_fd(fd, file, files);
 		spin_unlock(&files->file_lock);
 		return;
 	}
@@ -605,6 +607,7 @@ void __fd_install(struct files_struct *files, unsigned int fd,
 	fdt = rcu_dereference_sched(files->fdt);
 	BUG_ON(fdt->fd[fd] != NULL);
 	rcu_assign_pointer(fdt->fd[fd], file);
+	install_inband_fd(fd, file, files);
 	rcu_read_unlock_sched();
 }
 
@@ -632,6 +635,7 @@ int __close_fd(struct files_struct *files, unsigned fd)
 		goto out_unlock;
 	rcu_assign_pointer(fdt->fd[fd], NULL);
 	__put_unused_fd(files, fd);
+	uninstall_inband_fd(fd, file, files);
 	spin_unlock(&files->file_lock);
 	return filp_close(file, files);
 
@@ -659,6 +663,7 @@ int __close_fd_get_file(unsigned int fd, struct file **res)
 		goto out_unlock;
 	rcu_assign_pointer(fdt->fd[fd], NULL);
 	__put_unused_fd(files, fd);
+	uninstall_inband_fd(fd, file, files);
 	spin_unlock(&files->file_lock);
 	get_file(file);
 	*res = file;
@@ -696,6 +701,7 @@ void do_close_on_exec(struct files_struct *files)
 				continue;
 			rcu_assign_pointer(fdt->fd[fd], NULL);
 			__put_unused_fd(files, fd);
+			uninstall_inband_fd(fd, file, files);
 			spin_unlock(&files->file_lock);
 			filp_close(file, files);
 			cond_resched();
@@ -872,6 +878,7 @@ __releases(&files->file_lock)
 		__set_close_on_exec(fd, fdt);
 	else
 		__clear_close_on_exec(fd, fdt);
+	replace_inband_fd(fd, file, files);
 	spin_unlock(&files->file_lock);
 
 	if (tofree)
diff --git a/include/asm-generic/atomic.h b/include/asm-generic/atomic.h
index 286867f593d2..e10b5059572d 100644
--- a/include/asm-generic/atomic.h
+++ b/include/asm-generic/atomic.h
@@ -76,9 +76,9 @@ static inline void atomic_##op(int i, atomic_t *v)			\
 {									\
 	unsigned long flags;						\
 									\
-	raw_local_irq_save(flags);					\
+	flags = hard_local_irq_save();					\
 	v->counter = v->counter c_op i;					\
-	raw_local_irq_restore(flags);					\
+	hard_local_irq_restore(flags);					\
 }
 
 #define ATOMIC_OP_RETURN(op, c_op)					\
@@ -87,9 +87,9 @@ static inline int atomic_##op##_return(int i, atomic_t *v)		\
 	unsigned long flags;						\
 	int ret;							\
 									\
-	raw_local_irq_save(flags);					\
+	flags = hard_local_irq_save();					\
 	ret = (v->counter = v->counter c_op i);				\
-	raw_local_irq_restore(flags);					\
+	hard_local_irq_restore(flags);					\
 									\
 	return ret;							\
 }
@@ -100,10 +100,10 @@ static inline int atomic_fetch_##op(int i, atomic_t *v)			\
 	unsigned long flags;						\
 	int ret;							\
 									\
-	raw_local_irq_save(flags);					\
+	flags = hard_local_irq_save();					\
 	ret = v->counter;						\
 	v->counter = v->counter c_op i;					\
-	raw_local_irq_restore(flags);					\
+	hard_local_irq_restore(flags);					\
 									\
 	return ret;							\
 }
diff --git a/include/asm-generic/cmpxchg-local.h b/include/asm-generic/cmpxchg-local.h
index f17f14f84d09..67d712ff0f01 100644
--- a/include/asm-generic/cmpxchg-local.h
+++ b/include/asm-generic/cmpxchg-local.h
@@ -23,7 +23,7 @@ static inline unsigned long __cmpxchg_local_generic(volatile void *ptr,
 	if (size == 8 && sizeof(unsigned long) != 8)
 		wrong_size_cmpxchg(ptr);
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 	switch (size) {
 	case 1: prev = *(u8 *)ptr;
 		if (prev == old)
@@ -44,7 +44,7 @@ static inline unsigned long __cmpxchg_local_generic(volatile void *ptr,
 	default:
 		wrong_size_cmpxchg(ptr);
 	}
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 	return prev;
 }
 
@@ -57,11 +57,11 @@ static inline u64 __cmpxchg64_local_generic(volatile void *ptr,
 	u64 prev;
 	unsigned long flags;
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 	prev = *(u64 *)ptr;
 	if (prev == old)
 		*(u64 *)ptr = new;
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 	return prev;
 }
 
diff --git a/include/asm-generic/cmpxchg.h b/include/asm-generic/cmpxchg.h
index 9a24510cd8c1..475206bd5c85 100644
--- a/include/asm-generic/cmpxchg.h
+++ b/include/asm-generic/cmpxchg.h
@@ -32,10 +32,10 @@ unsigned long __xchg(unsigned long x, volatile void *ptr, int size)
 #ifdef __xchg_u8
 		return __xchg_u8(x, ptr);
 #else
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 		ret = *(volatile u8 *)ptr;
 		*(volatile u8 *)ptr = x;
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 		return ret;
 #endif /* __xchg_u8 */
 
@@ -43,10 +43,10 @@ unsigned long __xchg(unsigned long x, volatile void *ptr, int size)
 #ifdef __xchg_u16
 		return __xchg_u16(x, ptr);
 #else
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 		ret = *(volatile u16 *)ptr;
 		*(volatile u16 *)ptr = x;
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 		return ret;
 #endif /* __xchg_u16 */
 
@@ -54,10 +54,10 @@ unsigned long __xchg(unsigned long x, volatile void *ptr, int size)
 #ifdef __xchg_u32
 		return __xchg_u32(x, ptr);
 #else
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 		ret = *(volatile u32 *)ptr;
 		*(volatile u32 *)ptr = x;
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 		return ret;
 #endif /* __xchg_u32 */
 
@@ -66,10 +66,10 @@ unsigned long __xchg(unsigned long x, volatile void *ptr, int size)
 #ifdef __xchg_u64
 		return __xchg_u64(x, ptr);
 #else
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 		ret = *(volatile u64 *)ptr;
 		*(volatile u64 *)ptr = x;
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 		return ret;
 #endif /* __xchg_u64 */
 #endif /* CONFIG_64BIT */
diff --git a/include/asm-generic/irq_pipeline.h b/include/asm-generic/irq_pipeline.h
new file mode 100644
index 000000000000..f056d9c17123
--- /dev/null
+++ b/include/asm-generic/irq_pipeline.h
@@ -0,0 +1,122 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#ifndef __ASM_GENERIC_IRQ_PIPELINE_H
+#define __ASM_GENERIC_IRQ_PIPELINE_H
+
+#include <linux/kconfig.h>
+#include <linux/types.h>
+
+#ifdef CONFIG_IRQ_PIPELINE
+
+#include <asm/bitsperlong.h>
+
+unsigned long inband_irq_save(void);
+void inband_irq_restore(unsigned long flags);
+void __inband_irq_enable(void);
+void inband_irq_enable(void);
+void inband_irq_disable(void);
+void inband_irq_restore_nosync(unsigned long flags);
+unsigned long inband_irqs_disabled(void);
+
+#define hard_cond_local_irq_enable()		hard_local_irq_enable()
+#define hard_cond_local_irq_disable()		hard_local_irq_disable()
+#define hard_cond_local_irq_save()		hard_local_irq_save()
+#define hard_cond_local_irq_restore(__flags)	hard_local_irq_restore(__flags)
+
+#define hard_local_irq_save()			native_irq_save()
+#define hard_local_irq_restore(__flags)		native_irq_restore(__flags)
+#define hard_local_irq_enable()			native_irq_enable()
+#define hard_local_irq_disable()		native_irq_disable()
+#define hard_local_save_flags()			native_save_flags()
+
+#define hard_irqs_disabled()			native_irqs_disabled()
+#define hard_irqs_disabled_flags(__flags)	native_irqs_disabled_flags(__flags)
+
+void irq_pipeline_nmi_enter(void);
+void irq_pipeline_nmi_exit(void);
+
+/* Swap then merge virtual and hardware interrupt states. */
+#define irqs_merge_flags(__flags, __stalled)				\
+	({								\
+		unsigned long __combo =					\
+			arch_irqs_virtual_to_native_flags(__stalled) |	\
+			arch_irqs_native_to_virtual_flags(__flags);	\
+		__combo;						\
+	})
+
+/* Extract swap virtual and hardware interrupt states. */
+#define irqs_split_flags(__combo, __stall_r)				\
+	({								\
+		unsigned long __virt = (__combo);			\
+		*(__stall_r) = hard_irqs_disabled_flags(__combo);	\
+		__virt &= ~arch_irqs_virtual_to_native_flags(*(__stall_r)); \
+		arch_irqs_virtual_to_native_flags(__virt);		\
+	})
+
+#else /* !CONFIG_IRQ_PIPELINE */
+
+#define hard_local_save_flags()			({ unsigned long __flags; \
+						raw_local_save_flags(__flags); __flags; })
+#define hard_local_irq_enable()			raw_local_irq_enable()
+#define hard_local_irq_disable()		raw_local_irq_disable()
+#define hard_local_irq_save()			({ unsigned long __flags; \
+						raw_local_irq_save(__flags); __flags; })
+#define hard_local_irq_restore(__flags)		raw_local_irq_restore(__flags)
+
+#define hard_cond_local_irq_enable()		do { } while(0)
+#define hard_cond_local_irq_disable()		do { } while(0)
+#define hard_cond_local_irq_save()		0
+#define hard_cond_local_irq_restore(__flags)	do { (void)(__flags); } while(0)
+
+#define hard_irqs_disabled()			irqs_disabled()
+#define hard_irqs_disabled_flags(__flags)	raw_irqs_disabled_flags(__flags)
+
+static inline void irq_pipeline_nmi_enter(void) { }
+static inline void irq_pipeline_nmi_exit(void) { }
+
+#endif /* !CONFIG_IRQ_PIPELINE */
+
+#define hard_local_irq_sync()			native_irq_sync()
+
+#if defined(CONFIG_SMP) && defined(CONFIG_IRQ_PIPELINE)
+#define hard_smp_local_irq_save()		hard_local_irq_save()
+#define hard_smp_local_irq_restore(__flags)	hard_local_irq_restore(__flags)
+#else /* !CONFIG_SMP */
+#define hard_smp_local_irq_save()		0
+#define hard_smp_local_irq_restore(__flags)	do { (void)(__flags); } while(0)
+#endif /* CONFIG_SMP */
+
+#ifdef CONFIG_DEBUG_IRQ_PIPELINE
+void check_inband_stage(void);
+#define check_hard_irqs_disabled()		\
+	WARN_ON_ONCE(!hard_irqs_disabled())
+#define check_hard_irqs_disabled_in_smp()	\
+	WARN_ON_ONCE(IS_ENABLED(CONFIG_SMP) && !hard_irqs_disabled())
+#else
+static inline void check_inband_stage(void) { }
+static inline int check_hard_irqs_disabled(void) { return 0; }
+static inline int check_hard_irqs_disabled_in_smp(void) { return 0; }
+#endif
+
+extern bool irq_pipeline_oopsing;
+
+static inline bool irqs_pipelined(void)
+{
+	return IS_ENABLED(CONFIG_IRQ_PIPELINE);
+}
+
+static inline bool irq_pipeline_debug(void)
+{
+	return IS_ENABLED(CONFIG_DEBUG_IRQ_PIPELINE) &&
+		!irq_pipeline_oopsing;
+}
+
+static inline bool irq_pipeline_debug_locking(void)
+{
+	return IS_ENABLED(CONFIG_DEBUG_HARD_LOCKS);
+}
+
+#endif /* __ASM_GENERIC_IRQ_PIPELINE_H */
diff --git a/include/asm-generic/percpu.h b/include/asm-generic/percpu.h
index c2de013b2cf4..e8800e5b968e 100644
--- a/include/asm-generic/percpu.h
+++ b/include/asm-generic/percpu.h
@@ -125,9 +125,9 @@ do {									\
 ({									\
 	typeof(pcp) __ret;						\
 	unsigned long __flags;						\
-	raw_local_irq_save(__flags);					\
+	__flags = hard_local_irq_save();				\
 	__ret = raw_cpu_generic_read(pcp);				\
-	raw_local_irq_restore(__flags);					\
+	hard_local_irq_restore(__flags);				\
 	__ret;								\
 })
 
@@ -144,9 +144,9 @@ do {									\
 #define this_cpu_generic_to_op(pcp, val, op)				\
 do {									\
 	unsigned long __flags;						\
-	raw_local_irq_save(__flags);					\
+	__flags = hard_local_irq_save();				\
 	raw_cpu_generic_to_op(pcp, val, op);				\
-	raw_local_irq_restore(__flags);					\
+	hard_local_irq_restore(__flags);				\
 } while (0)
 
 
@@ -154,9 +154,9 @@ do {									\
 ({									\
 	typeof(pcp) __ret;						\
 	unsigned long __flags;						\
-	raw_local_irq_save(__flags);					\
+	__flags = hard_local_irq_save();				\
 	__ret = raw_cpu_generic_add_return(pcp, val);			\
-	raw_local_irq_restore(__flags);					\
+	hard_local_irq_restore(__flags);				\
 	__ret;								\
 })
 
@@ -164,9 +164,9 @@ do {									\
 ({									\
 	typeof(pcp) __ret;						\
 	unsigned long __flags;						\
-	raw_local_irq_save(__flags);					\
+	__flags = hard_local_irq_save();				\
 	__ret = raw_cpu_generic_xchg(pcp, nval);			\
-	raw_local_irq_restore(__flags);					\
+	hard_local_irq_restore(__flags);				\
 	__ret;								\
 })
 
@@ -174,9 +174,9 @@ do {									\
 ({									\
 	typeof(pcp) __ret;						\
 	unsigned long __flags;						\
-	raw_local_irq_save(__flags);					\
+	__flags = hard_local_irq_save();				\
 	__ret = raw_cpu_generic_cmpxchg(pcp, oval, nval);		\
-	raw_local_irq_restore(__flags);					\
+	hard_local_irq_restore(__flags);				\
 	__ret;								\
 })
 
@@ -184,10 +184,10 @@ do {									\
 ({									\
 	int __ret;							\
 	unsigned long __flags;						\
-	raw_local_irq_save(__flags);					\
+	__flags = hard_local_irq_save();				\
 	__ret = raw_cpu_generic_cmpxchg_double(pcp1, pcp2,		\
 			oval1, oval2, nval1, nval2);			\
-	raw_local_irq_restore(__flags);					\
+	hard_local_irq_restore(__flags);				\
 	__ret;								\
 })
 
diff --git a/include/dovetail/irq.h b/include/dovetail/irq.h
new file mode 100644
index 000000000000..ac8b5310e29d
--- /dev/null
+++ b/include/dovetail/irq.h
@@ -0,0 +1,11 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _DOVETAIL_IRQ_H
+#define _DOVETAIL_IRQ_H
+
+/* Placeholders for pre- and post-IRQ handling. */
+
+static inline void irq_enter_pipeline(void) { }
+
+static inline void irq_exit_pipeline(void) { }
+
+#endif /* !_DOVETAIL_IRQ_H */
diff --git a/include/dovetail/mm_info.h b/include/dovetail/mm_info.h
new file mode 100644
index 000000000000..504bd1d875d1
--- /dev/null
+++ b/include/dovetail/mm_info.h
@@ -0,0 +1,12 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _DOVETAIL_MM_INFO_H
+#define _DOVETAIL_MM_INFO_H
+
+/*
+ * Placeholder for per-mm state information defined by the co-kernel.
+ */
+
+struct oob_mm_state {
+};
+
+#endif /* !_DOVETAIL_MM_INFO_H */
diff --git a/include/dovetail/thread_info.h b/include/dovetail/thread_info.h
new file mode 100644
index 000000000000..4dea8bf1ecff
--- /dev/null
+++ b/include/dovetail/thread_info.h
@@ -0,0 +1,13 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _DOVETAIL_THREAD_INFO_H
+#define _DOVETAIL_THREAD_INFO_H
+
+/*
+ * Placeholder for per-thread state information defined by the
+ * co-kernel.
+ */
+
+struct oob_thread_state {
+};
+
+#endif /* !_DOVETAIL_THREAD_INFO_H */
diff --git a/include/linux/clockchips.h b/include/linux/clockchips.h
index 8ae9a95ebf5b..dd980dca81e7 100644
--- a/include/linux/clockchips.h
+++ b/include/linux/clockchips.h
@@ -15,6 +15,7 @@
 # include <linux/cpumask.h>
 # include <linux/ktime.h>
 # include <linux/notifier.h>
+# include <linux/irq_pipeline.h>
 
 struct clock_event_device;
 struct module;
@@ -38,6 +39,7 @@ enum clock_event_state {
 	CLOCK_EVT_STATE_PERIODIC,
 	CLOCK_EVT_STATE_ONESHOT,
 	CLOCK_EVT_STATE_ONESHOT_STOPPED,
+	CLOCK_EVT_STATE_RESERVED,
 };
 
 /*
@@ -67,6 +69,17 @@ enum clock_event_state {
  */
 # define CLOCK_EVT_FEAT_HRTIMER		0x000080
 
+/*
+ * Interrupt pipeline support:
+ *
+ * - Clockevent device can work with pipelined timer events (i.e. proxied).
+ * - Device currently delivers high-precision events via out-of-band interrupts.
+ * - Device acts as a proxy for timer interrupt pipelining.
+ */
+# define CLOCK_EVT_FEAT_PIPELINE	0x000100
+# define CLOCK_EVT_FEAT_OOB		0x000200
+# define CLOCK_EVT_FEAT_PROXY		0x000400
+
 /**
  * struct clock_event_device - clock event device descriptor
  * @event_handler:	Assigned by the framework to be called by the low
@@ -91,7 +104,7 @@ enum clock_event_state {
  * @max_delta_ticks:	maximum delta value in ticks stored for reconfiguration
  * @name:		ptr to clock event name
  * @rating:		variable to rate clock event devices
- * @irq:		IRQ number (only for non CPU local devices)
+ * @irq:		IRQ number (only for non CPU local devices, or pipelined timers)
  * @bound_on:		Bound on CPU
  * @cpumask:		cpumask to indicate for which CPUs this device works
  * @list:		list head for the management code
@@ -137,6 +150,11 @@ static inline bool clockevent_state_detached(struct clock_event_device *dev)
 	return dev->state_use_accessors == CLOCK_EVT_STATE_DETACHED;
 }
 
+static inline bool clockevent_state_reserved(struct clock_event_device *dev)
+{
+	return dev->state_use_accessors == CLOCK_EVT_STATE_RESERVED;
+}
+
 static inline bool clockevent_state_shutdown(struct clock_event_device *dev)
 {
 	return dev->state_use_accessors == CLOCK_EVT_STATE_SHUTDOWN;
@@ -157,6 +175,11 @@ static inline bool clockevent_state_oneshot_stopped(struct clock_event_device *d
 	return dev->state_use_accessors == CLOCK_EVT_STATE_ONESHOT_STOPPED;
 }
 
+static inline bool clockevent_is_oob(struct clock_event_device *dev)
+{
+	return !!(dev->features & CLOCK_EVT_FEAT_OOB);
+}
+
 /*
  * Calculate a multiplication factor for scaled math, which is used to convert
  * nanoseconds based values to clock ticks:
@@ -186,6 +209,8 @@ extern int clockevents_unbind_device(struct clock_event_device *ced, int cpu);
 extern void clockevents_config_and_register(struct clock_event_device *dev,
 					    u32 freq, unsigned long min_delta,
 					    unsigned long max_delta);
+extern void clockevents_switch_state(struct clock_event_device *dev,
+				     enum clock_event_state state);
 
 extern int clockevents_update_freq(struct clock_event_device *ce, u32 freq);
 
@@ -215,6 +240,49 @@ static inline int tick_check_broadcast_expired(void) { return 0; }
 static inline void tick_setup_hrtimer_broadcast(void) { }
 # endif
 
+#ifdef CONFIG_IRQ_PIPELINE
+
+struct clock_proxy_device {
+	struct clock_event_device proxy_device;
+	struct clock_event_device *real_device;
+	void (*handle_oob_event)(struct clock_event_device *dev);
+	void (*__setup_handler)(struct clock_proxy_device *dev);
+	void (*__original_handler)(struct clock_event_device *dev);
+};
+
+void tick_notify_proxy(void);
+
+static inline
+void clockevents_handle_event(struct clock_event_device *ced)
+{
+	/*
+	 * If called from the in-band stage, or for delivering a
+	 * high-precision timer event to the out-of-band stage, call
+	 * the event handler immediately.
+	 *
+	 * Otherwise, ced is still the in-band tick device for the
+	 * current CPU, so just relay the incoming tick to the in-band
+	 * stage via tick_notify_proxy().  This situation can happen
+	 * when all CPUs receive the same out-of-band IRQ from a given
+	 * clock event device, but only a subset of the online CPUs has
+	 * enabled a proxy.
+	 */
+	if (clockevent_is_oob(ced) || running_inband())
+		ced->event_handler(ced);
+	else
+		tick_notify_proxy();
+}
+
+#else
+
+static inline
+void clockevents_handle_event(struct clock_event_device *ced)
+{
+	ced->event_handler(ced);
+}
+
+#endif	/* !CONFIG_IRQ_PIPELINE */
+
 #else /* !CONFIG_GENERIC_CLOCKEVENTS: */
 
 static inline void clockevents_suspend(void) { }
diff --git a/include/linux/clocksource.h b/include/linux/clocksource.h
index b21db536fd52..971ce7a3bde1 100644
--- a/include/linux/clocksource.h
+++ b/include/linux/clocksource.h
@@ -13,12 +13,15 @@
 #include <linux/timex.h>
 #include <linux/time.h>
 #include <linux/list.h>
+#include <linux/hashtable.h>
 #include <linux/cache.h>
 #include <linux/timer.h>
+#include <linux/cdev.h>
 #include <linux/init.h>
 #include <linux/of.h>
 #include <asm/div64.h>
 #include <asm/io.h>
+#include <uapi/linux/clocksource.h>
 
 struct clocksource;
 struct module;
@@ -109,6 +112,36 @@ struct clocksource {
 	struct module *owner;
 };
 
+struct clocksource_mmio {
+	void __iomem *reg;
+	struct clocksource clksrc;
+};
+
+struct clocksource_user_mmio {
+	struct clocksource_mmio mmio;
+	void __iomem *reg_upper;
+	unsigned int bits_lower;
+	unsigned int mask_lower;
+	unsigned int mask_upper;
+	enum clksrc_user_mmio_type type;
+	unsigned long phys_lower;
+	unsigned long phys_upper;
+	unsigned int id;
+	struct device *dev;
+	struct cdev cdev;
+	DECLARE_HASHTABLE(mappings, 10);
+	struct spinlock lock;
+	struct list_head link;
+};
+
+struct clocksource_mmio_regs {
+	void __iomem *reg_upper;
+	void __iomem *reg_lower;
+	unsigned int bits_upper;
+	unsigned int bits_lower;
+	unsigned long (*revmap)(void *);
+};
+
 /*
  * Clock source flags bits::
  */
@@ -253,10 +286,21 @@ extern u64 clocksource_mmio_readl_up(struct clocksource *);
 extern u64 clocksource_mmio_readl_down(struct clocksource *);
 extern u64 clocksource_mmio_readw_up(struct clocksource *);
 extern u64 clocksource_mmio_readw_down(struct clocksource *);
+extern u64 clocksource_dual_mmio_readw_up(struct clocksource *);
+extern u64 clocksource_dual_mmio_readl_up(struct clocksource *);
 
 extern int clocksource_mmio_init(void __iomem *, const char *,
 	unsigned long, int, unsigned, u64 (*)(struct clocksource *));
 
+extern int clocksource_user_mmio_init(struct clocksource_user_mmio *ucs,
+				      const struct clocksource_mmio_regs *regs,
+				      unsigned long hz);
+
+extern int clocksource_user_single_mmio_init(
+	void __iomem *base, const char *name,
+	unsigned long hz, int rating, unsigned int bits,
+	u64 (*read)(struct clocksource *));
+
 extern int clocksource_i8253_init(void);
 
 #define TIMER_OF_DECLARE(name, compat, fn) \
diff --git a/include/linux/console.h b/include/linux/console.h
index d09951d5a94e..f45d24f0e1ce 100644
--- a/include/linux/console.h
+++ b/include/linux/console.h
@@ -145,6 +145,7 @@ static inline int con_debug_leave(void)
 struct console {
 	char	name[16];
 	void	(*write)(struct console *, const char *, unsigned);
+	void	(*write_raw)(struct console *, const char *, unsigned);
 	int	(*read)(struct console *, char *, unsigned);
 	struct tty_driver *(*device)(struct console *, int *);
 	void	(*unblank)(void);
diff --git a/include/linux/context_tracking.h b/include/linux/context_tracking.h
index d05609ad329d..2bec38de9647 100644
--- a/include/linux/context_tracking.h
+++ b/include/linux/context_tracking.h
@@ -108,7 +108,7 @@ static inline void guest_enter_irqoff(void)
 	else
 		current->flags |= PF_VCPU;
 
-	if (context_tracking_is_enabled())
+	if (running_inband() && context_tracking_is_enabled())
 		__context_tracking_enter(CONTEXT_GUEST);
 
 	/* KVM does not hold any references to rcu protected data when it
diff --git a/include/linux/dovetail.h b/include/linux/dovetail.h
new file mode 100644
index 000000000000..466858c67192
--- /dev/null
+++ b/include/linux/dovetail.h
@@ -0,0 +1,282 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#ifndef _LINUX_DOVETAIL_H
+#define _LINUX_DOVETAIL_H
+
+#ifdef CONFIG_DOVETAIL
+
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/thread_info.h>
+#include <asm/dovetail.h>
+
+struct pt_regs;
+struct task_struct;
+struct file;
+struct files_struct;
+
+enum inband_event_type {
+	INBAND_TASK_SCHEDULE,
+	INBAND_TASK_SIGNAL,
+	INBAND_TASK_MIGRATION,
+	INBAND_TASK_EXIT,
+	INBAND_PROCESS_CLEANUP,
+};
+
+struct dovetail_migration_data {
+	struct task_struct *task;
+	int dest_cpu;
+};
+
+struct dovetail_altsched_context {
+	struct task_struct *task;
+	struct mm_struct *active_mm;
+	bool borrowed_mm;
+};
+
+void inband_task_init(struct task_struct *p);
+
+int pipeline_syscall(struct thread_info *ti,
+		     unsigned long syscall, struct pt_regs *regs);
+
+void __oob_trap_notify(unsigned int trapnr,
+		       struct pt_regs *regs);
+
+static inline void oob_trap_notify(unsigned int trapnr,
+				   struct pt_regs *regs)
+{
+	if (running_oob())
+		__oob_trap_notify(trapnr, regs);
+}
+
+void inband_event_notify(enum inband_event_type,
+			 void *data);
+
+void inband_clock_was_set(void);
+
+static inline void inband_signal_notify(struct task_struct *p)
+{
+	if (test_ti_local_flags(task_thread_info(p), _TLF_DOVETAIL))
+		inband_event_notify(INBAND_TASK_SIGNAL, p);
+}
+
+static inline void inband_migration_notify(struct task_struct *p, int cpu)
+{
+	if (test_ti_local_flags(task_thread_info(p), _TLF_DOVETAIL)) {
+		struct dovetail_migration_data d = {
+			.task = p,
+			.dest_cpu = cpu,
+		};
+		inband_event_notify(INBAND_TASK_MIGRATION, &d);
+	}
+}
+
+static inline void inband_exit_notify(void)
+{
+	if (test_thread_local_flags(_TLF_DOVETAIL))
+		inband_event_notify(INBAND_TASK_EXIT, NULL);
+}
+
+static inline void inband_cleanup_notify(struct mm_struct *mm)
+{
+	/*
+	 * Notify regardless of _TLF_DOVETAIL: current may have
+	 * resources to clean up although it might not be interested
+	 * in other kernel events.
+	 */
+	inband_event_notify(INBAND_PROCESS_CLEANUP, mm);
+}
+
+static inline
+void prepare_inband_switch(struct task_struct *next)
+{
+	struct task_struct *prev = current;
+
+	if (test_ti_local_flags(task_thread_info(next), _TLF_DOVETAIL)) {
+		__this_cpu_write(irq_pipeline.rqlock_owner, prev);
+		inband_event_notify(INBAND_TASK_SCHEDULE, next);
+	}
+
+	hard_local_irq_disable();
+}
+
+static inline void inband_enter_guest(struct kvm_oob_notifier *nfy)
+{
+	struct irq_pipeline_data *p = raw_cpu_ptr(&irq_pipeline);
+	p->vcpu_notify = nfy;
+	barrier();
+}
+
+static inline void inband_exit_guest(void)
+{
+	struct irq_pipeline_data *p = raw_cpu_ptr(&irq_pipeline);
+	p->vcpu_notify = NULL;
+	barrier();
+}
+
+int inband_switch_tail(void);
+
+void oob_trampoline(void);
+
+#ifdef CONFIG_KVM
+void oob_notify_kvm(void);
+#else
+static inline void oob_notify_kvm(void)
+{ }
+#endif
+
+void arch_inband_task_init(struct task_struct *p);
+
+void sync_inband_irqs(void);
+
+#define protect_inband_mm(__flags)			\
+	do {						\
+		(__flags) = hard_cond_local_irq_save();	\
+		barrier();				\
+	} while (0)					\
+
+#define unprotect_inband_mm(__flags)			\
+	do {						\
+		barrier();				\
+		hard_cond_local_irq_restore(__flags);	\
+	} while (0)					\
+
+int dovetail_start(void);
+
+void dovetail_stop(void);
+
+void dovetail_init_altsched(struct dovetail_altsched_context *p);
+
+void dovetail_start_altsched(void);
+
+void dovetail_stop_altsched(void);
+
+__must_check int dovetail_leave_inband(void);
+
+static inline void dovetail_leave_oob(void)
+{
+	clear_thread_local_flags(_TLF_OOB|_TLF_OFFSTAGE);
+	clear_thread_flag(TIF_MAYDAY);
+}
+
+void dovetail_resume_inband(void);
+
+bool dovetail_context_switch(struct dovetail_altsched_context *out,
+			struct dovetail_altsched_context *in,
+			bool leave_inband);
+
+static inline
+struct oob_thread_state *dovetail_current_state(void)
+{
+	return &current_thread_info()->oob_state;
+}
+
+static inline
+struct oob_thread_state *dovetail_task_state(struct task_struct *p)
+{
+	return &task_thread_info(p)->oob_state;
+}
+
+static inline
+struct oob_mm_state *dovetail_mm_state(void)
+{
+	if (current->flags & PF_KTHREAD)
+		return NULL;
+
+	return &current->mm->oob_state;
+}
+
+void dovetail_call_mayday(struct thread_info *ti,
+			  struct pt_regs *regs);
+
+static inline void dovetail_send_mayday(struct task_struct *castaway)
+{
+	struct thread_info *ti = task_thread_info(castaway);
+
+	if (test_ti_local_flags(ti, _TLF_DOVETAIL))
+		set_ti_thread_flag(ti, TIF_MAYDAY);
+}
+
+void install_inband_fd(unsigned int fd, struct file *file,
+		       struct files_struct *files);
+
+void uninstall_inband_fd(unsigned int fd, struct file *file,
+			 struct files_struct *files);
+
+void replace_inband_fd(unsigned int fd, struct file *file,
+		       struct files_struct *files);
+
+#else	/* !CONFIG_DOVETAIL */
+
+struct files_struct;
+
+static inline
+void inband_task_init(struct task_struct *p) { }
+
+#define oob_trap_notify(__trapnr, __regs)	 do { } while (0)
+
+static inline
+int pipeline_syscall(struct thread_info *ti,
+		     unsigned long syscall, struct pt_regs *regs)
+{
+	return 0;
+}
+
+static inline void inband_signal_notify(struct task_struct *p) { }
+
+static inline
+void inband_migration_notify(struct task_struct *p, int cpu) { }
+
+static inline void inband_exit_notify(void) { }
+
+static inline void inband_cleanup_notify(struct mm_struct *mm) { }
+
+static inline void oob_trampoline(void) { }
+
+static inline void prepare_inband_switch(struct task_struct *next) { }
+
+#define inband_enter_guest(__nfy)	do { } while (0)
+
+#define inband_exit_guest()		do { } while (0)
+
+static inline int inband_switch_tail(void)
+{
+	return 0;
+}
+
+#define protect_inband_mm(__flags)	\
+	do { (void)(__flags); } while (0)
+
+#define unprotect_inband_mm(__flags)	\
+	do { (void)(__flags); } while (0)
+
+static inline void inband_clock_was_set(void) { }
+
+static inline
+void install_inband_fd(unsigned int fd, struct file *file,
+		       struct files_struct *files) { }
+
+static inline
+void uninstall_inband_fd(unsigned int fd, struct file *file,
+			 struct files_struct *files) { }
+
+static inline
+void replace_inband_fd(unsigned int fd, struct file *file,
+		       struct files_struct *files) { }
+
+#endif	/* !CONFIG_DOVETAIL */
+
+static inline bool dovetailing(void)
+{
+	return IS_ENABLED(CONFIG_DOVETAIL);
+}
+
+static inline bool dovetail_debug(void)
+{
+	return IS_ENABLED(CONFIG_DEBUG_DOVETAIL);
+}
+
+#endif /* _LINUX_DOVETAIL_H */
diff --git a/include/linux/dw_apb_timer.h b/include/linux/dw_apb_timer.h
index 14f072edbca5..abd5a050a529 100644
--- a/include/linux/dw_apb_timer.h
+++ b/include/linux/dw_apb_timer.h
@@ -31,7 +31,7 @@ struct dw_apb_clock_event_device {
 
 struct dw_apb_clocksource {
 	struct dw_apb_timer			timer;
-	struct clocksource			cs;
+	struct clocksource_user_mmio		ummio;
 };
 
 void dw_apb_clockevent_register(struct dw_apb_clock_event_device *dw_ced);
diff --git a/include/linux/fs.h b/include/linux/fs.h
index e0d909d35763..d95ce9460f92 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -54,6 +54,7 @@ struct kiocb;
 struct kobject;
 struct pipe_inode_info;
 struct poll_table_struct;
+struct oob_poll_wait;
 struct kstatfs;
 struct vm_area_struct;
 struct vfsmount;
@@ -962,6 +963,7 @@ struct file {
 #endif
 	/* needed for tty driver, and maybe others */
 	void			*private_data;
+	void			*oob_data;
 
 #ifdef CONFIG_EPOLL
 	/* Used by fs/eventpoll.c to link all the hooks to this file */
@@ -1824,6 +1826,10 @@ struct file_operations {
 	__poll_t (*poll) (struct file *, struct poll_table_struct *);
 	long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long);
 	long (*compat_ioctl) (struct file *, unsigned int, unsigned long);
+	ssize_t (*oob_read) (struct file *, char __user *, size_t);
+	ssize_t (*oob_write) (struct file *, const char __user *, size_t);
+	long (*oob_ioctl) (struct file *, unsigned int, unsigned long);
+	__poll_t (*oob_poll) (struct file *, struct oob_poll_wait *);
 	int (*mmap) (struct file *, struct vm_area_struct *);
 	unsigned long mmap_supported_flags;
 	int (*open) (struct inode *, struct file *);
diff --git a/include/linux/hardirq.h b/include/linux/hardirq.h
index da0af631ded5..2acfcd9223db 100644
--- a/include/linux/hardirq.h
+++ b/include/linux/hardirq.h
@@ -6,6 +6,7 @@
 #include <linux/lockdep.h>
 #include <linux/ftrace_irq.h>
 #include <linux/vtime.h>
+#include <asm-generic/irq_pipeline.h>
 #include <asm/hardirq.h>
 
 
@@ -44,6 +45,7 @@ extern void rcu_nmi_exit(void);
  * Enter irq context (on NO_HZ, update jiffies):
  */
 extern void irq_enter(void);
+void irq_enter_if_inband(void);
 
 /*
  * Exit irq context without processing softirqs:
@@ -59,6 +61,7 @@ extern void irq_enter(void);
  * Exit irq context and process softirqs if needed:
  */
 extern void irq_exit(void);
+void irq_exit_if_inband(void);
 
 #ifndef arch_nmi_enter
 #define arch_nmi_enter()	do { } while (0)
@@ -67,6 +70,7 @@ extern void irq_exit(void);
 
 #define nmi_enter()						\
 	do {							\
+		irq_pipeline_nmi_enter();			\
 		arch_nmi_enter();				\
 		printk_nmi_enter();				\
 		lockdep_off();					\
@@ -87,6 +91,17 @@ extern void irq_exit(void);
 		lockdep_on();					\
 		printk_nmi_exit();				\
 		arch_nmi_exit();				\
+		irq_pipeline_nmi_exit();			\
 	} while (0)
 
+static inline bool start_irq_flow(void)
+{
+	return !irqs_pipelined() || in_pipeline();
+}
+
+static inline bool on_pipeline_entry(void)
+{
+	return irqs_pipelined() && in_pipeline();
+}
+
 #endif /* LINUX_HARDIRQ_H */
diff --git a/include/linux/intel-iommu.h b/include/linux/intel-iommu.h
index 6d8bf4bdf240..88dd46e18bf5 100644
--- a/include/linux/intel-iommu.h
+++ b/include/linux/intel-iommu.h
@@ -523,7 +523,7 @@ struct intel_iommu {
 	u64		cap;
 	u64		ecap;
 	u32		gcmd; /* Holds TE, EAFL. Don't need SRTP, SFL, WBF */
-	raw_spinlock_t	register_lock; /* protect register handling */
+	hard_spinlock_t	register_lock; /* protect register handling */
 	int		seq_id;	/* sequence id of the iommu */
 	int		agaw; /* agaw of this iommu */
 	int		msagaw; /* max sagaw of this iommu */
diff --git a/include/linux/interrupt.h b/include/linux/interrupt.h
index 89fc59dab57d..070dd60e3e4d 100644
--- a/include/linux/interrupt.h
+++ b/include/linux/interrupt.h
@@ -61,6 +61,12 @@
  *                interrupt handler after suspending interrupts. For system
  *                wakeup devices users need to implement wakeup detection in
  *                their interrupt handlers.
+ * IRQF_OOB - Interrupt is attached to an out-of-band handler living
+ *            on the heading stage of the interrupt pipeline
+ *            (CONFIG_IRQ_PIPELINE).  It may be delivered to the
+ *            handler any time interrupts are enabled in the CPU,
+ *            regardless of the (virtualized) interrupt state
+ *            maintained by local_irq_save/disable().
  */
 #define IRQF_SHARED		0x00000080
 #define IRQF_PROBE_SHARED	0x00000100
@@ -74,6 +80,7 @@
 #define IRQF_NO_THREAD		0x00010000
 #define IRQF_EARLY_RESUME	0x00020000
 #define IRQF_COND_SUSPEND	0x00040000
+#define IRQF_OOB		0x00080000
 
 #define IRQF_TIMER		(__IRQF_TIMER | IRQF_NO_SUSPEND | IRQF_NO_THREAD)
 
@@ -499,9 +506,29 @@ extern bool force_irqthreads;
  * to ensure that after a local_irq_disable(), interrupts have
  * really been disabled in hardware. Such architectures need to
  * implement the following hook.
+ *
+ * Those cases also apply when interrupt pipelining is in effect,
+ * since we are virtualizing the interrupt disable state here too.
  */
 #ifndef hard_irq_disable
-#define hard_irq_disable()	do { } while(0)
+#define hard_irq_disable()	hard_cond_local_irq_disable()
+#endif
+
+/*
+ * Unlike other virtualized interrupt disabling schemes may assume, we
+ * can't expect local_irq_restore() to turn hard interrupts on when
+ * pipelining.  hard_irq_enable() is introduced to be paired with
+ * hard_irq_disable(), for unconditionally turning them on. The only
+ * sane sequence mixing virtual and real disable state manipulation
+ * is:
+ *
+ * 1. local_irq_save/disable
+ * 2. hard_irq_disable
+ * 3. hard_irq_enable
+ * 4. local_irq_restore/enable
+ */
+#ifndef hard_irq_enable
+#define hard_irq_enable()	hard_cond_local_irq_enable()
 #endif
 
 /* PLEASE, avoid to allocate new softirqs, if you need not _really_ high
diff --git a/include/linux/irq.h b/include/linux/irq.h
index fb301cf29148..2f9d6193b45c 100644
--- a/include/linux/irq.h
+++ b/include/linux/irq.h
@@ -71,6 +71,11 @@ enum irqchip_irq_state;
  *				  it from the spurious interrupt detection
  *				  mechanism and from core side polling.
  * IRQ_DISABLE_UNLAZY		- Disable lazy irq disable
+ * IRQ_OOB                      - Interrupt can be delivered to the out-of-band handler
+ *                                when pipelining is enabled (CONFIG_IRQ_PIPELINE),
+ *                                regardless of the (virtualized) interrupt state
+ *                                maintained by local_irq_save/disable().
+ * IRQ_CHAINED                  - Interrupt is chained.
  */
 enum {
 	IRQ_TYPE_NONE		= 0x00000000,
@@ -97,13 +102,15 @@ enum {
 	IRQ_PER_CPU_DEVID	= (1 << 17),
 	IRQ_IS_POLLED		= (1 << 18),
 	IRQ_DISABLE_UNLAZY	= (1 << 19),
+	IRQ_OOB			= (1 << 19),
+	IRQ_CHAINED		= (1 << 20),
 };
 
 #define IRQF_MODIFY_MASK	\
 	(IRQ_TYPE_SENSE_MASK | IRQ_NOPROBE | IRQ_NOREQUEST | \
 	 IRQ_NOAUTOEN | IRQ_MOVE_PCNTXT | IRQ_LEVEL | IRQ_NO_BALANCING | \
 	 IRQ_PER_CPU | IRQ_NESTED_THREAD | IRQ_NOTHREAD | IRQ_PER_CPU_DEVID | \
-	 IRQ_IS_POLLED | IRQ_DISABLE_UNLAZY)
+	 IRQ_IS_POLLED | IRQ_DISABLE_UNLAZY | IRQ_OOB)
 
 #define IRQ_NO_BALANCING_MASK	(IRQ_PER_CPU | IRQ_NO_BALANCING)
 
@@ -511,6 +518,7 @@ struct irq_chip {
  * IRQCHIP_EOI_THREADED:	Chip requires eoi() on unmask in threaded mode
  * IRQCHIP_SUPPORTS_LEVEL_MSI	Chip can provide two doorbells for Level MSIs
  * IRQCHIP_SUPPORTS_NMI:	Chip can deliver NMIs, only for root irqchips
+ * IRQCHIP_PIPELINE_SAFE:	Chip can work in pipelined mode
  */
 enum {
 	IRQCHIP_SET_TYPE_MASKED		= (1 <<  0),
@@ -522,6 +530,7 @@ enum {
 	IRQCHIP_EOI_THREADED		= (1 <<  6),
 	IRQCHIP_SUPPORTS_LEVEL_MSI	= (1 <<  7),
 	IRQCHIP_SUPPORTS_NMI		= (1 <<  8),
+	IRQCHIP_PIPELINE_SAFE		= (1 <<  9),
 };
 
 #include <linux/irqdesc.h>
@@ -600,6 +609,7 @@ extern void handle_percpu_irq(struct irq_desc *desc);
 extern void handle_percpu_devid_irq(struct irq_desc *desc);
 extern void handle_bad_irq(struct irq_desc *desc);
 extern void handle_nested_irq(unsigned int irq);
+extern void handle_synthetic_irq(struct irq_desc *desc);
 
 extern void handle_fasteoi_nmi(struct irq_desc *desc);
 extern void handle_percpu_devid_fasteoi_nmi(struct irq_desc *desc);
@@ -744,7 +754,13 @@ extern int irq_set_irq_type(unsigned int irq, unsigned int type);
 extern int irq_set_msi_desc(unsigned int irq, struct msi_desc *entry);
 extern int irq_set_msi_desc_off(unsigned int irq_base, unsigned int irq_offset,
 				struct msi_desc *entry);
-extern struct irq_data *irq_get_irq_data(unsigned int irq);
+
+static inline struct irq_data *irq_get_irq_data(unsigned int irq)
+{
+	struct irq_desc *desc = irq_to_desc(irq);
+
+	return desc ? &desc->irq_data : NULL;
+}
 
 static inline struct irq_chip *irq_get_chip(unsigned int irq)
 {
@@ -987,7 +1003,7 @@ struct irq_chip_type {
  * different flow mechanisms (level/edge) for it.
  */
 struct irq_chip_generic {
-	raw_spinlock_t		lock;
+	hard_spinlock_t		lock;
 	void __iomem		*reg_base;
 	u32			(*reg_readl)(void __iomem *addr);
 	void			(*reg_writel)(u32 val, void __iomem *addr);
@@ -1114,6 +1130,12 @@ static inline struct irq_chip_type *irq_data_get_chip_type(struct irq_data *d)
 
 #define IRQ_MSK(n) (u32)((n) < 32 ? ((1 << (n)) - 1) : UINT_MAX)
 
+#ifdef CONFIG_IRQ_PIPELINE
+
+int irq_switch_oob(unsigned int irq, bool on);
+
+#endif	/* !CONFIG_IRQ_PIPELINE */
+
 #ifdef CONFIG_SMP
 static inline void irq_gc_lock(struct irq_chip_generic *gc)
 {
diff --git a/include/linux/irq_pipeline.h b/include/linux/irq_pipeline.h
new file mode 100644
index 000000000000..80ec18a0c0db
--- /dev/null
+++ b/include/linux/irq_pipeline.h
@@ -0,0 +1,135 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2002 Philippe Gerum  <rpm@xenomai.org>.
+ *               2006 Gilles Chanteperdrix.
+ *               2007 Jan Kiszka.
+ */
+#ifndef _LINUX_IRQ_PIPELINE_H
+#define _LINUX_IRQ_PIPELINE_H
+
+struct cpuidle_device;
+struct cpuidle_state;
+struct irq_desc;
+
+#ifdef CONFIG_IRQ_PIPELINE
+
+#include <linux/compiler.h>
+#include <linux/irqdomain.h>
+#include <linux/percpu.h>
+#include <linux/interrupt.h>
+#include <linux/irqstage.h>
+#include <linux/thread_info.h>
+#include <asm/irqflags.h>
+
+void irq_pipeline_init_early(void);
+
+void irq_pipeline_init(void);
+
+void arch_irq_pipeline_init(void);
+
+int irq_inject_pipeline(unsigned int irq);
+
+void synchronize_pipeline(void);
+
+static __always_inline void synchronize_pipeline_on_irq(void)
+{
+	/*
+	 * Optimize if we preempted the high priority oob stage: we
+	 * don't need to synchronize the pipeline unless there is a
+	 * pending interrupt for it.
+	 */
+	if (running_inband() ||
+	    stage_irqs_pending(this_oob_staged()))
+		synchronize_pipeline();
+}
+
+void dovetail_call_mayday(struct thread_info *ti,
+			  struct pt_regs *regs);
+
+bool handle_oob_irq(struct irq_desc *desc);
+
+void arch_do_IRQ_pipelined(struct irq_desc *desc);
+
+void irq_pipeline_clear(struct irq_desc *desc);
+
+#ifdef CONFIG_SMP
+void irq_pipeline_send_remote(unsigned int ipi,
+			      const struct cpumask *cpumask);
+#endif	/* CONFIG_SMP */
+
+void irq_pipeline_oops(void);
+
+bool irq_pipeline_steal_tick(void);
+
+bool irq_cpuidle_enter(struct cpuidle_device *dev,
+		       struct cpuidle_state *state);
+
+int run_oob_call(int (*fn)(void *arg), void *arg);
+
+extern bool irq_pipeline_active;
+
+static inline bool inband_unsafe(void)
+{
+	return running_oob() ||
+		(hard_irqs_disabled() && irq_pipeline_active);
+}
+
+static inline bool inband_irq_pending(void)
+{
+	check_hard_irqs_disabled();
+
+	return stage_irqs_pending(this_inband_staged());
+}
+
+int handle_irq_pipelined(struct pt_regs *regs);
+
+extern struct irq_domain *synthetic_irq_domain;
+
+#else /* !CONFIG_IRQ_PIPELINE */
+
+#include <linux/irqstage.h>
+#include <asm/irq_pipeline.h>
+
+static inline
+void irq_pipeline_init_early(void) { }
+
+static inline
+void irq_pipeline_init(void) { }
+
+static inline
+void irq_pipeline_clear(struct irq_desc *desc) { }
+
+static inline
+void irq_pipeline_oops(void) { }
+
+static inline bool handle_oob_irq(struct irq_desc *desc)
+{
+	return false;
+}
+
+static inline bool irq_cpuidle_enter(struct cpuidle_device *dev,
+				     struct cpuidle_state *state)
+{
+	return true;
+}
+
+static inline bool inband_unsafe(void)
+{
+	return false;
+}
+
+static inline bool inband_irq_pending(void)
+{
+	return false;
+}
+
+#endif /* !CONFIG_IRQ_PIPELINE */
+
+#if !defined(CONFIG_IRQ_PIPELINE) || !defined(CONFIG_SPARSE_IRQ)
+static inline void uncache_irq_desc(unsigned int irq) { }
+#else
+void uncache_irq_desc(unsigned int irq);
+#endif
+
+#endif /* _LINUX_IRQ_PIPELINE_H */
diff --git a/include/linux/irqdesc.h b/include/linux/irqdesc.h
index d6e2ab538ef2..3f38f784f839 100644
--- a/include/linux/irqdesc.h
+++ b/include/linux/irqdesc.h
@@ -72,7 +72,7 @@ struct irq_desc {
 	unsigned int		irqs_unhandled;
 	atomic_t		threads_handled;
 	int			threads_handled_last;
-	raw_spinlock_t		lock;
+	mutable_spinlock_t	lock;
 	struct cpumask		*percpu_enabled;
 	const struct cpumask	*percpu_affinity;
 #ifdef CONFIG_SMP
@@ -158,6 +158,8 @@ static inline void generic_handle_irq_desc(struct irq_desc *desc)
 
 int generic_handle_irq(unsigned int irq);
 
+int generic_pipeline_irq(unsigned int irq, struct pt_regs *regs);
+
 #ifdef CONFIG_HANDLE_DOMAIN_IRQ
 /*
  * Convert a HW interrupt number to a logical one using a IRQ domain,
@@ -168,11 +170,24 @@ int generic_handle_irq(unsigned int irq);
 int __handle_domain_irq(struct irq_domain *domain, unsigned int hwirq,
 			bool lookup, struct pt_regs *regs);
 
+#ifdef CONFIG_IRQ_PIPELINE
+unsigned int irq_find_mapping(struct irq_domain *host,
+			irq_hw_number_t hwirq);
+
+static inline int handle_domain_irq(struct irq_domain *domain,
+				    unsigned int hwirq, struct pt_regs *regs)
+{
+	unsigned int irq = irq_find_mapping(domain, hwirq);
+
+	return generic_pipeline_irq(irq, regs);
+}
+#else
 static inline int handle_domain_irq(struct irq_domain *domain,
 				    unsigned int hwirq, struct pt_regs *regs)
 {
 	return __handle_domain_irq(domain, hwirq, true, regs);
 }
+#endif	/* !CONFIG_IRQ_PIPELINE */
 
 #ifdef CONFIG_IRQ_DOMAIN
 int handle_domain_nmi(struct irq_domain *domain, unsigned int hwirq,
@@ -256,6 +271,14 @@ static inline bool irq_is_percpu_devid(unsigned int irq)
 	return desc->status_use_accessors & IRQ_PER_CPU_DEVID;
 }
 
+static inline int irq_is_oob(unsigned int irq)
+{
+	struct irq_desc *desc;
+
+	desc = irq_to_desc(irq);
+	return desc->status_use_accessors & IRQ_OOB;
+}
+
 static inline void
 irq_set_lockdep_class(unsigned int irq, struct lock_class_key *lock_class,
 		      struct lock_class_key *request_class)
diff --git a/include/linux/irqflags.h b/include/linux/irqflags.h
index 21619c92c377..ac79e9e04955 100644
--- a/include/linux/irqflags.h
+++ b/include/linux/irqflags.h
@@ -13,6 +13,7 @@
 #define _LINUX_TRACE_IRQFLAGS_H
 
 #include <linux/typecheck.h>
+#include <asm-generic/irq_pipeline.h>
 #include <asm/irqflags.h>
 
 /* Currently trace_softirqs_on/off is used only by lockdep */
@@ -148,6 +149,23 @@ do {						\
 
 #endif /* CONFIG_TRACE_IRQFLAGS */
 
+#ifdef CONFIG_IRQ_PIPELINE
+#define local_irq_enable_full()			\
+	do {					\
+		hard_local_irq_enable();	\
+		local_irq_enable();		\
+	} while (0)
+
+#define local_irq_disable_full()		\
+	do {					\
+		local_irq_disable();		\
+		hard_local_irq_disable();	\
+	} while (0)
+#else
+#define local_irq_enable_full()		local_irq_enable()
+#define local_irq_disable_full()	local_irq_disable()
+#endif
+
 #define local_save_flags(flags)	raw_local_save_flags(flags)
 
 /*
diff --git a/include/linux/irqstage.h b/include/linux/irqstage.h
new file mode 100644
index 000000000000..830a7cb8e8ac
--- /dev/null
+++ b/include/linux/irqstage.h
@@ -0,0 +1,394 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016, 2019 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#ifndef _LINUX_IRQSTAGE_H
+#define _LINUX_IRQSTAGE_H
+
+#ifdef CONFIG_IRQ_PIPELINE
+
+#include <linux/percpu.h>
+#include <linux/bitops.h>
+#include <linux/preempt.h>
+#include <asm/irq_pipeline.h>
+
+struct task_struct;
+struct kvm_oob_notifier;
+
+struct irq_stage {
+	int index;
+	const char *name;
+};
+
+extern struct irq_stage inband_stage;
+
+extern struct irq_stage oob_stage;
+
+/* Interrupts disabled for a stage. */
+#define STAGE_STALL_BIT  0
+
+struct irq_event_map;
+
+struct irq_log {
+	unsigned long himap;
+	struct irq_event_map *map;
+};
+
+/* Per-CPU, per-stage data. */
+struct irq_stage_data {
+	unsigned long status;
+	struct irq_log log;
+	struct irq_stage *stage;
+#ifdef CONFIG_DEBUG_IRQ_PIPELINE
+	int cpu;
+#endif
+};
+
+/* Per-CPU pipeline descriptor. */
+struct irq_pipeline_data {
+	struct irq_stage_data stages[2];
+	struct pt_regs tick_regs;
+#ifdef CONFIG_DOVETAIL
+	struct task_struct *task_inflight;
+	struct task_struct *rqlock_owner;
+	struct kvm_oob_notifier *vcpu_notify;
+#endif
+};
+
+DECLARE_PER_CPU(struct irq_pipeline_data, irq_pipeline);
+
+/**
+ * this_staged - IRQ stage data on the current CPU
+ *
+ * Return the address of @stage's data on the current CPU. IRQs must
+ * be hard disabled to prevent CPU migration.
+ */
+static inline
+struct irq_stage_data *this_staged(struct irq_stage *stage)
+{
+	return &raw_cpu_ptr(irq_pipeline.stages)[stage->index];
+}
+
+/**
+ * percpu_inband_staged - IRQ stage data on specified CPU
+ *
+ * Return the address of @stage's data on @cpu.
+ *
+ * This is the slowest accessor, use it carefully. Prefer
+ * this_staged() for requests referring to the current
+ * CPU. Additionally, if the target stage is known at build time,
+ * consider using this_{inband, oob}_staged() instead.
+ */
+static inline
+struct irq_stage_data *percpu_inband_staged(struct irq_stage *stage, int cpu)
+{
+	return &per_cpu(irq_pipeline.stages, cpu)[stage->index];
+}
+
+/**
+ * this_inband_staged - return the address of the pipeline context
+ * data for the inband stage on the current CPU. CPU migration must be
+ * disabled.
+ *
+ * This accessor is recommended when the stage we refer to is known at
+ * build time to be the inband one.
+ */
+static inline struct irq_stage_data *this_inband_staged(void)
+{
+	return raw_cpu_ptr(&irq_pipeline.stages[0]);
+}
+
+/**
+ * this_oob_staged - return the address of the pipeline context data
+ * for the registered oob stage on the current CPU. CPU migration must
+ * be disabled.
+ *
+ * This accessor is recommended when the stage we refer to is known at
+ * build time to be the registered oob stage. This address is always
+ * different from the context data of the inband stage, even in
+ * absence of registered oob stage.
+ */
+static inline struct irq_stage_data *this_oob_staged(void)
+{
+	return raw_cpu_ptr(&irq_pipeline.stages[1]);
+}
+
+static inline struct irq_stage_data *__current_irq_staged(void)
+{
+	return &raw_cpu_ptr(irq_pipeline.stages)[stage_level()];
+}
+
+/**
+ * current_irq_staged - return the address of the pipeline context
+ * data for the current stage. CPU migration must be disabled.
+ */
+#define current_irq_staged __current_irq_staged()
+
+static inline
+void check_staged_locality(struct irq_stage_data *pd)
+{
+#ifdef CONFIG_DEBUG_IRQ_PIPELINE
+	/*
+	 * Setting our context with another processor's is a really
+	 * bad idea, our caller definitely went loopy.
+	 */
+	WARN_ON_ONCE(raw_smp_processor_id() != pd->cpu);
+#endif
+}
+
+/**
+ * switch_oob(), switch_inband() - switch the current CPU to the
+ * specified stage context. CPU migration must be disabled.
+ *
+ * Calling these routines is the only sane and safe way to change the
+ * interrupt stage for the current CPU. Don't bypass them, ever.
+ * Really.
+ */
+static inline
+void switch_oob(struct irq_stage_data *pd)
+{
+	check_staged_locality(pd);
+	if (!(preempt_count() & STAGE_MASK))
+		preempt_count_add(STAGE_OFFSET);
+}
+
+static inline
+void switch_inband(struct irq_stage_data *pd)
+{
+	check_staged_locality(pd);
+	if (preempt_count() & STAGE_MASK)
+		preempt_count_sub(STAGE_OFFSET);
+}
+
+static inline
+void set_current_irq_staged(struct irq_stage_data *pd)
+{
+	if (pd->stage == &inband_stage)
+		switch_inband(pd);
+	else
+		switch_oob(pd);
+}
+
+static inline struct irq_stage *__current_irq_stage(void)
+{
+	/*
+	 * We don't have to hard disable irqs while accessing the
+	 * per-CPU stage data here, because there is no way we could
+	 * switch stage and CPU at the same time.
+	 */
+	return __current_irq_staged()->stage;
+}
+
+#define current_irq_stage	__current_irq_stage()
+
+static inline bool oob_stage_present(void)
+{
+	return oob_stage.index != 0;
+}
+
+/**
+ * stage_irqs_pending() - Whether we have interrupts pending
+ * (i.e. logged) on the current CPU for the given stage. Hard IRQs
+ * must be disabled.
+ */
+static inline int stage_irqs_pending(struct irq_stage_data *pd)
+{
+	return pd->log.himap != 0;
+}
+
+void sync_current_irq_stage(void);
+
+void sync_irq_stage(struct irq_stage *top);
+
+void irq_post_stage(struct irq_stage *stage,
+		    unsigned int irq);
+
+#ifdef CONFIG_DEBUG_IRQ_PIPELINE
+
+#define __check_stage_bit_access(__pd)			\
+	({						\
+		check_hard_irqs_disabled_in_smp();	\
+		(__pd)->cpu != raw_smp_processor_id();	\
+	})
+
+#define check_stage_bit_access(__op, __bit, __pd)			\
+	do {								\
+		if (__check_stage_bit_access(__pd))			\
+			trace_printk("REMOTE %s(%s) to %s/%d\n",	\
+			     __op, __bit,  __pd->stage->name, __pd->cpu); \
+	} while (0)
+
+#define set_stage_bit(__bit, __pd)					\
+	do {								\
+		__set_bit(__bit, &(__pd)->status);			\
+		check_stage_bit_access("set", # __bit, __pd);		\
+	} while (0)
+
+#define clear_stage_bit(__bit, __pd)					\
+	do {								\
+		__clear_bit(__bit, &(__pd)->status);			\
+		check_stage_bit_access("clear", # __bit, __pd);		\
+	} while (0)
+
+#define test_and_set_stage_bit(__bit, __pd)				\
+	({								\
+		int __ret;						\
+		__ret = __test_and_set_bit(__bit, &(__pd)->status);	\
+		check_stage_bit_access("test_and_set", # __bit, __pd);	\
+		__ret;							\
+	})
+
+#define __test_stage_bit(__bit, __pd)					\
+	test_bit(__bit, &(__pd)->status)
+
+#define test_stage_bit(__bit, __pd)					\
+	({								\
+		int __ret;						\
+		__ret = __test_stage_bit(__bit,  __pd);			\
+		check_stage_bit_access("test", # __bit, __pd);		\
+		__ret;							\
+	})
+
+#else
+
+static inline
+void set_stage_bit(int bit, struct irq_stage_data *pd)
+{
+	__set_bit(bit, &pd->status);
+}
+
+static inline
+void clear_stage_bit(int bit, struct irq_stage_data *pd)
+{
+	__clear_bit(bit, &pd->status);
+}
+
+static inline
+int test_and_set_stage_bit(int bit, struct irq_stage_data *pd)
+{
+	return __test_and_set_bit(bit, &pd->status);
+}
+
+static inline
+int __test_stage_bit(int bit, struct irq_stage_data *pd)
+{
+	return test_bit(bit, &pd->status);
+}
+
+static inline
+int test_stage_bit(int bit, struct irq_stage_data *pd)
+{
+	return __test_stage_bit(bit, pd);
+}
+
+#endif /* !CONFIG_DEBUG_IRQ_PIPELINE */
+
+static inline void irq_post_oob(unsigned int irq)
+{
+	irq_post_stage(&oob_stage, irq);
+}
+
+static inline void irq_post_inband(unsigned int irq)
+{
+	irq_post_stage(&inband_stage, irq);
+}
+
+static inline void oob_irq_disable(void)
+{
+	hard_local_irq_disable();
+	set_stage_bit(STAGE_STALL_BIT, this_oob_staged());
+}
+
+static inline unsigned long oob_irq_save(void)
+{
+	hard_local_irq_disable();
+
+	return test_and_set_stage_bit(STAGE_STALL_BIT, this_oob_staged());
+}
+
+static inline unsigned long oob_irqs_disabled(void)
+{
+	unsigned long flags, ret;
+
+	/*
+	 * Here we __must__ guard against CPU migration because we may
+	 * be reading the oob stage data from the inband stage. In
+	 * such a case, the oob stage on the destination CPU might be
+	 * in a different (stalled) state than the oob stage is on the
+	 * source one.
+	 */
+	flags = hard_smp_local_irq_save();
+	ret = test_stage_bit(STAGE_STALL_BIT, this_oob_staged());
+	hard_smp_local_irq_restore(flags);
+
+	return ret;
+}
+
+void oob_irq_enable(void);
+
+void __oob_irq_restore(unsigned long x);
+
+static inline void oob_irq_restore(unsigned long x)
+{
+	if ((x ^ test_stage_bit(STAGE_STALL_BIT, this_oob_staged())) & 1)
+		__oob_irq_restore(x);
+}
+
+bool stage_disabled(void);
+
+unsigned long test_and_disable_stage(int *irqsoff);
+
+static inline unsigned long disable_stage(void)
+{
+	return test_and_disable_stage(NULL);
+}
+
+void restore_stage(unsigned long combo);
+
+#define stage_save_flags(__combo)					\
+	do {								\
+		(__combo) = irqs_merge_flags(hard_local_save_flags(),	\
+					     irqs_disabled());		\
+	} while (0)
+
+int enable_oob_stage(const char *name);
+
+int arch_enable_oob_stage(void);
+
+void disable_oob_stage(void);
+
+#else /* !CONFIG_IRQ_PIPELINE */
+
+static inline bool oob_stage_present(void)
+{
+	return false;
+}
+
+static inline bool stage_disabled(void)
+{
+	return irqs_disabled();
+}
+
+#define test_and_disable_stage(__irqsoff)			\
+	({							\
+		unsigned long __flags;				\
+		raw_local_irq_save(__flags);			\
+		*(__irqsoff) = irqs_disabled_flags(__flags);	\
+		__flags;					\
+	})
+
+#define disable_stage()				\
+	({					\
+		unsigned long __flags;		\
+		raw_local_irq_save(__flags);	\
+		__flags;			\
+	})
+
+#define restore_stage(__flags)	raw_local_irq_restore(__flags)
+
+#define stage_save_flags(__flags)	raw_local_save_flags(__flags)
+
+#endif /* !CONFIG_IRQ_PIPELINE */
+
+#endif	/* !_LINUX_IRQSTAGE_H */
diff --git a/include/linux/kernel.h b/include/linux/kernel.h
index d83d403dac2e..449b9101d64d 100644
--- a/include/linux/kernel.h
+++ b/include/linux/kernel.h
@@ -14,6 +14,7 @@
 #include <linux/typecheck.h>
 #include <linux/printk.h>
 #include <linux/build_bug.h>
+#include <asm-generic/irq_pipeline.h>
 #include <asm/byteorder.h>
 #include <asm/div64.h>
 #include <uapi/linux/kernel.h>
@@ -203,9 +204,12 @@ struct user;
 
 #ifdef CONFIG_PREEMPT_VOLUNTARY
 extern int _cond_resched(void);
-# define might_resched() _cond_resched()
+# define might_resched() do { \
+		check_inband_stage(); \
+		_cond_resched(); \
+	} while (0)
 #else
-# define might_resched() do { } while (0)
+# define might_resched() check_inband_stage()
 #endif
 
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h
index d41c521a39da..a62daedd1f62 100644
--- a/include/linux/kvm_host.h
+++ b/include/linux/kvm_host.h
@@ -260,10 +260,22 @@ struct kvm_mmio_fragment {
 	unsigned len;
 };
 
+/*
+ * Called when the host is about to leave the inband stage. Typically
+ * used for switching the current vcpu out of guest mode before a
+ * co-kernel reinstates an oob task context.
+ */
+struct kvm_oob_notifier {
+	void (*handler)(struct kvm_oob_notifier *nfy);
+};
+
 struct kvm_vcpu {
 	struct kvm *kvm;
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	struct preempt_notifier preempt_notifier;
+#endif
+#ifdef CONFIG_DOVETAIL
+	struct kvm_oob_notifier oob_notifier;
 #endif
 	int cpu;
 	int vcpu_id;
diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h
index b8a835fd611b..dd2f3a0d6222 100644
--- a/include/linux/lockdep.h
+++ b/include/linux/lockdep.h
@@ -309,21 +309,22 @@ extern void lockdep_init_map(struct lockdep_map *lock, const char *name,
  * or they are too narrow (they suffer from a false class-split):
  */
 #define lockdep_set_class(lock, key) \
-		lockdep_init_map(&(lock)->dep_map, #key, key, 0)
+	lockdep_init_map(LOCKDEP_ALT_DEPMAP(lock), #key, key, 0)
 #define lockdep_set_class_and_name(lock, key, name) \
-		lockdep_init_map(&(lock)->dep_map, name, key, 0)
+	lockdep_init_map(LOCKDEP_ALT_DEPMAP(lock), name, key, 0)
 #define lockdep_set_class_and_subclass(lock, key, sub) \
-		lockdep_init_map(&(lock)->dep_map, #key, key, sub)
+	lockdep_init_map(LOCKDEP_ALT_DEPMAP(lock), #key, key, sub)
 #define lockdep_set_subclass(lock, sub)	\
-		lockdep_init_map(&(lock)->dep_map, #lock, \
-				 (lock)->dep_map.key, sub)
+	lockdep_init_map(LOCKDEP_ALT_DEPMAP(lock), #lock,	\
+			 LOCKDEP_ALT_DEPMAP(lock)->key, sub)
 
 #define lockdep_set_novalidate_class(lock) \
 	lockdep_set_class_and_name(lock, &__lockdep_no_validate__, #lock)
 /*
  * Compare locking classes
  */
-#define lockdep_match_class(lock, key) lockdep_match_key(&(lock)->dep_map, key)
+#define lockdep_match_class(lock, key) \
+	lockdep_match_key(LOCKDEP_ALT_DEPMAP(lock), key)
 
 static inline int lockdep_match_key(struct lockdep_map *lock,
 				    struct lock_class_key *key)
@@ -362,8 +363,8 @@ static inline int lock_is_held(const struct lockdep_map *lock)
 	return lock_is_held_type(lock, -1);
 }
 
-#define lockdep_is_held(lock)		lock_is_held(&(lock)->dep_map)
-#define lockdep_is_held_type(lock, r)	lock_is_held_type(&(lock)->dep_map, (r))
+#define lockdep_is_held(lock)		lock_is_held(LOCKDEP_ALT_DEPMAP(lock))
+#define lockdep_is_held_type(lock, r)	lock_is_held_type(LOCKDEP_ALT_DEPMAP(lock), (r))
 
 extern void lock_set_class(struct lockdep_map *lock, const char *name,
 			   struct lock_class_key *key, unsigned int subclass,
@@ -388,26 +389,27 @@ extern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);
 #define lockdep_depth(tsk)	(debug_locks ? (tsk)->lockdep_depth : 0)
 
 #define lockdep_assert_held(l)	do {				\
-		WARN_ON(debug_locks && !lockdep_is_held(l));	\
+		WARN_ON(debug_locks && !LOCKDEP_HARD_DEBUG_RET(l, 1, lockdep_is_held(l))); \
 	} while (0)
 
 #define lockdep_assert_held_write(l)	do {			\
-		WARN_ON(debug_locks && !lockdep_is_held_type(l, 0));	\
+		WARN_ON(debug_locks && !LOCKDEP_HARD_DEBUG_RET(l, 1, lockdep_is_held_type(l, 0))); \
 	} while (0)
 
 #define lockdep_assert_held_read(l)	do {				\
-		WARN_ON(debug_locks && !lockdep_is_held_type(l, 1));	\
+		WARN_ON(debug_locks && !LOCKDEP_HARD_DEBUG_RET(l, 1, lockdep_is_held_type(l, 1))); \
 	} while (0)
 
 #define lockdep_assert_held_once(l)	do {				\
-		WARN_ON_ONCE(debug_locks && !lockdep_is_held(l));	\
+		WARN_ON_ONCE(debug_locks && !LOCKDEP_HARD_DEBUG_RET(l, 1, lockdep_is_held(l))); \
 	} while (0)
 
 #define lockdep_recursing(tsk)	((tsk)->lockdep_recursion)
 
-#define lockdep_pin_lock(l)	lock_pin_lock(&(l)->dep_map)
-#define lockdep_repin_lock(l,c)	lock_repin_lock(&(l)->dep_map, (c))
-#define lockdep_unpin_lock(l,c)	lock_unpin_lock(&(l)->dep_map, (c))
+#define lockdep_pin_lock(l)	LOCKDEP_HARD_DEBUG_RET(l, ({ struct pin_cookie cookie; cookie;} ), \
+							lock_pin_lock(LOCKDEP_ALT_DEPMAP(l)))
+#define lockdep_repin_lock(l,c)	LOCKDEP_HARD_DEBUG(l,, lock_repin_lock(LOCKDEP_ALT_DEPMAP(l), (c)))
+#define lockdep_unpin_lock(l,c)	LOCKDEP_HARD_DEBUG(l,, lock_unpin_lock(LOCKDEP_ALT_DEPMAP(l), (c)))
 
 #else /* !CONFIG_LOCKDEP */
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index a2adf95b3f9c..61decade042c 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -19,6 +19,7 @@
 #include <linux/pfn.h>
 #include <linux/percpu-refcount.h>
 #include <linux/bit_spinlock.h>
+#include <linux/dovetail.h>
 #include <linux/shrinker.h>
 #include <linux/resource.h>
 #include <linux/page_ext.h>
@@ -2873,5 +2874,16 @@ static inline int pages_identical(struct page *page1, struct page *page2)
 	return !memcmp_pages(page1, page2);
 }
 
+#ifdef CONFIG_DOVETAIL
+int commit_vma(struct mm_struct *mm, struct vm_area_struct *vma);
+int force_commit_memory(void);
+#else
+static inline
+int commit_vma(struct mm_struct *mm, struct vm_area_struct *vma)
+{
+	return 0;
+}
+#endif
+
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MM_H */
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 270aa8fd2800..fe15c1fabcfe 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -14,6 +14,7 @@
 #include <linux/uprobes.h>
 #include <linux/page-flags-layout.h>
 #include <linux/workqueue.h>
+#include <dovetail/mm_info.h>
 
 #include <asm/mmu.h>
 
@@ -522,6 +523,9 @@ struct mm_struct {
 		struct uprobes_state uprobes_state;
 #ifdef CONFIG_HUGETLB_PAGE
 		atomic_long_t hugetlb_usage;
+#endif
+#ifdef CONFIG_DOVETAIL
+		struct oob_mm_state oob_state;
 #endif
 		struct work_struct async_put_work;
 	} __randomize_layout;
diff --git a/include/linux/poll.h b/include/linux/poll.h
index 1cdc32b1f1b0..09fd27887fd5 100644
--- a/include/linux/poll.h
+++ b/include/linux/poll.h
@@ -110,6 +110,13 @@ struct poll_wqueues {
 	struct poll_table_entry inline_entries[N_INLINE_POLL_ENTRIES];
 };
 
+/*
+ * Generic poll operation descriptor for f_op->oob_poll.
+ */
+struct oob_poll_wait {
+	struct list_head next;
+};
+
 extern void poll_initwait(struct poll_wqueues *pwq);
 extern void poll_freewait(struct poll_wqueues *pwq);
 extern u64 select_estimate_accuracy(struct timespec64 *tv);
diff --git a/include/linux/preempt.h b/include/linux/preempt.h
index bbb68dba37cc..febc7f6fd9f8 100644
--- a/include/linux/preempt.h
+++ b/include/linux/preempt.h
@@ -27,17 +27,23 @@
  *         SOFTIRQ_MASK:	0x0000ff00
  *         HARDIRQ_MASK:	0x000f0000
  *             NMI_MASK:	0x00100000
+ *         PIPELINE_MASK:	0x00200000
+ *         STAGE_MASK:		0x00400000
  * PREEMPT_NEED_RESCHED:	0x80000000
  */
 #define PREEMPT_BITS	8
 #define SOFTIRQ_BITS	8
 #define HARDIRQ_BITS	4
 #define NMI_BITS	1
+#define PIPELINE_BITS	1
+#define STAGE_BITS	1
 
 #define PREEMPT_SHIFT	0
 #define SOFTIRQ_SHIFT	(PREEMPT_SHIFT + PREEMPT_BITS)
 #define HARDIRQ_SHIFT	(SOFTIRQ_SHIFT + SOFTIRQ_BITS)
 #define NMI_SHIFT	(HARDIRQ_SHIFT + HARDIRQ_BITS)
+#define PIPELINE_SHIFT	(NMI_SHIFT + NMI_BITS)
+#define STAGE_SHIFT	(PIPELINE_SHIFT + PIPELINE_BITS)
 
 #define __IRQ_MASK(x)	((1UL << (x))-1)
 
@@ -45,11 +51,15 @@
 #define SOFTIRQ_MASK	(__IRQ_MASK(SOFTIRQ_BITS) << SOFTIRQ_SHIFT)
 #define HARDIRQ_MASK	(__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)
 #define NMI_MASK	(__IRQ_MASK(NMI_BITS)     << NMI_SHIFT)
+#define PIPELINE_MASK	(__IRQ_MASK(PIPELINE_BITS) << PIPELINE_SHIFT)
+#define STAGE_MASK	(__IRQ_MASK(STAGE_BITS) << STAGE_SHIFT)
 
 #define PREEMPT_OFFSET	(1UL << PREEMPT_SHIFT)
 #define SOFTIRQ_OFFSET	(1UL << SOFTIRQ_SHIFT)
 #define HARDIRQ_OFFSET	(1UL << HARDIRQ_SHIFT)
 #define NMI_OFFSET	(1UL << NMI_SHIFT)
+#define PIPELINE_OFFSET	(1UL << PIPELINE_SHIFT)
+#define STAGE_OFFSET	(1UL << STAGE_SHIFT)
 
 #define SOFTIRQ_DISABLE_OFFSET	(2 * SOFTIRQ_OFFSET)
 
@@ -82,6 +92,9 @@
 #define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK \
 				 | NMI_MASK))
 
+/* The current IRQ stage level: 0=inband, 1=oob */
+#define stage_level()	((preempt_count() & STAGE_MASK) >> STAGE_SHIFT)
+
 /*
  * Are we doing bottom half or hardware interrupt processing?
  *
@@ -91,6 +104,7 @@
  * in_serving_softirq() - We're in softirq context
  * in_nmi()       - We're in NMI context
  * in_task()	  - We're in task context
+ * in_pipeline()  - We're on pipeline entry
  *
  * Note: due to the BH disabled confusion: in_softirq(),in_interrupt() really
  *       should not be used in new code.
@@ -102,6 +116,7 @@
 #define in_nmi()		(preempt_count() & NMI_MASK)
 #define in_task()		(!(preempt_count() & \
 				   (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET)))
+#define in_pipeline()		(preempt_count() & PIPELINE_MASK)
 
 /*
  * The preempt_count offset after preempt_disable();
@@ -180,7 +195,8 @@ do { \
 
 #define preempt_enable_no_resched() sched_preempt_enable_no_resched()
 
-#define preemptible()	(preempt_count() == 0 && !irqs_disabled())
+#define preemptible()	(preempt_count() == 0 && \
+			 !hard_irqs_disabled() && !irqs_disabled())
 
 #ifdef CONFIG_PREEMPTION
 #define preempt_enable() \
@@ -322,4 +338,43 @@ static inline void preempt_notifier_init(struct preempt_notifier *notifier,
 
 #endif
 
+#ifdef CONFIG_IRQ_PIPELINE
+
+static inline bool running_inband(void)
+{
+	return stage_level() == 0;
+}
+
+static inline bool running_oob(void)
+{
+	return !running_inband();
+}
+
+unsigned long hard_preempt_disable(void);
+void hard_preempt_enable(unsigned long flags);
+
+#else
+
+static inline bool running_inband(void)
+{
+	return true;
+}
+
+static inline bool running_oob(void)
+{
+	return false;
+}
+
+#define hard_preempt_disable()		\
+({					\
+	preempt_disable();		\
+	0;				\
+})
+#define hard_preempt_enable(__flags)	\
+	do {				\
+		preempt_enable();	\
+		(void)(__flags);	\
+	} while (0)
+#endif
+
 #endif /* __LINUX_PREEMPT_H */
diff --git a/include/linux/printk.h b/include/linux/printk.h
index c09d67edda3a..686516af0d78 100644
--- a/include/linux/printk.h
+++ b/include/linux/printk.h
@@ -158,6 +158,20 @@ static inline void printk_nmi_direct_enter(void) { }
 static inline void printk_nmi_direct_exit(void) { }
 #endif /* PRINTK_NMI */
 
+#ifdef CONFIG_RAW_PRINTK
+void raw_puts(const char *s, size_t len);
+void raw_vprintk(const char *fmt, va_list ap);
+asmlinkage __printf(1, 2)
+void raw_printk(const char *fmt, ...);
+#else
+static inline __cold
+void raw_puts(const char *s, size_t len) { }
+static inline __cold
+void raw_vprintk(const char *s, va_list ap) { }
+static inline __printf(1, 2) __cold
+void raw_printk(const char *s, ...) { }
+#endif
+
 #ifdef CONFIG_PRINTK
 asmlinkage __printf(5, 0)
 int vprintk_emit(int facility, int level,
diff --git a/include/linux/rcupdate.h b/include/linux/rcupdate.h
index 75a2eded7aa2..4b91f72f3630 100644
--- a/include/linux/rcupdate.h
+++ b/include/linux/rcupdate.h
@@ -240,7 +240,7 @@ static inline int rcu_read_lock_bh_held(void)
 
 static inline int rcu_read_lock_sched_held(void)
 {
-	return !preemptible();
+	return !running_inband() || !preemptible();
 }
 
 static inline int rcu_read_lock_any_held(void)
diff --git a/include/linux/regmap.h b/include/linux/regmap.h
index dfe493ac692d..3a35bf1a12e4 100644
--- a/include/linux/regmap.h
+++ b/include/linux/regmap.h
@@ -373,6 +373,7 @@ struct regmap_config {
 	int (*reg_write)(void *context, unsigned int reg, unsigned int val);
 
 	bool fast_io;
+	bool oob_io;
 
 	unsigned int max_register;
 	const struct regmap_access_table *wr_table;
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 67a1d86981a9..915c0a0895fe 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -23,6 +23,7 @@
 #include <linux/rcupdate.h>
 #include <linux/refcount.h>
 #include <linux/resource.h>
+#include <linux/irqstage.h>
 #include <linux/latencytop.h>
 #include <linux/sched/prio.h>
 #include <linux/sched/types.h>
@@ -117,6 +118,12 @@ struct task_group;
 					 (task->flags & PF_FROZEN) == 0 && \
 					 (task->state & TASK_NOLOAD) == 0)
 
+#ifdef CONFIG_DOVETAIL
+#define task_is_off_stage(task)		test_ti_local_flags(task_thread_info(task), _TLF_OFFSTAGE)
+#else
+#define task_is_off_stage(task)		0
+#endif
+
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 
 /*
diff --git a/include/linux/sched/coredump.h b/include/linux/sched/coredump.h
index ecdc6542070f..cefa20991d06 100644
--- a/include/linux/sched/coredump.h
+++ b/include/linux/sched/coredump.h
@@ -73,6 +73,7 @@ static inline int get_dumpable(struct mm_struct *mm)
 #define MMF_OOM_VICTIM		25	/* mm is the oom victim */
 #define MMF_OOM_REAP_QUEUED	26	/* mm was queued for oom_reaper */
 #define MMF_DISABLE_THP_MASK	(1 << MMF_DISABLE_THP)
+#define MMF_VM_PINNED		31	/* disable ondemand memory */
 
 #define MMF_INIT_MASK		(MMF_DUMPABLE_MASK | MMF_DUMP_FILTER_MASK |\
 				 MMF_DISABLE_THP_MASK)
diff --git a/include/linux/smp.h b/include/linux/smp.h
index 6fc856c9eda5..fb5ce68614d6 100644
--- a/include/linux/smp.h
+++ b/include/linux/smp.h
@@ -222,6 +222,21 @@ static inline int get_boot_cpu_id(void)
 #define get_cpu()		({ preempt_disable(); __smp_processor_id(); })
 #define put_cpu()		preempt_enable()
 
+#ifdef CONFIG_IRQ_PIPELINE
+#define hard_get_cpu(flags)	({			\
+		(flags) = hard_preempt_disable();	\
+		raw_smp_processor_id();			\
+	})
+#define hard_put_cpu(flags)	hard_preempt_enable(flags)
+#else
+#define hard_get_cpu(flags)	({ (void)(flags); get_cpu(); })
+#define hard_put_cpu(flags)	\
+	do {			\
+		(void)(flags);	\
+		put_cpu();	\
+	} while (0)
+#endif
+
 /*
  * Callback to arch code if there's nosmp or maxcpus=0 on the
  * boot command line:
diff --git a/include/linux/spinlock.h b/include/linux/spinlock.h
index 031ce8617df8..459f21773192 100644
--- a/include/linux/spinlock.h
+++ b/include/linux/spinlock.h
@@ -95,21 +95,27 @@
   extern void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,
 				   struct lock_class_key *key);
 # define raw_spin_lock_init(lock)				\
+	LOCK_ALTERNATIVES(lock,	spin_lock_init,			\
 do {								\
 	static struct lock_class_key __key;			\
 								\
-	__raw_spin_lock_init((lock), #lock, &__key);		\
-} while (0)
+	__raw_spin_lock_init(__RAWLOCK(lock), #lock, &__key);	\
+} while (0))
 
 #else
 # define raw_spin_lock_init(lock)				\
-	do { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)
+	LOCK_ALTERNATIVES(lock,	spin_lock_init,			\
+	do { *(__RAWLOCK(lock)) = __RAW_SPIN_LOCK_UNLOCKED(__RAWLOCK(lock)); } while (0))
 #endif
 
-#define raw_spin_is_locked(lock)	arch_spin_is_locked(&(lock)->raw_lock)
+#define raw_spin_is_locked(lock)		\
+	LOCK_ALTERNATIVES_RET(lock, spin_is_locked,	\
+	      arch_spin_is_locked(&(__RAWLOCK(lock))->raw_lock))
 
 #ifdef arch_spin_is_contended
-#define raw_spin_is_contended(lock)	arch_spin_is_contended(&(lock)->raw_lock)
+#define raw_spin_is_contended(lock)			\
+	LOCK_ALTERNATIVES_RET(lock, spin_is_contended,	\
+	      arch_spin_is_contended(&(__RAWLOCK(lock))->raw_lock))
 #else
 #define raw_spin_is_contended(lock)	(((void)(lock), 0))
 #endif /*arch_spin_is_contended*/
@@ -218,13 +224,19 @@ static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
  * various methods are defined as nops in the case they are not
  * required.
  */
-#define raw_spin_trylock(lock)	__cond_lock(lock, _raw_spin_trylock(lock))
+#define raw_spin_trylock(lock)			\
+	__cond_lock(lock,			\
+		    LOCK_ALTERNATIVES_RET(lock,	\
+		    spin_trylock, _raw_spin_trylock(__RAWLOCK(lock))))
 
-#define raw_spin_lock(lock)	_raw_spin_lock(lock)
+#define raw_spin_lock(lock)	\
+	LOCK_ALTERNATIVES(lock, spin_lock, _raw_spin_lock(__RAWLOCK(lock)))
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
+
 # define raw_spin_lock_nested(lock, subclass) \
-	_raw_spin_lock_nested(lock, subclass)
+	LOCK_ALTERNATIVES(lock, spin_lock_nested, \
+		_raw_spin_lock_nested(__RAWLOCK(lock), subclass), subclass)
 
 # define raw_spin_lock_nest_lock(lock, nest_lock)			\
 	 do {								\
@@ -237,18 +249,20 @@ static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
  * warns about set-but-not-used variables when building with
  * CONFIG_DEBUG_LOCK_ALLOC=n and with W=1.
  */
-# define raw_spin_lock_nested(lock, subclass)		\
-	_raw_spin_lock(((void)(subclass), (lock)))
+# define raw_spin_lock_nested(lock, subclass)	\
+	LOCK_ALTERNATIVES(lock, spin_lock_nested, \
+		_raw_spin_lock(((void)(subclass), __RAWLOCK(lock))), subclass)
 # define raw_spin_lock_nest_lock(lock, nest_lock)	_raw_spin_lock(lock)
 #endif
 
 #if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
 
-#define raw_spin_lock_irqsave(lock, flags)			\
-	do {						\
-		typecheck(unsigned long, flags);	\
-		flags = _raw_spin_lock_irqsave(lock);	\
-	} while (0)
+#define raw_spin_lock_irqsave(lock, flags)				\
+	LOCK_ALTERNATIVES(lock, spin_lock_irqsave,			\
+	do {								\
+		typecheck(unsigned long, flags);			\
+		flags = _raw_spin_lock_irqsave(__RAWLOCK(lock));	\
+	} while (0), flags)
 
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 #define raw_spin_lock_irqsave_nested(lock, flags, subclass)		\
@@ -266,45 +280,55 @@ static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
 
 #else
 
-#define raw_spin_lock_irqsave(lock, flags)		\
-	do {						\
-		typecheck(unsigned long, flags);	\
-		_raw_spin_lock_irqsave(lock, flags);	\
-	} while (0)
+#define raw_spin_lock_irqsave(lock, flags)			\
+	LOCK_ALTERNATIVES(lock, spin_lock_irqsave,		\
+	do {							\
+		typecheck(unsigned long, flags);		\
+		_raw_spin_lock_irqsave(__RAWLOCK(lock), flags);	\
+	} while (0), flags)
 
 #define raw_spin_lock_irqsave_nested(lock, flags, subclass)	\
 	raw_spin_lock_irqsave(lock, flags)
 
 #endif
 
-#define raw_spin_lock_irq(lock)		_raw_spin_lock_irq(lock)
+#define raw_spin_lock_irq(lock)		       \
+	LOCK_ALTERNATIVES(lock, spin_lock_irq, \
+			  _raw_spin_lock_irq(__RAWLOCK(lock)))
 #define raw_spin_lock_bh(lock)		_raw_spin_lock_bh(lock)
-#define raw_spin_unlock(lock)		_raw_spin_unlock(lock)
-#define raw_spin_unlock_irq(lock)	_raw_spin_unlock_irq(lock)
-
-#define raw_spin_unlock_irqrestore(lock, flags)		\
-	do {							\
-		typecheck(unsigned long, flags);		\
-		_raw_spin_unlock_irqrestore(lock, flags);	\
-	} while (0)
+#define raw_spin_unlock(lock)		     \
+	LOCK_ALTERNATIVES(lock, spin_unlock, \
+			  _raw_spin_unlock(__RAWLOCK(lock)))
+#define raw_spin_unlock_irq(lock)	\
+	LOCK_ALTERNATIVES(lock, spin_unlock_irq, \
+			  _raw_spin_unlock_irq(__RAWLOCK(lock)))
+
+#define raw_spin_unlock_irqrestore(lock, flags)				\
+	LOCK_ALTERNATIVES(lock, spin_unlock_irqrestore,			\
+	do {								\
+		typecheck(unsigned long, flags);			\
+		_raw_spin_unlock_irqrestore(__RAWLOCK(lock), flags);	\
+	} while (0), flags)
 #define raw_spin_unlock_bh(lock)	_raw_spin_unlock_bh(lock)
 
 #define raw_spin_trylock_bh(lock) \
 	__cond_lock(lock, _raw_spin_trylock_bh(lock))
 
 #define raw_spin_trylock_irq(lock) \
+	LOCK_ALTERNATIVES_RET(lock, spin_trylock_irq, \
 ({ \
 	local_irq_disable(); \
-	raw_spin_trylock(lock) ? \
+	raw_spin_trylock(__RAWLOCK(lock)) ?	\
 	1 : ({ local_irq_enable(); 0;  }); \
-})
+}))
 
 #define raw_spin_trylock_irqsave(lock, flags) \
+	LOCK_ALTERNATIVES_RET(lock, spin_trylock_irqsave, \
 ({ \
 	local_irq_save(flags); \
-	raw_spin_trylock(lock) ? \
+	raw_spin_trylock(__RAWLOCK(lock)) ?	\
 	1 : ({ local_irq_restore(flags); 0; }); \
-})
+}), flags)
 
 /* Include rwlock functions */
 #include <linux/rwlock.h>
@@ -318,6 +342,11 @@ static inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)
 # include <linux/spinlock_api_up.h>
 #endif
 
+/* Pull the lock types specific to the IRQ pipeline. */
+#ifdef CONFIG_IRQ_PIPELINE
+#include <linux/spinlock_pipeline.h>
+#endif
+
 /*
  * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
  */
diff --git a/include/linux/spinlock_api_up.h b/include/linux/spinlock_api_up.h
index d0d188861ad6..6895779e81bd 100644
--- a/include/linux/spinlock_api_up.h
+++ b/include/linux/spinlock_api_up.h
@@ -30,21 +30,33 @@
 #define __LOCK(lock) \
   do { preempt_disable(); ___LOCK(lock); } while (0)
 
+#define __HARD_LOCK(lock) \
+  do { ___LOCK(lock); } while (0)
+
 #define __LOCK_BH(lock) \
   do { __local_bh_disable_ip(_THIS_IP_, SOFTIRQ_LOCK_OFFSET); ___LOCK(lock); } while (0)
 
 #define __LOCK_IRQ(lock) \
   do { local_irq_disable(); __LOCK(lock); } while (0)
 
+#define __HARD_LOCK_IRQ(lock) \
+  do { hard_local_irq_disable(); __HARD_LOCK(lock); } while (0)
+
 #define __LOCK_IRQSAVE(lock, flags) \
   do { local_irq_save(flags); __LOCK(lock); } while (0)
 
+#define __HARD_LOCK_IRQSAVE(lock, flags) \
+  do { flags = hard_local_irq_save(); __HARD_LOCK(lock); } while (0)
+
 #define ___UNLOCK(lock) \
   do { __release(lock); (void)(lock); } while (0)
 
 #define __UNLOCK(lock) \
   do { preempt_enable(); ___UNLOCK(lock); } while (0)
 
+#define __HARD_UNLOCK(lock) \
+  do { ___UNLOCK(lock); } while (0)
+
 #define __UNLOCK_BH(lock) \
   do { __local_bh_enable_ip(_THIS_IP_, SOFTIRQ_LOCK_OFFSET); \
        ___UNLOCK(lock); } while (0)
@@ -52,9 +64,15 @@
 #define __UNLOCK_IRQ(lock) \
   do { local_irq_enable(); __UNLOCK(lock); } while (0)
 
+#define __HARD_UNLOCK_IRQ(lock) \
+  do { hard_local_irq_enable(); __HARD_UNLOCK(lock); } while (0)
+
 #define __UNLOCK_IRQRESTORE(lock, flags) \
   do { local_irq_restore(flags); __UNLOCK(lock); } while (0)
 
+#define __HARD_UNLOCK_IRQRESTORE(lock, flags) \
+  do { hard_local_irq_restore(flags); __HARD_UNLOCK(lock); } while (0)
+
 #define _raw_spin_lock(lock)			__LOCK(lock)
 #define _raw_spin_lock_nested(lock, subclass)	__LOCK(lock)
 #define _raw_read_lock(lock)			__LOCK(lock)
diff --git a/include/linux/spinlock_pipeline.h b/include/linux/spinlock_pipeline.h
new file mode 100644
index 000000000000..cc17d0c74908
--- /dev/null
+++ b/include/linux/spinlock_pipeline.h
@@ -0,0 +1,355 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#ifndef __LINUX_SPINLOCK_PIPELINE_H
+#define __LINUX_SPINLOCK_PIPELINE_H
+
+#ifndef __LINUX_SPINLOCK_H
+# error "Please don't include this file directly. Use spinlock.h."
+#endif
+
+#define hard_spin_lock_irqsave(__rlock, __flags)		\
+	do {							\
+		(__flags) = __hard_spin_lock_irqsave(__rlock);	\
+	} while (0)
+
+#define hard_spin_trylock_irqsave(__rlock, __flags)			\
+	({								\
+		int __locked;						\
+		(__flags) = __hard_spin_trylock_irqsave(__rlock, &__locked); \
+		__locked;						\
+	})
+
+#define mutable_spin_lock_init(__rlock)	hard_spin_lock_init(__rlock)
+
+/*
+ * CAUTION: We don't want the hand-coded irq-enable of
+ * do_raw_spin_lock_flags(), hard locked sections assume that
+ * interrupts are not re-enabled during lock-acquire.
+ */
+#define hard_lock_acquire(__rlock, __try, __ip)				\
+	do {								\
+		if (irq_pipeline_debug_locking()) {			\
+			spin_acquire(&(__rlock)->dep_map, 0, __try, __ip); \
+			LOCK_CONTENDED(__rlock, do_raw_spin_trylock, do_raw_spin_lock); \
+		} else							\
+			do_raw_spin_lock(__rlock);			\
+	} while (0)
+
+#define hard_lock_acquire_nested(__rlock, __subclass, __ip)		\
+	do {								\
+		if (irq_pipeline_debug_locking()) {			\
+			spin_acquire(&(__rlock)->dep_map, __subclass, 0, __ip); \
+			LOCK_CONTENDED(__rlock, do_raw_spin_trylock, do_raw_spin_lock); \
+		} else							\
+			do_raw_spin_lock(__rlock);			\
+	} while (0)
+
+#define hard_trylock_acquire(__rlock, __try, __ip)			\
+	do {								\
+		if (irq_pipeline_debug_locking())			\
+			spin_acquire(&(__rlock)->dep_map, 0, __try, __ip); \
+	} while (0)
+
+#define hard_lock_release(__rlock, __ip)				\
+	do {								\
+		if (irq_pipeline_debug_locking())			\
+			spin_release(&(__rlock)->dep_map, 1, __ip);	\
+		do_raw_spin_unlock(__rlock);				\
+	} while (0)
+
+#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)
+
+#ifdef CONFIG_DEBUG_SPINLOCK
+#define hard_spin_lock_init(__lock)				\
+	do {							\
+		static struct lock_class_key __key;		\
+		__raw_spin_lock_init((raw_spinlock_t *)__lock, #__lock, &__key); \
+	} while (0)
+#else
+#define hard_spin_lock_init(__rlock)				\
+	do { *(__rlock) = __HARD_SPIN_LOCK_UNLOCKED(__rlock); } while (0)
+#endif
+
+/*
+ * XXX: no preempt_enable/disable when hard locking.
+ */
+
+static inline
+void hard_spin_lock(struct raw_spinlock *rlock)
+{
+	hard_lock_acquire(rlock, 0, _THIS_IP_);
+}
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+static inline
+void hard_spin_lock_nested(struct raw_spinlock *rlock, int subclass)
+{
+	hard_lock_acquire_nested(rlock, subclass, _THIS_IP_);
+}
+#else
+static inline
+void hard_spin_lock_nested(struct raw_spinlock *rlock, int subclass)
+{
+	hard_spin_lock(rlock);
+}
+#endif
+
+static inline
+void hard_spin_unlock(struct raw_spinlock *rlock)
+{
+	hard_lock_release(rlock, _THIS_IP_);
+}
+
+static inline
+void hard_spin_lock_irq(struct raw_spinlock *rlock)
+{
+	hard_local_irq_disable();
+	hard_lock_acquire(rlock, 0, _THIS_IP_);
+}
+
+static inline
+void hard_spin_unlock_irq(struct raw_spinlock *rlock)
+{
+	hard_lock_release(rlock, _THIS_IP_);
+	hard_local_irq_enable();
+}
+
+static inline
+void hard_spin_unlock_irqrestore(struct raw_spinlock *rlock,
+				 unsigned long flags)
+{
+	hard_lock_release(rlock, _THIS_IP_);
+	hard_local_irq_restore(flags);
+}
+
+static inline
+unsigned long __hard_spin_lock_irqsave(struct raw_spinlock *rlock)
+{
+	unsigned long flags = hard_local_irq_save();
+
+	hard_lock_acquire(rlock, 0, _THIS_IP_);
+
+	return flags;
+}
+
+static inline
+int hard_spin_trylock(struct raw_spinlock *rlock)
+{
+	if (do_raw_spin_trylock(rlock)) {
+		hard_trylock_acquire(rlock, 1, _THIS_IP_);
+		return 1;
+	}
+	return 0;
+}
+
+static inline
+unsigned long __hard_spin_trylock_irqsave(struct raw_spinlock *rlock,
+					  int *locked)
+{
+	unsigned long flags = hard_local_irq_save();
+	*locked = hard_spin_trylock(rlock);
+	return *locked ? flags : ({ hard_local_irq_restore(flags); flags; });
+}
+
+static inline
+int hard_spin_trylock_irq(struct raw_spinlock *rlock)
+{
+	hard_local_irq_disable();
+	return hard_spin_trylock(rlock) ? : ({ hard_local_irq_enable(); 0; });
+}
+
+static inline
+int hard_spin_is_locked(struct raw_spinlock *rlock)
+{
+	return arch_spin_is_locked(&rlock->raw_lock);
+}
+
+static inline
+int hard_spin_is_contended(struct raw_spinlock *rlock)
+{
+#ifdef CONFIG_GENERIC_LOCKBREAK
+	return rlock->break_lock;
+#elif defined(arch_spin_is_contended)
+	return arch_spin_is_contended(&rlock->raw_lock);
+#else
+	return 0;
+#endif
+}
+
+#else  /* !SMP && !DEBUG_SPINLOCK */
+
+#define hard_spin_lock_init(__rlock)	do { (void)(__rlock); } while (0)
+#define hard_spin_lock(__rlock)		__HARD_LOCK(__rlock)
+#define hard_spin_lock_nested(__rlock, __subclass)  \
+	do { __HARD_LOCK(__rlock); (void)(__subclass); } while (0)
+#define hard_spin_unlock(__rlock)	__HARD_UNLOCK(__rlock)
+#define hard_spin_lock_irq(__rlock)	__HARD_LOCK_IRQ(__rlock)
+#define hard_spin_unlock_irq(__rlock)	__HARD_UNLOCK_IRQ(__rlock)
+#define hard_spin_unlock_irqrestore(__rlock, __flags)	\
+	__HARD_UNLOCK_IRQRESTORE(__rlock, __flags)
+#define __hard_spin_lock_irqsave(__rlock)		\
+	({						\
+		unsigned long __flags;			\
+		__HARD_LOCK_IRQSAVE(__rlock, __flags);	\
+		__flags;				\
+	})
+#define __hard_spin_trylock_irqsave(__rlock, __locked)	\
+	({						\
+		unsigned long __flags;			\
+		__HARD_LOCK_IRQSAVE(__rlock, __flags);	\
+		*(__locked) = 1;			\
+		__flags;				\
+	})
+#define hard_spin_trylock(__rlock)	({ __HARD_LOCK(__rlock); 1; })
+#define hard_spin_trylock_irq(__rlock)	({ __HARD_LOCK_IRQ(__rlock); 1; })
+#define hard_spin_is_locked(__rlock)	((void)(__rlock), 0)
+#define hard_spin_is_contended(__rlock)	((void)(__rlock), 0)
+#endif	/* !SMP && !DEBUG_SPINLOCK */
+
+/*
+ * In the pipeline entry context, the regular preemption and root
+ * stall logic do not apply since we may actually have preempted any
+ * critical section of the kernel which is protected by regular
+ * locking (spin or stall), or we may even have preempted the head
+ * stage. Therefore, we just need to grab the raw spinlock underlying
+ * a mutable spinlock to exclude other CPUs.
+ *
+ * NOTE: When entering the pipeline, IRQs are already hard disabled.
+ */
+
+void __mutable_spin_lock(struct raw_spinlock *rlock);
+void __mutable_spin_lock_nested(struct raw_spinlock *rlock, int subclass);
+
+static inline void mutable_spin_lock(struct raw_spinlock *rlock)
+{
+	if (in_pipeline())
+		hard_lock_acquire(rlock, 0, _THIS_IP_);
+	else
+		__mutable_spin_lock(rlock);
+}
+
+#ifdef CONFIG_DEBUG_LOCK_ALLOC
+static inline
+void mutable_spin_lock_nested(struct raw_spinlock *rlock, int subclass)
+{
+	if (in_pipeline())
+		hard_lock_acquire_nested(rlock, subclass, _THIS_IP_);
+	else
+		__mutable_spin_lock_nested(rlock, subclass);
+}
+#else
+static inline
+void mutable_spin_lock_nested(struct raw_spinlock *rlock, int subclass)
+{
+	mutable_spin_lock(rlock);
+}
+#endif
+
+void __mutable_spin_unlock(struct raw_spinlock *rlock);
+
+static inline void mutable_spin_unlock(struct raw_spinlock *rlock)
+{
+	if (in_pipeline())
+		hard_lock_release(rlock, _THIS_IP_);
+	else
+		__mutable_spin_unlock(rlock);
+}
+
+void __mutable_spin_lock_irq(struct raw_spinlock *rlock);
+
+static inline void mutable_spin_lock_irq(struct raw_spinlock *rlock)
+{
+	if (in_pipeline())
+		hard_lock_acquire(rlock, 0, _THIS_IP_);
+	else
+		__mutable_spin_lock_irq(rlock);
+}
+
+void __mutable_spin_unlock_irq(struct raw_spinlock *rlock);
+
+static inline void mutable_spin_unlock_irq(struct raw_spinlock *rlock)
+{
+	if (in_pipeline())
+		hard_lock_release(rlock, _THIS_IP_);
+	else
+		__mutable_spin_unlock_irq(rlock);
+}
+
+unsigned long __mutable_spin_lock_irqsave(struct raw_spinlock *rlock);
+
+#define mutable_spin_lock_irqsave(__rlock, __flags)			\
+	do {								\
+		if (in_pipeline()) {					\
+			hard_lock_acquire(__rlock, 0, _THIS_IP_);	\
+			(__flags) = hard_local_save_flags();		\
+		} else							\
+			(__flags) = __mutable_spin_lock_irqsave(__rlock); \
+	} while (0)
+
+void __mutable_spin_unlock_irqrestore(struct raw_spinlock *rlock,
+				      unsigned long flags);
+
+static inline void mutable_spin_unlock_irqrestore(struct raw_spinlock *rlock,
+						  unsigned long flags)
+{
+
+	if (in_pipeline())
+		hard_lock_release(rlock, _THIS_IP_);
+	else
+		__mutable_spin_unlock_irqrestore(rlock, flags);
+}
+
+int __mutable_spin_trylock(struct raw_spinlock *rlock);
+
+static inline int mutable_spin_trylock(struct raw_spinlock *rlock)
+{
+	if (in_pipeline()) {
+		if (do_raw_spin_trylock(rlock)) {
+			hard_trylock_acquire(rlock, 1, _THIS_IP_);
+			return 1;
+		}
+		return 0;
+	}
+
+	return __mutable_spin_trylock(rlock);
+}
+
+int __mutable_spin_trylock_irqsave(struct raw_spinlock *rlock,
+				   unsigned long *flags);
+
+#define mutable_spin_trylock_irqsave(__rlock, __flags)			\
+	({								\
+		int __ret = 1;						\
+		if (in_pipeline()) {					\
+			if (do_raw_spin_trylock(__rlock)) {		\
+				hard_trylock_acquire(__rlock, 1, _THIS_IP_); \
+				(__flags) = hard_local_save_flags();	\
+			} else						\
+				__ret = 0;				\
+		} else							\
+			__ret = __mutable_spin_trylock_irqsave(__rlock, &(__flags)); \
+		__ret;							\
+	})
+
+static inline int mutable_spin_trylock_irq(struct raw_spinlock *rlock)
+{
+	unsigned long flags;
+	return mutable_spin_trylock_irqsave(rlock, flags);
+}
+
+static inline
+int mutable_spin_is_locked(struct raw_spinlock *rlock)
+{
+	return hard_spin_is_locked(rlock);
+}
+
+static inline
+int mutable_spin_is_contended(struct raw_spinlock *rlock)
+{
+	return hard_spin_is_contended(rlock);
+}
+
+#endif /* __LINUX_SPINLOCK_PIPELINE_H */
diff --git a/include/linux/spinlock_types.h b/include/linux/spinlock_types.h
index 24b4e6f2c1a2..50a9860b4cca 100644
--- a/include/linux/spinlock_types.h
+++ b/include/linux/spinlock_types.h
@@ -80,6 +80,142 @@ typedef struct spinlock {
 
 #define DEFINE_SPINLOCK(x)	spinlock_t x = __SPIN_LOCK_UNLOCKED(x)
 
+#ifdef CONFIG_IRQ_PIPELINE
+
+void __bad_spinlock_type(void);
+
+#define __RAWLOCK(x) ((struct raw_spinlock *)(x))
+
+#define LOCK_ALTERNATIVES(__lock, __base_op, __raw_form, __args...)	\
+	do {								\
+		if (__builtin_types_compatible_p(typeof(__lock),	\
+						 raw_spinlock_t *))	\
+			__raw_form;					\
+		else if (__builtin_types_compatible_p(typeof(__lock),	\
+						 hard_spinlock_t *))	\
+			hard_ ## __base_op(__RAWLOCK(__lock), ##__args); \
+		else if (__builtin_types_compatible_p(typeof(__lock),	\
+						 mutable_spinlock_t *))	\
+			mutable_ ## __base_op(__RAWLOCK(__lock), ##__args); \
+		else							\
+			__bad_spinlock_type();				\
+	} while (0)
+
+#define LOCK_ALTERNATIVES_RET(__lock, __base_op, __raw_form, __args...) \
+	({								\
+		long __ret = 0;						\
+		if (__builtin_types_compatible_p(typeof(__lock),	\
+						 raw_spinlock_t *))	\
+			__ret = __raw_form;				\
+		else if (__builtin_types_compatible_p(typeof(__lock),	\
+						 hard_spinlock_t *))	\
+			__ret = hard_ ## __base_op(__RAWLOCK(__lock), ##__args); \
+		else if (__builtin_types_compatible_p(typeof(__lock),	\
+						 mutable_spinlock_t *))	\
+			__ret = mutable_ ## __base_op(__RAWLOCK(__lock), ##__args); \
+		else							\
+			__bad_spinlock_type();				\
+		__ret;							\
+	})
+
+#define LOCKDEP_ALT_DEPMAP(__lock)					\
+	({								\
+		struct lockdep_map *__ret;				\
+		if (__builtin_types_compatible_p(typeof(&(__lock)->dep_map), \
+						 struct phony_lockdep_map *)) \
+			__ret = &__RAWLOCK(__lock)->dep_map;		\
+		else							\
+			__ret = (struct lockdep_map *)(&(__lock)->dep_map); \
+		__ret;							\
+	})
+
+#define LOCKDEP_HARD_DEBUG(__lock, __nodebug, __debug)	\
+	do {						\
+		if (__builtin_types_compatible_p(typeof(__lock),	\
+						raw_spinlock_t *) ||	\
+			irq_pipeline_debug_locking()) {			\
+			__debug;			\
+		} else {				\
+			__nodebug;			\
+		}					\
+	} while (0)
+
+#define LOCKDEP_HARD_DEBUG_RET(__lock, __nodebug, __debug)	\
+	({						\
+		typeof(__nodebug) __ret;		\
+		if (__builtin_types_compatible_p(typeof(__lock),	\
+						raw_spinlock_t *) ||	\
+			irq_pipeline_debug_locking()) {			\
+			__ret = (__debug);		\
+		} else {				\
+			__ret = (__nodebug);		\
+		}					\
+		__ret;					\
+	})
+
+#define __HARD_SPIN_LOCK_UNLOCKED(__rlock)	\
+	__RAW_SPIN_LOCK_UNLOCKED(__rlock)
+
+#define __HARD_SPIN_LOCK_INITIALIZER(__lock)				\
+	{								\
+		.rlock = __HARD_SPIN_LOCK_UNLOCKED((__lock).rlock),	\
+	}
+
+#define DEFINE_HARD_SPINLOCK(x)	hard_spinlock_t x = {	\
+		.rlock = __HARD_SPIN_LOCK_UNLOCKED(x),	\
+	}
+
+struct phony_lockdep_map {
+	/* empty */
+};
+
+typedef struct hard_spinlock {
+	/* XXX: offset_of(struct hard_spinlock, rlock) == 0 */
+	struct raw_spinlock rlock;
+	struct phony_lockdep_map dep_map;
+} hard_spinlock_t;
+
+#define DEFINE_MUTABLE_SPINLOCK(x)	mutable_spinlock_t x = {	\
+		.rlock = __RAW_SPIN_LOCK_UNLOCKED(x),			\
+	}
+
+typedef struct mutable_spinlock {
+	/* XXX: offset_of(struct mutable_spinlock, rlock) == 0 */
+	struct raw_spinlock rlock;
+	unsigned long hwflags;
+	struct phony_lockdep_map dep_map;
+} mutable_spinlock_t;
+
+#else
+
+typedef raw_spinlock_t hard_spinlock_t;
+
+typedef raw_spinlock_t mutable_spinlock_t;
+
+#define LOCK_ALTERNATIVES(__lock, __base_op, __raw_form, __args...)	\
+	__raw_form
+
+#define LOCK_ALTERNATIVES_RET(__lock, __base_op, __raw_form, __args...) \
+	__raw_form
+
+#define LOCKDEP_ALT_DEPMAP(__lock)	(&(__lock)->dep_map)
+
+#define LOCKDEP_HARD_DEBUG(__lock, __nondebug, __debug)		do { __debug; } while (0)
+
+#define LOCKDEP_HARD_DEBUG_RET(__lock, __nondebug, __debug)	({ __debug; })
+
+#define DEFINE_HARD_SPINLOCK(x)		DEFINE_RAW_SPINLOCK(x)
+
+#define DEFINE_MUTABLE_SPINLOCK(x)	DEFINE_RAW_SPINLOCK(x)
+
+#define __RAWLOCK(x) (x)
+
+#define __HARD_SPIN_LOCK_UNLOCKED(__lock)	__RAW_SPIN_LOCK_UNLOCKED(__lock)
+
+#define __HARD_SPIN_LOCK_INITIALIZER(__lock)	__RAW_SPIN_LOCK_UNLOCKED(__lock)
+
+#endif	/* CONFIG_IRQ_PIPELINE */
+
 #include <linux/rwlock_types.h>
 
 #endif /* __LINUX_SPINLOCK_TYPES_H */
diff --git a/include/linux/stop_machine.h b/include/linux/stop_machine.h
index f9a0c6189852..132752acfa82 100644
--- a/include/linux/stop_machine.h
+++ b/include/linux/stop_machine.h
@@ -6,6 +6,7 @@
 #include <linux/cpumask.h>
 #include <linux/smp.h>
 #include <linux/list.h>
+#include <linux/interrupt.h>
 
 /*
  * stop_cpu[s]() is simplistic per-cpu maximum priority cpu
@@ -145,7 +146,9 @@ static inline int stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,
 	unsigned long flags;
 	int ret;
 	local_irq_save(flags);
+	hard_irq_disable();
 	ret = fn(data);
+	hard_irq_enable();
 	local_irq_restore(flags);
 	return ret;
 }
diff --git a/include/linux/thread_info.h b/include/linux/thread_info.h
index 659a4400517b..643623109640 100644
--- a/include/linux/thread_info.h
+++ b/include/linux/thread_info.h
@@ -155,6 +155,71 @@ check_copy_size(const void *addr, size_t bytes, bool is_source)
 static inline void arch_setup_new_exec(void) { }
 #endif
 
+#ifdef ti_local_flags
+/*
+ * If the arch defines a set of per-thread synchronous flags, provide
+ * generic accessors to them.
+ */
+static inline void set_ti_local_flags(struct thread_info *ti, unsigned int mask)
+{
+	ti_local_flags(ti) |= mask;
+}
+
+static inline void set_thread_local_flags(unsigned int mask)
+{
+	set_ti_local_flags(current_thread_info(), mask);
+}
+
+static inline
+int test_and_set_ti_local_flags(struct thread_info *ti, unsigned int mask)
+{
+	int old = ti_local_flags(ti) & mask;
+	ti_local_flags(ti) |= mask;
+	return old != 0;
+}
+
+static inline int test_and_set_thread_local_flags(unsigned int mask)
+{
+	return test_and_set_ti_local_flags(current_thread_info(), mask);
+}
+
+static inline
+void clear_ti_local_flags(struct thread_info *ti, unsigned int mask)
+{
+	ti_local_flags(ti) &= ~mask;
+}
+
+static inline
+int test_and_clear_ti_local_flags(struct thread_info *ti, unsigned int mask)
+{
+	int old = ti_local_flags(ti) & mask;
+	ti_local_flags(ti) &= ~mask;
+	return old != 0;
+}
+
+static inline int test_and_clear_thread_local_flags(unsigned int mask)
+{
+	return test_and_clear_ti_local_flags(current_thread_info(), mask);
+}
+
+static inline void clear_thread_local_flags(unsigned int mask)
+{
+	clear_ti_local_flags(current_thread_info(), mask);
+}
+
+static inline
+bool test_ti_local_flags(struct thread_info *ti, unsigned int mask)
+{
+	return (ti_local_flags(ti) & mask) != 0;
+}
+
+static inline bool test_thread_local_flags(unsigned int mask)
+{
+	return test_ti_local_flags(current_thread_info(), mask);
+}
+
+#endif	/* ti_local_flags */
+
 #endif	/* __KERNEL__ */
 
 #endif /* _LINUX_THREAD_INFO_H */
diff --git a/include/linux/tick.h b/include/linux/tick.h
index f92a10b5e112..9c2d03a153c1 100644
--- a/include/linux/tick.h
+++ b/include/linux/tick.h
@@ -20,6 +20,14 @@ extern void tick_suspend_local(void);
 extern void tick_resume_local(void);
 extern void tick_handover_do_timer(void);
 extern void tick_cleanup_dead_cpu(int cpu);
+
+#ifdef CONFIG_IRQ_PIPELINE
+int tick_install_proxy(void (*setup_proxy)(struct clock_proxy_device *dev),
+		const struct cpumask *cpumask);
+void tick_uninstall_proxy(const struct cpumask *cpumask);
+void tick_notify_proxy(void);
+#endif
+
 #else /* CONFIG_GENERIC_CLOCKEVENTS */
 static inline void tick_init(void) { }
 static inline void tick_suspend_local(void) { }
diff --git a/include/linux/timekeeping.h b/include/linux/timekeeping.h
index b27e2ffa96c1..31278a7c4833 100644
--- a/include/linux/timekeeping.h
+++ b/include/linux/timekeeping.h
@@ -69,6 +69,7 @@ extern ktime_t ktime_get_with_offset(enum tk_offsets offs);
 extern ktime_t ktime_get_coarse_with_offset(enum tk_offsets offs);
 extern ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs);
 extern ktime_t ktime_get_raw(void);
+extern ktime_t ktime_get_real_fast(void);
 extern u32 ktime_get_resolution_ns(void);
 
 /**
diff --git a/include/linux/tracepoint.h b/include/linux/tracepoint.h
index 1fb11daa5c53..fd43fa65f20a 100644
--- a/include/linux/tracepoint.h
+++ b/include/linux/tracepoint.h
@@ -210,7 +210,7 @@ static inline struct tracepoint *tracepoint_ptr_deref(tracepoint_ptr_t *p)
 			__DO_TRACE(&__tracepoint_##name,		\
 				TP_PROTO(data_proto),			\
 				TP_ARGS(data_args),			\
-				TP_CONDITION(cond), 1);			\
+				TP_CONDITION(cond), running_inband());	\
 	}
 #else
 #define __DECLARE_TRACE_RCU(name, proto, args, cond, data_proto, data_args)
diff --git a/include/linux/vmalloc.h b/include/linux/vmalloc.h
index 4e7809408073..1d96a633dcc1 100644
--- a/include/linux/vmalloc.h
+++ b/include/linux/vmalloc.h
@@ -234,5 +234,6 @@ pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)
 
 int register_vmap_purge_notifier(struct notifier_block *nb);
 int unregister_vmap_purge_notifier(struct notifier_block *nb);
+void arch_advertise_page_mapping(unsigned long start, unsigned long end);
 
 #endif /* _LINUX_VMALLOC_H */
diff --git a/include/trace/events/irq.h b/include/trace/events/irq.h
index eeceafaaea4c..e76f462f5f54 100644
--- a/include/trace/events/irq.h
+++ b/include/trace/events/irq.h
@@ -100,6 +100,48 @@ TRACE_EVENT(irq_handler_exit,
 		  __entry->irq, __entry->ret ? "handled" : "unhandled")
 );
 
+/**
+ * irq_pipeline_entry - called when an external irq enters the pipeline
+ * @irq: irq number
+ */
+TRACE_EVENT(irq_pipeline_entry,
+
+	TP_PROTO(int irq),
+
+	TP_ARGS(irq),
+
+	TP_STRUCT__entry(
+		__field(	int,	irq		)
+	),
+
+	TP_fast_assign(
+		__entry->irq = irq;
+	),
+
+	TP_printk("irq=%d", __entry->irq)
+);
+
+/**
+ * irq_pipeline_exit - called when an external irq leaves the pipeline
+ * @irq: irq number
+ */
+TRACE_EVENT(irq_pipeline_exit,
+
+	TP_PROTO(int irq),
+
+	TP_ARGS(irq),
+
+	TP_STRUCT__entry(
+		__field(	int,	irq		)
+	),
+
+	TP_fast_assign(
+		__entry->irq = irq;
+	),
+
+	TP_printk("irq=%d", __entry->irq)
+);
+
 DECLARE_EVENT_CLASS(softirq,
 
 	TP_PROTO(unsigned int vec_nr),
diff --git a/include/uapi/linux/clocksource.h b/include/uapi/linux/clocksource.h
new file mode 100644
index 000000000000..a0a1c2747398
--- /dev/null
+++ b/include/uapi/linux/clocksource.h
@@ -0,0 +1,33 @@
+/*
+ * Definitions for user-mappable clock sources.
+ *
+ * Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>
+ */
+#ifndef _UAPI_LINUX_CLOCKSOURCE_H
+#define _UAPI_LINUX_CLOCKSOURCE_H
+
+enum clksrc_user_mmio_type {
+	CLKSRC_MMIO_L_UP,
+	CLKSRC_MMIO_L_DOWN,
+	CLKSRC_MMIO_W_UP,
+	CLKSRC_MMIO_W_DOWN,
+	CLKSRC_DMMIO_L_UP,
+	CLKSRC_DMMIO_W_UP,
+
+	CLKSRC_MMIO_TYPE_NR,
+};
+
+struct clksrc_user_mmio_info {
+	enum clksrc_user_mmio_type type;
+	void *reg_lower;
+	unsigned int mask_lower;
+	unsigned int bits_lower;
+	void *reg_upper;
+	unsigned int mask_upper;
+};
+
+#define CLKSRC_USER_MMIO_MAX 16
+
+#define CLKSRC_USER_MMIO_MAP _IOWR(0xC1, 0, struct clksrc_user_mmio_info)
+
+#endif /* _UAPI_LINUX_CLOCKSOURCE_H */
diff --git a/include/vdso/helpers.h b/include/vdso/helpers.h
index 01641dbb68ef..3f378e332f41 100644
--- a/include/vdso/helpers.h
+++ b/include/vdso/helpers.h
@@ -34,6 +34,7 @@ static __always_inline void vdso_write_begin(struct vdso_data *vd)
 	 * updates to vd[x].seq and it is possible that the value seen by the
 	 * reader it is inconsistent.
 	 */
+	hard_cond_local_irq_disable();
 	WRITE_ONCE(vd[CS_HRES_COARSE].seq, vd[CS_HRES_COARSE].seq + 1);
 	WRITE_ONCE(vd[CS_RAW].seq, vd[CS_RAW].seq + 1);
 	smp_wmb();
@@ -49,6 +50,7 @@ static __always_inline void vdso_write_end(struct vdso_data *vd)
 	 */
 	WRITE_ONCE(vd[CS_HRES_COARSE].seq, vd[CS_HRES_COARSE].seq + 1);
 	WRITE_ONCE(vd[CS_RAW].seq, vd[CS_RAW].seq + 1);
+	hard_cond_local_irq_enable();
 }
 
 #endif /* !__ASSEMBLY__ */
diff --git a/init/Kconfig b/init/Kconfig
index b4daad2bac23..9309faae3456 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1433,7 +1433,19 @@ config PRINTK
 config PRINTK_NMI
 	def_bool y
 	depends on PRINTK
-	depends on HAVE_NMI
+	depends on HAVE_NMI || IRQ_PIPELINE
+
+config RAW_PRINTK
+       bool "Enable support for raw printk"
+       default n
+       help
+         This option enables a printk variant called raw_printk() for
+         writing all output unmodified to a raw console channel
+         immediately, without any header or preparation whatsoever,
+         usable from any context.
+
+	 Unlike early_printk() console devices, raw_printk() devices
+         can live past the boot sequence.
 
 config BUG
 	bool "BUG() support" if EXPERT
diff --git a/init/Makefile b/init/Makefile
index 6246a06364d0..1d060bc15c98 100644
--- a/init/Makefile
+++ b/init/Makefile
@@ -35,4 +35,5 @@ include/generated/compile.h: FORCE
 	@$($(quiet)chk_compile.h)
 	$(Q)$(CONFIG_SHELL) $(srctree)/scripts/mkcompile_h $@	\
 	"$(UTS_MACHINE)" "$(CONFIG_SMP)" "$(CONFIG_PREEMPT)"	\
-	"$(CONFIG_PREEMPT_RT)" "$(CC) $(KBUILD_CFLAGS)"
+	"$(CONFIG_PREEMPT_RT)" "$(CONFIG_IRQ_PIPELINE)" \
+	"$(CC) $(KBUILD_CFLAGS)"
diff --git a/init/main.c b/init/main.c
index 91f6ebb30ef0..3f338902d01f 100644
--- a/init/main.c
+++ b/init/main.c
@@ -49,6 +49,7 @@
 #include <linux/tick.h>
 #include <linux/sched/isolation.h>
 #include <linux/interrupt.h>
+#include <linux/irq_pipeline.h>
 #include <linux/taskstats_kern.h>
 #include <linux/delayacct.h>
 #include <linux/unistd.h>
@@ -583,7 +584,7 @@ asmlinkage __visible void __init start_kernel(void)
 
 	cgroup_init_early();
 
-	local_irq_disable();
+	hard_local_irq_disable();
 	early_boot_irqs_disabled = true;
 
 	/*
@@ -623,6 +624,7 @@ asmlinkage __visible void __init start_kernel(void)
 	setup_log_buf(0);
 	vfs_caches_init_early();
 	sort_main_extable();
+	irq_pipeline_init_early();
 	trap_init();
 	mm_init();
 
@@ -672,6 +674,7 @@ asmlinkage __visible void __init start_kernel(void)
 	/* init some links before init_ISA_irqs() */
 	early_irq_init();
 	init_IRQ();
+	irq_pipeline_init();
 	tick_init();
 	rcu_init_nohz();
 	init_timers();
@@ -700,7 +703,7 @@ asmlinkage __visible void __init start_kernel(void)
 	WARN(!irqs_disabled(), "Interrupts were enabled early\n");
 
 	early_boot_irqs_disabled = false;
-	local_irq_enable();
+	local_irq_enable_full();
 
 	kmem_cache_init_late();
 
diff --git a/kernel/Kconfig.dovetail b/kernel/Kconfig.dovetail
new file mode 100644
index 000000000000..adc6922d64e1
--- /dev/null
+++ b/kernel/Kconfig.dovetail
@@ -0,0 +1,14 @@
+
+# DOVETAIL co-kernel interface
+config HAVE_DOVETAIL
+	bool
+
+config DOVETAIL
+	bool "Dovetail interface"
+	depends on HAVE_DOVETAIL
+	select IRQ_PIPELINE
+	default n
+	---help---
+	  Activate this option if you want to enable the interface for
+	  running a secondary kernel side-by-side with Linux (aka
+	  "dual kernel" configuration).
diff --git a/kernel/Makefile b/kernel/Makefile
index daad787fb795..3743adee6652 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -100,6 +100,7 @@ obj-$(CONFIG_TRACE_CLOCK) += trace/
 obj-$(CONFIG_RING_BUFFER) += trace/
 obj-$(CONFIG_TRACEPOINTS) += trace/
 obj-$(CONFIG_IRQ_WORK) += irq_work.o
+obj-$(CONFIG_DOVETAIL) += dovetail.o
 obj-$(CONFIG_CPU_PM) += cpu_pm.o
 obj-$(CONFIG_BPF) += bpf/
 
diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index be01a4d627c9..554742481e07 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -114,7 +114,7 @@ void context_tracking_enter(enum ctx_state state)
 	 * helpers are enough to protect RCU uses inside the exception. So
 	 * just return immediately if we detect we are in an IRQ.
 	 */
-	if (in_interrupt())
+	if (!running_inband() || in_interrupt())
 		return;
 
 	local_irq_save(flags);
@@ -170,7 +170,7 @@ void context_tracking_exit(enum ctx_state state)
 {
 	unsigned long flags;
 
-	if (in_interrupt())
+	if (!running_inband() || in_interrupt())
 		return;
 
 	local_irq_save(flags);
diff --git a/kernel/debug/debug_core.c b/kernel/debug/debug_core.c
index f76d6f77dd5e..0b354ac9d1b9 100644
--- a/kernel/debug/debug_core.c
+++ b/kernel/debug/debug_core.c
@@ -120,8 +120,8 @@ static struct kgdb_bkpt		kgdb_break[KGDB_MAX_BREAKPOINTS] = {
  */
 atomic_t			kgdb_active = ATOMIC_INIT(-1);
 EXPORT_SYMBOL_GPL(kgdb_active);
-static DEFINE_RAW_SPINLOCK(dbg_master_lock);
-static DEFINE_RAW_SPINLOCK(dbg_slave_lock);
+static DEFINE_HARD_SPINLOCK(dbg_master_lock);
+static DEFINE_HARD_SPINLOCK(dbg_slave_lock);
 
 /*
  * We use NR_CPUs not PERCPU, in case kgdb is used to debug early
@@ -549,7 +549,7 @@ static int kgdb_cpu_enter(struct kgdb_state *ks, struct pt_regs *regs,
 	 * Interrupts will be restored by the 'trap return' code, except when
 	 * single stepping.
 	 */
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	cpu = ks->cpu;
 	kgdb_info[cpu].debuggerinfo = regs;
@@ -600,7 +600,7 @@ static int kgdb_cpu_enter(struct kgdb_state *ks, struct pt_regs *regs,
 			smp_mb__before_atomic();
 			atomic_dec(&slaves_in_kgdb);
 			dbg_touch_watchdogs();
-			local_irq_restore(flags);
+			hard_local_irq_restore(flags);
 			return 0;
 		}
 		cpu_relax();
@@ -618,7 +618,7 @@ static int kgdb_cpu_enter(struct kgdb_state *ks, struct pt_regs *regs,
 		atomic_set(&kgdb_active, -1);
 		raw_spin_unlock(&dbg_master_lock);
 		dbg_touch_watchdogs();
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 
 		goto acquirelock;
 	}
@@ -651,8 +651,11 @@ static int kgdb_cpu_enter(struct kgdb_state *ks, struct pt_regs *regs,
 		atomic_set(ks->send_ready, 1);
 
 	/* Signal the other CPUs to enter kgdb_wait() */
-	else if ((!kgdb_single_step) && kgdb_do_roundup)
+	else if ((!kgdb_single_step) && kgdb_do_roundup && running_inband()) {
+		hard_cond_local_irq_enable();
 		kgdb_roundup_cpus();
+		hard_cond_local_irq_disable();
+	}
 #endif
 
 	/*
@@ -737,7 +740,7 @@ static int kgdb_cpu_enter(struct kgdb_state *ks, struct pt_regs *regs,
 	atomic_set(&kgdb_active, -1);
 	raw_spin_unlock(&dbg_master_lock);
 	dbg_touch_watchdogs();
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 
 	return kgdb_info[cpu].ret_state;
 }
@@ -855,7 +858,7 @@ static void kgdb_console_write(struct console *co, const char *s,
 	if (!kgdb_connected || atomic_read(&kgdb_active) != -1 || dbg_kdb_mode)
 		return;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	gdbstub_msg_write(s, count);
 	local_irq_restore(flags);
 }
diff --git a/kernel/dovetail.c b/kernel/dovetail.c
new file mode 100644
index 000000000000..7569ecaee6c1
--- /dev/null
+++ b/kernel/dovetail.c
@@ -0,0 +1,395 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#include <linux/timekeeper_internal.h>
+#include <linux/sched/signal.h>
+#include <linux/irq_pipeline.h>
+#include <linux/dovetail.h>
+#include <linux/kvm_host.h>
+#include <asm/unistd.h>
+#include <asm/syscall.h>
+
+static bool dovetail_enabled;
+
+void __weak arch_inband_task_init(struct task_struct *p)
+{
+}
+
+void inband_task_init(struct task_struct *p)
+{
+	struct thread_info *ti = task_thread_info(p);
+
+	clear_ti_local_flags(ti, _TLF_DOVETAIL|_TLF_OOB|_TLF_OFFSTAGE);
+	arch_inband_task_init(p);
+}
+
+#ifdef CONFIG_KVM
+void oob_notify_kvm(void)
+{
+	struct kvm_oob_notifier *nfy;
+	struct irq_pipeline_data *p;
+
+	check_hard_irqs_disabled();
+	p = raw_cpu_ptr(&irq_pipeline);
+	nfy = p->vcpu_notify;
+	if (unlikely(nfy))
+		nfy->handler(nfy);
+}
+EXPORT_SYMBOL_GPL(oob_notify_kvm);
+#endif
+
+void dovetail_init_altsched(struct dovetail_altsched_context *p)
+{
+	struct task_struct *tsk = current;
+
+	check_inband_stage();
+	p->task = tsk;
+	p->active_mm = tsk->mm;
+	p->borrowed_mm = false;
+}
+EXPORT_SYMBOL_GPL(dovetail_init_altsched);
+
+void dovetail_start_altsched(void)
+{
+	check_inband_stage();
+	set_thread_local_flags(_TLF_DOVETAIL);
+}
+EXPORT_SYMBOL_GPL(dovetail_start_altsched);
+
+void dovetail_stop_altsched(void)
+{
+	clear_thread_local_flags(_TLF_DOVETAIL);
+	clear_thread_flag(TIF_MAYDAY);
+}
+EXPORT_SYMBOL_GPL(dovetail_stop_altsched);
+
+void __weak handle_oob_syscall(struct pt_regs *regs)
+{
+}
+
+int __weak handle_pipelined_syscall(struct irq_stage *stage,
+				    struct pt_regs *regs)
+{
+	return 0;
+}
+
+void __weak handle_oob_mayday(struct pt_regs *regs)
+{
+}
+
+static inline
+void call_mayday(struct thread_info *ti, struct pt_regs *regs)
+{
+	clear_ti_thread_flag(ti, TIF_MAYDAY);
+	handle_oob_mayday(regs);
+}
+
+void dovetail_call_mayday(struct thread_info *ti, struct pt_regs *regs)
+{
+	unsigned long flags;
+
+	flags = hard_local_irq_save();
+	call_mayday(ti, regs);
+	hard_local_irq_restore(flags);
+}
+
+int __pipeline_syscall(struct thread_info *ti, struct pt_regs *regs)
+{
+	struct irq_stage *caller_stage, *target_stage;
+	struct irq_stage_data *p, *this_context;
+	unsigned long flags;
+	int ret = 0;
+
+	/*
+	 * We should definitely not pipeline a syscall through the
+	 * slow path with IRQs off.
+	 */
+	WARN_ON_ONCE(dovetail_debug() && hard_irqs_disabled());
+
+	if (!dovetail_enabled)
+		return 0;
+
+	flags = hard_local_irq_save();
+	caller_stage = current_irq_stage;
+	this_context = current_irq_staged;
+	target_stage = &oob_stage;
+next:
+	p = this_staged(target_stage);
+	set_current_irq_staged(p);
+	hard_local_irq_restore(flags);
+	ret = handle_pipelined_syscall(caller_stage, regs);
+	flags = hard_local_irq_save();
+	/*
+	 * Be careful about stage switching _and_ CPU migration that
+	 * might have happened as a result of handing over the syscall
+	 * to the out-of-band handler.
+	 *
+	 * - if a stage migration is detected, fetch the new
+	 * per-stage, per-CPU context pointer.
+	 *
+	 * - if no stage migration happened, switch back to the
+	 * initial caller's stage, on a possibly different CPU though.
+	 */
+	if (current_irq_stage != target_stage)
+		this_context = current_irq_staged;
+	else {
+		p = this_staged(this_context->stage);
+		set_current_irq_staged(p);
+	}
+
+	if (this_context->stage == &inband_stage) {
+		if (target_stage != &inband_stage && ret == 0) {
+			target_stage = &inband_stage;
+			goto next;
+		}
+		p = this_inband_staged();
+		if (stage_irqs_pending(p))
+			sync_current_irq_stage();
+	} else if (test_ti_thread_flag(ti, TIF_MAYDAY))
+		call_mayday(ti, regs);
+
+	hard_local_irq_restore(flags);
+
+	return ret;
+}
+
+void sync_inband_irqs(void)
+{
+	struct irq_stage_data *p;
+	unsigned long flags;
+
+	flags = hard_local_irq_save();
+
+	p = this_inband_staged();
+	if (stage_irqs_pending(p))
+		sync_current_irq_stage();
+
+	hard_local_irq_restore(flags);
+}
+
+int pipeline_syscall(struct thread_info *ti,
+		     unsigned long nr, struct pt_regs *regs)
+{
+	unsigned long local_flags = READ_ONCE(ti_local_flags(ti));
+	int ret;
+
+	/*
+	 * If the syscall number is out of bounds and we are not
+	 * running in-band, this has to be a non-native system call
+	 * handled by some co-kernel from the oob stage. Hand it over
+	 * via the fast syscall handler.
+	 *
+	 * Otherwise, if the system call is out of bounds or alternate
+	 * scheduling is enabled for the current thread, propagate the
+	 * syscall through the pipeline stages. This allows:
+	 *
+	 * - the co-kernel to receive any initial - foreign - syscall
+	 * a thread should send for enabling dovetailing from the
+	 * in-band stage.
+	 *
+	 * - the co-kernel to manipulate the current execution stage
+	 * for handling the request, which includes switching the
+	 * current thread back to the in-band context if the syscall
+	 * is a native one, or promoting it to the oob stage if
+	 * handling a foreign syscall requires this.
+	 *
+	 * Native syscalls from common (non-dovetailed) threads are
+	 * ignored by this routine, flowing down to the in-band system
+	 * call handler.
+	 */
+
+	if (nr >= NR_syscalls && (local_flags & _TLF_OOB)) {
+		handle_oob_syscall(regs);
+		local_flags = READ_ONCE(ti_local_flags(ti));
+		if (local_flags & _TLF_OOB) {
+			if (test_ti_thread_flag(ti, TIF_MAYDAY))
+				dovetail_call_mayday(ti, regs);
+			return 1; /* don't pass down, no tail work. */
+		} else {
+			sync_inband_irqs();
+			return -1; /* don't pass down, do tail work. */
+		}
+	}
+
+	if ((local_flags & _TLF_DOVETAIL) || nr >= NR_syscalls) {
+		ret = __pipeline_syscall(ti, regs);
+		local_flags = READ_ONCE(ti_local_flags(ti));
+		if (local_flags & _TLF_OOB)
+			return 1; /* don't pass down, no tail work. */
+		if (ret)
+			return -1; /* don't pass down, do tail work. */
+	}
+
+	return 0; /* pass syscall down to the host. */
+}
+
+void __weak handle_oob_trap(unsigned int trapnr, struct pt_regs *regs)
+{
+}
+
+void __oob_trap_notify(unsigned int exception, struct pt_regs *regs)
+{
+	unsigned long flags;
+
+	/*
+	 * We send a notification about all traps raised over a
+	 * registered oob stage only.
+	 *
+	 * CAUTION: The out-of-band trap handler expects hard irqs off
+	 * on entry, and might demote the current context to the
+	 * in-band stage, returning with hard irqs on. So we have to
+	 * protect the call accordingly.
+	 */
+	if (dovetail_enabled) {
+		flags = hard_local_irq_save();
+		handle_oob_trap(exception, regs);
+		hard_local_irq_restore(flags);
+	}
+}
+
+void __weak handle_inband_event(enum inband_event_type event, void *data)
+{
+}
+
+void inband_event_notify(enum inband_event_type event, void *data)
+{
+	check_inband_stage();
+
+	if (dovetail_enabled)
+		handle_inband_event(event, data);
+}
+
+void __weak resume_oob_task(struct task_struct *p)
+{
+}
+
+static void finalize_oob_transition(void) /* hard IRQs off */
+{
+	struct irq_pipeline_data *pd;
+	struct irq_stage_data *p;
+	struct task_struct *t;
+
+	pd = raw_cpu_ptr(&irq_pipeline);
+	t = pd->task_inflight;
+	if (t == NULL)
+		return;
+
+	/*
+	 * @t which is in flight to the oob stage might have received
+	 * a signal while waiting in off-stage state to be actually
+	 * scheduled out. We can't act upon that signal safely from
+	 * here, we simply let the task complete the migration process
+	 * to the oob stage. The pending signal will be handled when
+	 * the task eventually exits the out-of-band context by the
+	 * converse migration.
+	 */
+	pd->task_inflight = NULL;
+
+	/*
+	 * IRQs are hard disabled, but the stage transition handler
+	 * may assume the oob stage is stalled: fix this up.
+	 */
+	p = this_oob_staged();
+	set_stage_bit(STAGE_STALL_BIT, p);
+	resume_oob_task(t);
+	clear_stage_bit(STAGE_STALL_BIT, p);
+	if (stage_irqs_pending(p))
+		/* Current stage (in-band) != p->stage (oob). */
+		sync_irq_stage(p->stage);
+}
+
+void oob_trampoline(void)
+{
+	unsigned long flags;
+
+	check_inband_stage();
+	flags = hard_local_irq_save();
+	finalize_oob_transition();
+	hard_local_irq_restore(flags);
+}
+
+int inband_switch_tail(void)
+{
+	bool oob;
+
+	check_hard_irqs_disabled();
+
+	/*
+	 * We may run this code either over the inband or oob
+	 * contexts. If inband, we may have a thread blocked in
+	 * dovetail_leave_inband(), waiting for the companion core to
+	 * schedule it back in over the oob context, in which case
+	 * finalize_oob_transition() should take care of it. If oob,
+	 * the core just switched us back, and we may update the
+	 * context markers before returning to context_switch().
+	 *
+	 * Since the preemption count does not reflect the active
+	 * stage yet upon inband -> oob transition, we figure out
+	 * which one we are on by testing _TLF_OFFSTAGE. Having this
+	 * bit set when running the inband switch tail code means that
+	 * we are completing such transition for the current task,
+	 * switched in by dovetail_context_switch() over the oob
+	 * stage. If so, update the context markers appropriately.
+	 */
+	oob = test_thread_local_flags(_TLF_OFFSTAGE);
+	if (oob) {
+		set_thread_local_flags(_TLF_OOB);
+		if (!IS_ENABLED(CONFIG_HAVE_PERCPU_PREEMPT_COUNT)) {
+			WARN_ON_ONCE(dovetail_debug() &&
+				(preempt_count() & STAGE_MASK));
+			preempt_count_add(STAGE_OFFSET);
+		}
+	} else {
+		finalize_oob_transition();
+		hard_local_irq_enable();
+	}
+
+	return oob;
+}
+
+void __weak inband_clock_was_set(void)
+{
+}
+
+void __weak install_inband_fd(unsigned int fd, struct file *file,
+			      struct files_struct *files)
+{
+}
+
+void __weak uninstall_inband_fd(unsigned int fd, struct file *file,
+				struct files_struct *files)
+{
+}
+
+void __weak replace_inband_fd(unsigned int fd, struct file *file,
+			      struct files_struct *files)
+{
+}
+
+int dovetail_start(void)
+{
+	check_inband_stage();
+
+	if (dovetail_enabled)
+		return -EBUSY;
+
+	if (!oob_stage_present())
+		return -EAGAIN;
+
+	dovetail_enabled = true;
+	smp_wmb();
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(dovetail_start);
+
+void dovetail_stop(void)
+{
+	check_inband_stage();
+
+	dovetail_enabled = false;
+	smp_wmb();
+}
+EXPORT_SYMBOL_GPL(dovetail_stop);
diff --git a/kernel/exit.c b/kernel/exit.c
index a46a50d67002..a175b180efdd 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -14,6 +14,7 @@
 #include <linux/sched/task_stack.h>
 #include <linux/sched/cputime.h>
 #include <linux/interrupt.h>
+#include <linux/irq_pipeline.h>
 #include <linux/module.h>
 #include <linux/capability.h>
 #include <linux/completion.h>
@@ -772,6 +773,7 @@ void __noreturn do_exit(long code)
 	 */
 	raw_spin_lock_irq(&tsk->pi_lock);
 	raw_spin_unlock_irq(&tsk->pi_lock);
+	inband_exit_notify();
 
 	if (unlikely(in_atomic())) {
 		pr_info("note: %s[%d] exited with preempt_count %d\n",
diff --git a/kernel/fork.c b/kernel/fork.c
index 13b38794efb5..12e0c9867d5e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -49,6 +49,7 @@
 #include <linux/cpu.h>
 #include <linux/cgroup.h>
 #include <linux/security.h>
+#include <linux/dovetail.h>
 #include <linux/hugetlb.h>
 #include <linux/seccomp.h>
 #include <linux/swap.h>
@@ -904,6 +905,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 #endif
 
 	setup_thread_stack(tsk, orig);
+	inband_task_init(tsk);
 	clear_user_return_notifier(tsk);
 	clear_tsk_need_resched(tsk);
 	set_task_stack_end_magic(tsk);
@@ -1076,6 +1078,7 @@ static inline void __mmput(struct mm_struct *mm)
 	exit_aio(mm);
 	ksm_exit(mm);
 	khugepaged_exit(mm); /* must run before exit_mmap */
+	inband_cleanup_notify(mm); /* ditto. */
 	exit_mmap(mm);
 	mm_put_huge_zero_page(mm);
 	set_mm_exe_file(mm, NULL);
diff --git a/kernel/irq/Kconfig b/kernel/irq/Kconfig
index f92d9a687372..6477561f6fe4 100644
--- a/kernel/irq/Kconfig
+++ b/kernel/irq/Kconfig
@@ -135,6 +135,20 @@ config GENERIC_IRQ_DEBUGFS
 
 	  If you don't know what to do here, say N.
 
+# Interrupt pipeline
+config HAVE_IRQ_PIPELINE
+	bool
+
+config IRQ_PIPELINE
+	bool "Interrupt pipeline"
+	depends on HAVE_IRQ_PIPELINE
+	select IRQ_DOMAIN
+	default n
+	---help---
+
+	  Activate this option if you want the interrupt pipeline to be
+	  compiled in.
+
 endmenu
 
 config GENERIC_IRQ_MULTI_HANDLER
diff --git a/kernel/irq/Makefile b/kernel/irq/Makefile
index b4f53717d143..b6e43ec9b267 100644
--- a/kernel/irq/Makefile
+++ b/kernel/irq/Makefile
@@ -9,6 +9,8 @@ obj-$(CONFIG_GENERIC_IRQ_CHIP) += generic-chip.o
 obj-$(CONFIG_GENERIC_IRQ_PROBE) += autoprobe.o
 obj-$(CONFIG_IRQ_DOMAIN) += irqdomain.o
 obj-$(CONFIG_IRQ_SIM) += irq_sim.o
+obj-$(CONFIG_IRQ_PIPELINE) += pipeline.o
+obj-$(CONFIG_IRQ_PIPELINE_TORTURE_TEST) += irqptorture.o
 obj-$(CONFIG_PROC_FS) += proc.o
 obj-$(CONFIG_GENERIC_PENDING_IRQ) += migration.o
 obj-$(CONFIG_GENERIC_IRQ_MIGRATION) += cpuhotplug.o
diff --git a/kernel/irq/chip.c b/kernel/irq/chip.c
index b76703b2c0af..c68034bb05dd 100644
--- a/kernel/irq/chip.c
+++ b/kernel/irq/chip.c
@@ -14,6 +14,7 @@
 #include <linux/interrupt.h>
 #include <linux/kernel_stat.h>
 #include <linux/irqdomain.h>
+#include <linux/irq_pipeline.h>
 
 #include <trace/events/irq.h>
 
@@ -48,6 +49,10 @@ int irq_set_chip(unsigned int irq, struct irq_chip *chip)
 
 	if (!chip)
 		chip = &no_irq_chip;
+	else
+		WARN_ONCE(irqs_pipelined() &&
+			  (chip->flags & IRQCHIP_PIPELINE_SAFE) == 0,
+			  "irqchip %s is not pipeline-safe!", chip->name);
 
 	desc->irq_data.chip = chip;
 	irq_put_desc_unlock(desc, flags);
@@ -155,14 +160,6 @@ int irq_set_chip_data(unsigned int irq, void *data)
 }
 EXPORT_SYMBOL(irq_set_chip_data);
 
-struct irq_data *irq_get_irq_data(unsigned int irq)
-{
-	struct irq_desc *desc = irq_to_desc(irq);
-
-	return desc ? &desc->irq_data : NULL;
-}
-EXPORT_SYMBOL_GPL(irq_get_irq_data);
-
 static void irq_state_clr_disabled(struct irq_desc *desc)
 {
 	irqd_clear(&desc->irq_data, IRQD_IRQ_DISABLED);
@@ -309,6 +306,7 @@ void irq_shutdown(struct irq_desc *desc)
 			desc->irq_data.chip->irq_shutdown(&desc->irq_data);
 			irq_state_set_disabled(desc);
 			irq_state_set_masked(desc);
+			irq_pipeline_clear(desc);
 		} else {
 			__irq_disable(desc, true);
 		}
@@ -358,6 +356,7 @@ static void __irq_disable(struct irq_desc *desc, bool mask)
 			mask_irq(desc);
 		}
 	}
+	irq_pipeline_clear(desc);
 }
 
 /**
@@ -382,7 +381,8 @@ static void __irq_disable(struct irq_desc *desc, bool mask)
  */
 void irq_disable(struct irq_desc *desc)
 {
-	__irq_disable(desc, irq_settings_disable_unlazy(desc));
+	__irq_disable(desc,
+	      irq_settings_disable_unlazy(desc) || irqs_pipelined());
 }
 
 void irq_percpu_enable(struct irq_desc *desc, unsigned int cpu)
@@ -401,6 +401,7 @@ void irq_percpu_disable(struct irq_desc *desc, unsigned int cpu)
 	else
 		desc->irq_data.chip->irq_mask(&desc->irq_data);
 	cpumask_clear_cpu(cpu, desc->percpu_enabled);
+	irq_pipeline_clear(desc);
 }
 
 static inline void mask_ack_irq(struct irq_desc *desc)
@@ -514,8 +515,22 @@ static bool irq_may_run(struct irq_desc *desc)
 	 * If the interrupt is an armed wakeup source, mark it pending
 	 * and suspended, disable it and notify the pm core about the
 	 * event.
+	 *
+	 * When pipelining, the logic is as follows:
+	 *
+	 * - from a pipeline entry context, we might have preempted
+	 * the oob stage, or irqs might be [virtually] off, so we may
+	 * not run the in-band PM code. Just make sure any wakeup
+	 * interrupt is detected later on when the flow handler
+	 * re-runs from the in-band stage.
+	 *
+	 * - from the in-band context, run the PM wakeup check.
 	 */
-	if (irq_pm_check_wakeup(desc))
+	if (irqs_pipelined()) {
+		WARN_ON_ONCE(irq_pipeline_debug() && !in_pipeline());
+		if (irqd_is_wakeup_armed(&desc->irq_data))
+			return true;
+	} else if (irq_pm_check_wakeup(desc))
 		return false;
 
 	/*
@@ -539,9 +554,14 @@ void handle_simple_irq(struct irq_desc *desc)
 {
 	raw_spin_lock(&desc->lock);
 
-	if (!irq_may_run(desc))
+	if (start_irq_flow() && !irq_may_run(desc))
 		goto out_unlock;
 
+	if (on_pipeline_entry()) {
+		handle_oob_irq(desc);
+		goto out_unlock;
+	}
+
 	desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);
 
 	if (unlikely(!desc->action || irqd_irq_disabled(&desc->irq_data))) {
@@ -576,8 +596,13 @@ void handle_untracked_irq(struct irq_desc *desc)
 
 	raw_spin_lock(&desc->lock);
 
-	if (!irq_may_run(desc))
+	if (start_irq_flow() && !irq_may_run(desc))
+		goto out_unlock;
+
+	if (on_pipeline_entry()) {
+		handle_oob_irq(desc);
 		goto out_unlock;
+	}
 
 	desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);
 
@@ -600,6 +625,20 @@ void handle_untracked_irq(struct irq_desc *desc)
 }
 EXPORT_SYMBOL_GPL(handle_untracked_irq);
 
+static inline void cond_eoi_irq(struct irq_desc *desc)
+{
+	struct irq_chip *chip = desc->irq_data.chip;
+
+	if (!(chip->flags & IRQCHIP_EOI_THREADED))
+		chip->irq_eoi(&desc->irq_data);
+}
+
+static inline void mask_cond_eoi_irq(struct irq_desc *desc)
+{
+	mask_irq(desc);
+	cond_eoi_irq(desc);
+}
+
 /*
  * Called unconditionally from handle_level_irq() and only for oneshot
  * interrupts from handle_fasteoi_irq()
@@ -630,10 +669,19 @@ static void cond_unmask_irq(struct irq_desc *desc)
 void handle_level_irq(struct irq_desc *desc)
 {
 	raw_spin_lock(&desc->lock);
-	mask_ack_irq(desc);
 
-	if (!irq_may_run(desc))
+	if (start_irq_flow()) {
+		mask_ack_irq(desc);
+
+		if (!irq_may_run(desc))
+			goto out_unlock;
+	}
+
+	if (on_pipeline_entry()) {
+		if (handle_oob_irq(desc))
+			goto out_unmask;
 		goto out_unlock;
+	}
 
 	desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);
 
@@ -648,7 +696,7 @@ void handle_level_irq(struct irq_desc *desc)
 
 	kstat_incr_irqs_this_cpu(desc);
 	handle_irq_event(desc);
-
+out_unmask:
 	cond_unmask_irq(desc);
 
 out_unlock:
@@ -669,7 +717,10 @@ static inline void preflow_handler(struct irq_desc *desc) { }
 static void cond_unmask_eoi_irq(struct irq_desc *desc, struct irq_chip *chip)
 {
 	if (!(desc->istate & IRQS_ONESHOT)) {
-		chip->irq_eoi(&desc->irq_data);
+		if (!irqs_pipelined())
+			chip->irq_eoi(&desc->irq_data);
+		else if (!irqd_irq_disabled(&desc->irq_data))
+			unmask_irq(desc);
 		return;
 	}
 	/*
@@ -680,9 +731,11 @@ static void cond_unmask_eoi_irq(struct irq_desc *desc, struct irq_chip *chip)
 	 */
 	if (!irqd_irq_disabled(&desc->irq_data) &&
 	    irqd_irq_masked(&desc->irq_data) && !desc->threads_oneshot) {
-		chip->irq_eoi(&desc->irq_data);
+		if (!irqs_pipelined())
+			chip->irq_eoi(&desc->irq_data);
 		unmask_irq(desc);
-	} else if (!(chip->flags & IRQCHIP_EOI_THREADED)) {
+	} else if (!irqs_pipelined() &&
+		   !(chip->flags & IRQCHIP_EOI_THREADED)) {
 		chip->irq_eoi(&desc->irq_data);
 	}
 }
@@ -702,9 +755,17 @@ void handle_fasteoi_irq(struct irq_desc *desc)
 
 	raw_spin_lock(&desc->lock);
 
-	if (!irq_may_run(desc))
+	if (start_irq_flow() && !irq_may_run(desc))
 		goto out;
 
+	if (on_pipeline_entry()) {
+		if (handle_oob_irq(desc))
+			chip->irq_eoi(&desc->irq_data);
+		else
+			mask_cond_eoi_irq(desc);
+		goto out_unlock;
+	}
+
 	desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);
 
 	/*
@@ -718,14 +779,14 @@ void handle_fasteoi_irq(struct irq_desc *desc)
 	}
 
 	kstat_incr_irqs_this_cpu(desc);
-	if (desc->istate & IRQS_ONESHOT)
+	if (!irqs_pipelined() && (desc->istate & IRQS_ONESHOT))
 		mask_irq(desc);
 
 	preflow_handler(desc);
 	handle_irq_event(desc);
 
 	cond_unmask_eoi_irq(desc, chip);
-
+out_unlock:
 	raw_spin_unlock(&desc->lock);
 	return;
 out:
@@ -785,30 +846,42 @@ EXPORT_SYMBOL_GPL(handle_fasteoi_nmi);
  */
 void handle_edge_irq(struct irq_desc *desc)
 {
+	struct irq_chip *chip = irq_desc_get_chip(desc);
+
 	raw_spin_lock(&desc->lock);
 
-	desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);
+	if (start_irq_flow()) {
+		desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);
 
-	if (!irq_may_run(desc)) {
-		desc->istate |= IRQS_PENDING;
-		mask_ack_irq(desc);
-		goto out_unlock;
+		if (!irq_may_run(desc)) {
+			desc->istate |= IRQS_PENDING;
+			mask_ack_irq(desc);
+			goto out_unlock;
+		}
+
+		/*
+		 * If its disabled or no action available then mask it
+		 * and get out of here.
+		 */
+		if (irqd_irq_disabled(&desc->irq_data) || !desc->action) {
+			desc->istate |= IRQS_PENDING;
+			mask_ack_irq(desc);
+			goto out_unlock;
+		}
 	}
 
-	/*
-	 * If its disabled or no action available then mask it and get
-	 * out of here.
-	 */
-	if (irqd_irq_disabled(&desc->irq_data) || !desc->action) {
-		desc->istate |= IRQS_PENDING;
-		mask_ack_irq(desc);
+	if (on_pipeline_entry()) {
+		chip->irq_ack(&desc->irq_data);
+		desc->istate |= IRQS_EDGE;
+		handle_oob_irq(desc);
 		goto out_unlock;
 	}
 
 	kstat_incr_irqs_this_cpu(desc);
 
 	/* Start handling the irq */
-	desc->irq_data.chip->irq_ack(&desc->irq_data);
+	if (!irqs_pipelined())
+		chip->irq_ack(&desc->irq_data);
 
 	do {
 		if (unlikely(!desc->action)) {
@@ -833,6 +906,8 @@ void handle_edge_irq(struct irq_desc *desc)
 		 !irqd_irq_disabled(&desc->irq_data));
 
 out_unlock:
+	if (on_pipeline_entry())
+		desc->istate &= ~IRQS_EDGE;
 	raw_spin_unlock(&desc->lock);
 }
 EXPORT_SYMBOL(handle_edge_irq);
@@ -851,11 +926,20 @@ void handle_edge_eoi_irq(struct irq_desc *desc)
 
 	raw_spin_lock(&desc->lock);
 
-	desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);
+	if (start_irq_flow()) {
+		desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);
 
-	if (!irq_may_run(desc)) {
-		desc->istate |= IRQS_PENDING;
-		goto out_eoi;
+		if (!irq_may_run(desc)) {
+			desc->istate |= IRQS_PENDING;
+			goto out_eoi;
+		}
+	}
+
+	if (on_pipeline_entry()) {
+		desc->istate |= IRQS_EDGE;
+		if (handle_oob_irq(desc))
+			goto out_eoi;
+		goto out;
 	}
 
 	/*
@@ -880,6 +964,9 @@ void handle_edge_eoi_irq(struct irq_desc *desc)
 
 out_eoi:
 	chip->irq_eoi(&desc->irq_data);
+out:
+	if (on_pipeline_entry())
+		desc->istate &= ~IRQS_EDGE;
 	raw_spin_unlock(&desc->lock);
 }
 #endif
@@ -893,6 +980,18 @@ void handle_edge_eoi_irq(struct irq_desc *desc)
 void handle_percpu_irq(struct irq_desc *desc)
 {
 	struct irq_chip *chip = irq_desc_get_chip(desc);
+	bool handled;
+
+	if (on_pipeline_entry()) {
+		if (chip->irq_ack)
+			chip->irq_ack(&desc->irq_data);
+		handled = handle_oob_irq(desc);
+		if (chip->irq_eoi)
+			chip->irq_eoi(&desc->irq_data);
+		if (!handled && chip->irq_mask)
+			chip->irq_mask(&desc->irq_data);
+		return;
+	}
 
 	/*
 	 * PER CPU interrupts are not serialized. Do not touch
@@ -900,13 +999,17 @@ void handle_percpu_irq(struct irq_desc *desc)
 	 */
 	__kstat_incr_irqs_this_cpu(desc);
 
-	if (chip->irq_ack)
-		chip->irq_ack(&desc->irq_data);
-
-	handle_irq_event_percpu(desc);
-
-	if (chip->irq_eoi)
-		chip->irq_eoi(&desc->irq_data);
+	if (irqs_pipelined()) {
+		handle_irq_event_percpu(desc);
+		if (chip->irq_unmask)
+			chip->irq_unmask(&desc->irq_data);
+	} else {
+		if (chip->irq_ack)
+			chip->irq_ack(&desc->irq_data);
+		handle_irq_event_percpu(desc);
+		if (chip->irq_eoi)
+			chip->irq_eoi(&desc->irq_data);
+	}
 }
 
 /**
@@ -926,6 +1029,18 @@ void handle_percpu_devid_irq(struct irq_desc *desc)
 	struct irqaction *action = desc->action;
 	unsigned int irq = irq_desc_get_irq(desc);
 	irqreturn_t res;
+	bool handled;
+
+	if (on_pipeline_entry()) {
+		if (chip->irq_ack)
+			chip->irq_ack(&desc->irq_data);
+		handled = handle_oob_irq(desc);
+		if (chip->irq_eoi)
+			chip->irq_eoi(&desc->irq_data);
+		if (!handled && chip->irq_mask)
+			chip->irq_mask(&desc->irq_data);
+		return;
+	}
 
 	/*
 	 * PER CPU interrupts are not serialized. Do not touch
@@ -933,7 +1048,7 @@ void handle_percpu_devid_irq(struct irq_desc *desc)
 	 */
 	__kstat_incr_irqs_this_cpu(desc);
 
-	if (chip->irq_ack)
+	if (!irqs_pipelined() && chip->irq_ack)
 		chip->irq_ack(&desc->irq_data);
 
 	if (likely(action)) {
@@ -951,8 +1066,11 @@ void handle_percpu_devid_irq(struct irq_desc *desc)
 			    enabled ? " and unmasked" : "", irq, cpu);
 	}
 
-	if (chip->irq_eoi)
-		chip->irq_eoi(&desc->irq_data);
+	if (irqs_pipelined()) {
+		if (chip->irq_unmask)
+			chip->irq_unmask(&desc->irq_data);
+	} else if (chip->irq_eoi)
+			chip->irq_eoi(&desc->irq_data);
 }
 
 /**
@@ -1042,6 +1160,7 @@ __irq_do_set_handler(struct irq_desc *desc, irq_flow_handler_t handle,
 			desc->handle_irq = handle;
 		}
 
+		irq_settings_set_chained(desc);
 		irq_settings_set_noprobe(desc);
 		irq_settings_set_norequest(desc);
 		irq_settings_set_nothread(desc);
@@ -1209,9 +1328,18 @@ void handle_fasteoi_ack_irq(struct irq_desc *desc)
 
 	raw_spin_lock(&desc->lock);
 
-	if (!irq_may_run(desc))
+	if (start_irq_flow() && !irq_may_run(desc))
 		goto out;
 
+	if (on_pipeline_entry()) {
+		chip->irq_ack(&desc->irq_data);
+		if (handle_oob_irq(desc))
+			chip->irq_eoi(&desc->irq_data);
+		else
+			mask_cond_eoi_irq(desc);
+		goto out_unlock;
+	}
+
 	desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);
 
 	/*
@@ -1225,11 +1353,13 @@ void handle_fasteoi_ack_irq(struct irq_desc *desc)
 	}
 
 	kstat_incr_irqs_this_cpu(desc);
-	if (desc->istate & IRQS_ONESHOT)
-		mask_irq(desc);
+	if (!irqs_pipelined()) {
+		if (desc->istate & IRQS_ONESHOT)
+			mask_irq(desc);
 
-	/* Start handling the irq */
-	desc->irq_data.chip->irq_ack(&desc->irq_data);
+		/* Start handling the irq */
+		chip->irq_ack(&desc->irq_data);
+	}
 
 	preflow_handler(desc);
 	handle_irq_event(desc);
@@ -1241,6 +1371,7 @@ void handle_fasteoi_ack_irq(struct irq_desc *desc)
 out:
 	if (!(chip->flags & IRQCHIP_EOI_IF_HANDLED))
 		chip->irq_eoi(&desc->irq_data);
+out_unlock:
 	raw_spin_unlock(&desc->lock);
 }
 EXPORT_SYMBOL_GPL(handle_fasteoi_ack_irq);
@@ -1260,10 +1391,21 @@ void handle_fasteoi_mask_irq(struct irq_desc *desc)
 	struct irq_chip *chip = desc->irq_data.chip;
 
 	raw_spin_lock(&desc->lock);
-	mask_ack_irq(desc);
 
-	if (!irq_may_run(desc))
-		goto out;
+	if (start_irq_flow()) {
+		mask_ack_irq(desc);
+
+		if (!irq_may_run(desc))
+			goto out;
+	}
+
+	if (on_pipeline_entry()) {
+		if (handle_oob_irq(desc))
+			chip->irq_eoi(&desc->irq_data);
+		else
+			cond_eoi_irq(desc);
+		goto out_unlock;
+	}
 
 	desc->istate &= ~(IRQS_REPLAY | IRQS_WAITING);
 
@@ -1278,7 +1420,7 @@ void handle_fasteoi_mask_irq(struct irq_desc *desc)
 	}
 
 	kstat_incr_irqs_this_cpu(desc);
-	if (desc->istate & IRQS_ONESHOT)
+	if (!irqs_pipelined() && (desc->istate & IRQS_ONESHOT))
 		mask_irq(desc);
 
 	preflow_handler(desc);
@@ -1291,6 +1433,7 @@ void handle_fasteoi_mask_irq(struct irq_desc *desc)
 out:
 	if (!(chip->flags & IRQCHIP_EOI_IF_HANDLED))
 		chip->irq_eoi(&desc->irq_data);
+out_unlock:
 	raw_spin_unlock(&desc->lock);
 }
 EXPORT_SYMBOL_GPL(handle_fasteoi_mask_irq);
diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 6c7ca2e983a5..08b171a518b7 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -155,6 +155,9 @@ void irq_migrate_all_off_this_cpu(void)
 {
 	struct irq_desc *desc;
 	unsigned int irq;
+	unsigned long flags;
+
+	flags = hard_local_irq_save();
 
 	for_each_active_irq(irq) {
 		bool affinity_broken;
@@ -169,6 +172,8 @@ void irq_migrate_all_off_this_cpu(void)
 					    irq, smp_processor_id());
 		}
 	}
+
+	hard_local_irq_restore(flags);
 }
 
 static void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)
diff --git a/kernel/irq/debug.h b/kernel/irq/debug.h
index 8ccb326d2977..40f726845748 100644
--- a/kernel/irq/debug.h
+++ b/kernel/irq/debug.h
@@ -33,6 +33,8 @@ static inline void print_irq_desc(unsigned int irq, struct irq_desc *desc)
 	___P(IRQ_NOREQUEST);
 	___P(IRQ_NOTHREAD);
 	___P(IRQ_NOAUTOEN);
+	___P(IRQ_OOB);
+	___P(IRQ_CHAINED);
 
 	___PS(IRQS_AUTODETECT);
 	___PS(IRQS_REPLAY);
diff --git a/kernel/irq/dummychip.c b/kernel/irq/dummychip.c
index 0b0cdf206dc4..7bf8cbee1b87 100644
--- a/kernel/irq/dummychip.c
+++ b/kernel/irq/dummychip.c
@@ -43,7 +43,7 @@ struct irq_chip no_irq_chip = {
 	.irq_enable	= noop,
 	.irq_disable	= noop,
 	.irq_ack	= ack_bad,
-	.flags		= IRQCHIP_SKIP_SET_WAKE,
+	.flags		= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 
 /*
@@ -59,6 +59,6 @@ struct irq_chip dummy_irq_chip = {
 	.irq_ack	= noop,
 	.irq_mask	= noop,
 	.irq_unmask	= noop,
-	.flags		= IRQCHIP_SKIP_SET_WAKE,
+	.flags		= IRQCHIP_SKIP_SET_WAKE | IRQCHIP_PIPELINE_SAFE,
 };
 EXPORT_SYMBOL_GPL(dummy_irq_chip);
diff --git a/kernel/irq/handle.c b/kernel/irq/handle.c
index a4ace611f47f..e3f249fef88d 100644
--- a/kernel/irq/handle.c
+++ b/kernel/irq/handle.c
@@ -32,9 +32,16 @@ void handle_bad_irq(struct irq_desc *desc)
 {
 	unsigned int irq = irq_desc_get_irq(desc);
 
+	/* Let the in-band stage report the issue. */
+	if (on_pipeline_entry()) {
+		ack_bad_irq(irq);
+		return;
+	}
+
 	print_irq_desc(irq, desc);
 	kstat_incr_irqs_this_cpu(desc);
-	ack_bad_irq(irq);
+	if (!irqs_pipelined())
+		ack_bad_irq(irq);
 }
 EXPORT_SYMBOL_GPL(handle_bad_irq);
 
diff --git a/kernel/irq/internals.h b/kernel/irq/internals.h
index 3924fbe829d4..df228c99f995 100644
--- a/kernel/irq/internals.h
+++ b/kernel/irq/internals.h
@@ -50,6 +50,7 @@ enum {
  * IRQS_PENDING			- irq is pending and replayed later
  * IRQS_SUSPENDED		- irq is suspended
  * IRQS_NMI			- irq line is used to deliver NMIs
+ * IRQS_EDGE			- irq line received an edge event
  */
 enum {
 	IRQS_AUTODETECT		= 0x00000001,
@@ -62,6 +63,7 @@ enum {
 	IRQS_SUSPENDED		= 0x00000800,
 	IRQS_TIMINGS		= 0x00001000,
 	IRQS_NMI		= 0x00002000,
+	IRQS_EDGE		= 0x00004000,
 };
 
 #include "debug.h"
diff --git a/kernel/irq/irqdesc.c b/kernel/irq/irqdesc.c
index 9be995fc3c5a..790237998823 100644
--- a/kernel/irq/irqdesc.c
+++ b/kernel/irq/irqdesc.c
@@ -16,6 +16,7 @@
 #include <linux/bitmap.h>
 #include <linux/irqdomain.h>
 #include <linux/sysfs.h>
+#include <linux/irq_pipeline.h>
 
 #include "internals.h"
 
@@ -452,6 +453,7 @@ static void free_desc(unsigned int irq)
 	 * irq_sysfs_init() as well.
 	 */
 	irq_sysfs_del(desc);
+	uncache_irq_desc(irq);
 	delete_irq_desc(irq);
 
 	/*
@@ -631,9 +633,12 @@ void irq_init_desc(unsigned int irq)
 #endif /* !CONFIG_SPARSE_IRQ */
 
 /**
- * generic_handle_irq - Invoke the handler for a particular irq
+ * generic_handle_irq - Handle a particular irq
  * @irq:	The irq number to handle
  *
+ * The handler is invoked, unless we are entering the interrupt
+ * pipeline, in which case the incoming IRQ is only scheduled for
+ * deferred delivery.
  */
 int generic_handle_irq(unsigned int irq)
 {
diff --git a/kernel/irq/irqptorture.c b/kernel/irq/irqptorture.c
new file mode 100644
index 000000000000..4ba0f28b9211
--- /dev/null
+++ b/kernel/irq/irqptorture.c
@@ -0,0 +1,253 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2017 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#include <linux/kernel.h>
+#include <linux/torture.h>
+#include <linux/printk.h>
+#include <linux/delay.h>
+#include <linux/tick.h>
+#include <linux/smp.h>
+#include <linux/cpumask.h>
+#include <linux/clockchips.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/irq_pipeline.h>
+#include <linux/stop_machine.h>
+#include <linux/irq_work.h>
+#include <linux/completion.h>
+#include <linux/slab.h>
+#include "settings.h"
+
+static void torture_event_handler(struct clock_event_device *dev)
+{
+	/*
+	 * We are running on the oob stage, in NMI-like mode. Schedule
+	 * a tick on the proxy device to satisfy the corresponding
+	 * timing request asap.
+	 */
+	tick_notify_proxy();
+}
+
+static void setup_proxy(struct clock_proxy_device *dev)
+{
+	dev->handle_oob_event = torture_event_handler;
+}
+
+static int start_tick_takeover_test(void)
+{
+	return tick_install_proxy(setup_proxy, cpu_online_mask);
+}
+
+static void stop_tick_takeover_test(void)
+{
+	tick_uninstall_proxy(cpu_online_mask);
+}
+
+struct stop_machine_p_data {
+	int origin_cpu;
+	cpumask_var_t disable_mask;
+};
+
+static int stop_machine_handler(void *arg)
+{
+	struct stop_machine_p_data *p = arg;
+	int cpu = raw_smp_processor_id();
+
+	/*
+	 * The stop_machine() handler must run with hard
+	 * IRQs off, note the current state in the result mask.
+	 */
+	if (hard_irqs_disabled())
+		cpumask_set_cpu(cpu, p->disable_mask);
+
+	if (cpu != p->origin_cpu)
+		pr_alert("irq_pipeline" TORTURE_FLAG
+			 " CPU%d responds to stop_machine()\n", cpu);
+	return 0;
+}
+
+/*
+ * We test stop_machine() as a way to validate IPI handling in a
+ * pipelined interrupt context.
+ */
+static int test_stop_machine(void)
+{
+	struct stop_machine_p_data d;
+	cpumask_var_t tmp_mask;
+	int ret = -EINVAL, cpu;
+
+	if (!zalloc_cpumask_var(&d.disable_mask, GFP_KERNEL)) {
+		WARN_ON(1);
+		return -EINVAL;
+	}
+
+	if (!alloc_cpumask_var(&tmp_mask, GFP_KERNEL)) {
+		WARN_ON(1);
+		goto fail;
+	}
+
+	d.origin_cpu = raw_smp_processor_id();
+	pr_alert("irq_pipeline" TORTURE_FLAG
+		 " CPU%d initiates stop_machine()\n",
+		 d.origin_cpu);
+
+	ret = stop_machine(stop_machine_handler, &d, cpu_online_mask);
+	WARN_ON(ret);
+	if (ret)
+		goto fail;
+
+	/*
+	 * Check whether all handlers did run with hard IRQs off. If
+	 * some of them did not, then we have a problem with the stop
+	 * IRQ delivery.
+	 */
+	cpumask_xor(tmp_mask, cpu_online_mask, d.disable_mask);
+	if (!cpumask_empty(tmp_mask)) {
+		for_each_cpu(cpu, tmp_mask)
+			pr_alert("irq_pipeline" TORTURE_FLAG
+				 " CPU%d: hard IRQs ON in stop_machine()"
+				 " handler!\n", cpu);
+	}
+
+	free_cpumask_var(tmp_mask);
+fail:
+	free_cpumask_var(d.disable_mask);
+
+	return ret;
+}
+
+static struct irq_work_tester {
+	struct irq_work work;
+	struct completion done;
+} irq_work_tester;
+
+static void irq_work_handler(struct irq_work *work)
+{
+	int cpu = raw_smp_processor_id();
+
+	if (!running_inband()) {
+		pr_alert("irq_pipeline" TORTURE_FLAG
+			 " CPU%d: irq_work handler not running on"
+			 " in-band stage?!\n", cpu);
+		return;
+	}
+
+	if (work != &irq_work_tester.work)
+		pr_alert("irq_pipeline" TORTURE_FLAG
+			 " CPU%d: irq_work handler received broken"
+			 " arg?!\n", cpu);
+	else {
+		pr_alert("irq_pipeline" TORTURE_FLAG
+			 " CPU%d: irq_work handled\n", cpu);
+		complete(&irq_work_tester.done);
+	}
+}
+
+static int trigger_oob_work(void *arg)
+{
+	int cpu = raw_smp_processor_id();
+
+	if (!running_oob()) {
+		pr_alert("irq_pipeline" TORTURE_FLAG
+			 " CPU%d: escalated request not running on"
+			 " oob stage?!\n", cpu);
+		return -EINVAL;
+	}
+
+	if ((struct irq_work_tester *)arg != &irq_work_tester) {
+		pr_alert("irq_pipeline" TORTURE_FLAG
+			 " CPU%d: escalation handler received broken"
+			 " arg?!\n", cpu);
+		return -EINVAL;
+	}
+
+	irq_work_queue(&irq_work_tester.work);
+	pr_alert("irq_pipeline" TORTURE_FLAG
+		 " CPU%d: stage escalation request works\n",
+		 cpu);
+
+	return 0;
+}
+
+static int test_interstage_work_injection(void)
+{
+	struct irq_work_tester *p = &irq_work_tester;
+	int ret, cpu = raw_smp_processor_id();
+	unsigned long rem;
+
+	init_completion(&p->done);
+	init_irq_work(&p->work, irq_work_handler);
+
+	/* Trigger over the in-band stage. */
+	irq_work_queue(&p->work);
+	rem = wait_for_completion_timeout(&p->done, HZ / 10);
+	if (!rem) {
+		pr_alert("irq_pipeline" TORTURE_FLAG
+			 " CPU%d: irq_work trigger from in-band stage not handled!\n",
+			 cpu);
+		return -EINVAL;
+	}
+
+	pr_alert("irq_pipeline" TORTURE_FLAG
+		 " CPU%d: in-band->in-band irq_work trigger works\n", cpu);
+
+	reinit_completion(&p->done);
+
+	/* Now try over the oob stage. */
+	ret = run_oob_call(trigger_oob_work, p);
+	if (ret)
+		return ret;
+
+	ret = wait_for_completion_timeout(&p->done, HZ / 10);
+	if (!rem) {
+		pr_alert("irq_pipeline" TORTURE_FLAG
+			 " CPU%d: irq_work trigger from oob"
+			 " stage not handled!\n", cpu);
+		return -EINVAL;
+	}
+
+	pr_alert("irq_pipeline" TORTURE_FLAG
+		 " CPU%d: oob->in-band irq_work trigger works\n",
+		 cpu);
+
+	return 0;
+}
+
+static int __init irqp_torture_init(void)
+{
+	int ret;
+
+	pr_info("Starting IRQ pipeline tests...");
+
+	ret = enable_oob_stage("torture");
+	if (ret) {
+		if (ret == -EBUSY)
+			pr_alert("irq_pipeline" TORTURE_FLAG
+			 " won't run, oob stage '%s' is already installed",
+			 oob_stage.name);
+
+		return ret;
+	}
+
+	ret = test_stop_machine();
+	if (ret)
+		goto out;
+
+	ret = start_tick_takeover_test();
+	if (ret)
+		goto out;
+
+	ret = test_interstage_work_injection();
+	if (!ret)
+		msleep(1000);
+
+	stop_tick_takeover_test();
+out:
+	disable_oob_stage();
+	pr_info("IRQ pipeline tests %s.", ret ? "FAILED" : "OK");
+
+	return 0;
+}
+late_initcall(irqp_torture_init);
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index 1753486b440c..121fe833e837 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -10,6 +10,7 @@
 
 #include <linux/irq.h>
 #include <linux/kthread.h>
+#include <linux/kconfig.h>
 #include <linux/module.h>
 #include <linux/random.h>
 #include <linux/interrupt.h>
@@ -737,6 +738,50 @@ int irq_set_irq_wake(unsigned int irq, unsigned int on)
 }
 EXPORT_SYMBOL(irq_set_irq_wake);
 
+#ifdef CONFIG_IRQ_PIPELINE
+
+/**
+ *	irq_switch_oob - Control out-of-band setting for a registered IRQ descriptor
+ *	@irq:	interrupt to control
+ *	@on:	enable/disable pipelining
+ *
+ *	Enable/disable out-of-band handling for an IRQ. At least one
+ *	action must have been previously registered for such
+ *	interrupt.
+ *
+ *      The previously registered action(s) need(s) not bearing the
+ *      IRQF_OOB flag for the IRQ to be switched to out-of-band
+ *      handling. This call enables switching pre-installed IRQs from
+ *      in-band to out-of-band handling.
+ *
+ *      NOTE: This routine affects all action handlers sharing the
+ *      IRQ.
+ */
+int irq_switch_oob(unsigned int irq, bool on)
+{
+	struct irq_desc *desc;
+	unsigned long flags;
+	int ret = 0;
+
+	desc = irq_get_desc_lock(irq, &flags, 0);
+	if (!desc)
+		return -EINVAL;
+
+	if (!desc->action)
+		ret = -EINVAL;
+	else if (on)
+		irq_settings_set_oob(desc);
+	else
+		irq_settings_clr_oob(desc);
+
+	irq_put_desc_unlock(desc, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(irq_switch_oob);
+
+#endif /* CONFIG_IRQ_PIPELINE */
+
 /*
  * Internal function that tells the architecture code whether a
  * particular irq has been exclusively allocated or is available
@@ -753,7 +798,8 @@ int can_request_irq(unsigned int irq, unsigned long irqflags)
 
 	if (irq_settings_can_request(desc)) {
 		if (!desc->action ||
-		    irqflags & desc->action->flags & IRQF_SHARED)
+		    ((irqflags & desc->action->flags & IRQF_SHARED) &&
+		     !((irqflags ^ desc->action->flags) & IRQF_OOB)))
 			canrequest = 1;
 	}
 	irq_put_desc_unlock(desc, flags);
@@ -1300,6 +1346,21 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 
 	new->irq = irq;
 
+	ret = -EINVAL;
+	/*
+	 *  Out-of-band interrupts can be shared but not threaded.  We
+	 *  silently ignore the OOB setting if interrupt pipelining is
+	 *  disabled.
+	 */
+	if (!irqs_pipelined())
+		new->flags &= ~IRQF_OOB;
+	else if (new->flags & IRQF_OOB) {
+		if (new->thread_fn)
+			goto out_mput;
+		new->flags |= IRQF_NO_THREAD;
+		new->flags &= ~IRQF_ONESHOT;
+	}
+
 	/*
 	 * If the trigger type is not specified by the caller,
 	 * then use the default for this interrupt.
@@ -1313,10 +1374,8 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 	 */
 	nested = irq_settings_is_nested_thread(desc);
 	if (nested) {
-		if (!new->thread_fn) {
-			ret = -EINVAL;
+		if (!new->thread_fn)
 			goto out_mput;
-		}
 		/*
 		 * Replace the primary handler which was provided from
 		 * the driver for non nested interrupt handling by the
@@ -1400,7 +1459,7 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 		 * the same type (level, edge, polarity). So both flag
 		 * fields must have IRQF_SHARED set and the bits which
 		 * set the trigger type must match. Also all must
-		 * agree on ONESHOT.
+		 * agree on ONESHOT and OOB.
 		 * Interrupt lines used for NMIs cannot be shared.
 		 */
 		unsigned int oldtype;
@@ -1425,7 +1484,7 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 
 		if (!((old->flags & new->flags) & IRQF_SHARED) ||
 		    (oldtype != (new->flags & IRQF_TRIGGER_MASK)) ||
-		    ((old->flags ^ new->flags) & IRQF_ONESHOT))
+		    ((old->flags ^ new->flags) & (IRQF_OOB|IRQF_ONESHOT)))
 			goto mismatch;
 
 		/* All handlers must agree on per-cpuness */
@@ -1545,6 +1604,9 @@ __setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
 		if (new->flags & IRQF_ONESHOT)
 			desc->istate |= IRQS_ONESHOT;
 
+		if (new->flags & IRQF_OOB)
+			irq_settings_set_oob(desc);
+
 		/* Exclude IRQ from balancing if requested */
 		if (new->flags & IRQF_NOBALANCING) {
 			irq_settings_set_no_balancing(desc);
@@ -1726,6 +1788,8 @@ static struct irqaction *__free_irq(struct irq_desc *desc, void *dev_id)
 		irq_settings_clr_disable_unlazy(desc);
 		/* Only shutdown. Deactivate after synchronize_hardirq() */
 		irq_shutdown(desc);
+		/* Turn off OOB handling (after shutdown). */
+		irq_settings_clr_oob(desc);
 	}
 
 #ifdef CONFIG_SMP
@@ -1762,14 +1826,15 @@ static struct irqaction *__free_irq(struct irq_desc *desc, void *dev_id)
 
 #ifdef CONFIG_DEBUG_SHIRQ
 	/*
-	 * It's a shared IRQ -- the driver ought to be prepared for an IRQ
-	 * event to happen even now it's being freed, so let's make sure that
-	 * is so by doing an extra call to the handler ....
+	 * It's a shared IRQ (with in-band handler) -- the driver
+	 * ought to be prepared for an IRQ event to happen even now
+	 * it's being freed, so let's make sure that is so by doing an
+	 * extra call to the handler ....
 	 *
 	 * ( We do this after actually deregistering it, to make sure that a
 	 *   'real' IRQ doesn't run in parallel with our fake. )
 	 */
-	if (action->flags & IRQF_SHARED) {
+	if ((action->flags & (IRQF_SHARED|IRQF_OOB)) == IRQF_SHARED) {
 		local_irq_save(flags);
 		action->handler(irq, dev_id);
 		local_irq_restore(flags);
@@ -2406,7 +2471,7 @@ int setup_percpu_irq(unsigned int irq, struct irqaction *act)
  *	__request_percpu_irq - allocate a percpu interrupt line
  *	@irq: Interrupt line to allocate
  *	@handler: Function to be called when the IRQ occurs.
- *	@flags: Interrupt type flags (IRQF_TIMER only)
+ *	@flags: Interrupt type flags (IRQF_TIMER and/or IRQF_OOB only)
  *	@devname: An ascii name for the claiming device
  *	@dev_id: A percpu cookie passed back to the handler function
  *
@@ -2435,7 +2500,7 @@ int __request_percpu_irq(unsigned int irq, irq_handler_t handler,
 	    !irq_settings_is_per_cpu_devid(desc))
 		return -EINVAL;
 
-	if (flags && flags != IRQF_TIMER)
+	if (flags & ~(IRQF_TIMER|IRQF_OOB))
 		return -EINVAL;
 
 	action = kzalloc(sizeof(struct irqaction), GFP_KERNEL);
diff --git a/kernel/irq/msi.c b/kernel/irq/msi.c
index ad26fbcfbfc8..c30b3faa828a 100644
--- a/kernel/irq/msi.c
+++ b/kernel/irq/msi.c
@@ -268,6 +268,9 @@ static void msi_domain_update_chip_ops(struct msi_domain_info *info)
 	struct irq_chip *chip = info->chip;
 
 	BUG_ON(!chip || !chip->irq_mask || !chip->irq_unmask);
+	WARN_ONCE(IS_ENABLED(CONFIG_IRQ_PIPELINE) &&
+		  (chip->flags & IRQCHIP_PIPELINE_SAFE) == 0,
+		  "MSI domain irqchip %s is not pipeline-safe!", chip->name);
 	if (!chip->irq_set_affinity)
 		chip->irq_set_affinity = msi_domain_set_affinity;
 }
diff --git a/kernel/irq/pipeline.c b/kernel/irq/pipeline.c
new file mode 100644
index 000000000000..4dcff9c50adc
--- /dev/null
+++ b/kernel/irq/pipeline.c
@@ -0,0 +1,1648 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/irqdomain.h>
+#include <linux/irq_pipeline.h>
+#include <linux/irq_work.h>
+#include <linux/jhash.h>
+#include <linux/debug_locks.h>
+#include <dovetail/irq.h>
+#include <trace/events/irq.h>
+#include "internals.h"
+
+#ifdef CONFIG_DEBUG_IRQ_PIPELINE
+#define trace_on_debug
+#else
+#define trace_on_debug  notrace
+#endif
+
+struct irq_stage inband_stage = {
+	.name = "Linux",
+};
+EXPORT_SYMBOL_GPL(inband_stage);
+
+struct irq_stage oob_stage;
+EXPORT_SYMBOL_GPL(oob_stage);
+
+struct irq_domain *synthetic_irq_domain;
+EXPORT_SYMBOL_GPL(synthetic_irq_domain);
+
+bool irq_pipeline_oopsing;
+EXPORT_SYMBOL_GPL(irq_pipeline_oopsing);
+
+bool irq_pipeline_active;
+EXPORT_SYMBOL_GPL(irq_pipeline_active);
+
+#define IRQ_LOW_MAPSZ	DIV_ROUND_UP(IRQ_BITMAP_BITS, BITS_PER_LONG)
+
+#if IRQ_LOW_MAPSZ > BITS_PER_LONG
+/*
+ * We need a 3-level mapping. This allows us to handle up to 32k IRQ
+ * vectors on 32bit machines, 256k on 64bit ones.
+ */
+#define __IRQ_STAGE_MAP_LEVELS	3
+#define IRQ_MID_MAPSZ	DIV_ROUND_UP(IRQ_LOW_MAPSZ, BITS_PER_LONG)
+#else
+/*
+ * 2-level mapping is enough. This allows us to handle up to 1024 IRQ
+ * vectors on 32bit machines, 4096 on 64bit ones.
+ */
+#define __IRQ_STAGE_MAP_LEVELS	2
+#endif
+
+struct irq_event_map {
+#if __IRQ_STAGE_MAP_LEVELS == 3
+	unsigned long mdmap[IRQ_MID_MAPSZ];
+#endif
+	unsigned long lomap[IRQ_LOW_MAPSZ];
+};
+
+#ifdef CONFIG_SMP
+
+static struct irq_event_map bootup_irq_map __initdata;
+
+static DEFINE_PER_CPU(struct irq_event_map, irq_map_array[2]);
+
+DEFINE_PER_CPU(struct irq_pipeline_data, irq_pipeline) = {
+	.stages = {
+		[0] = {
+			.log = {
+				.map = &bootup_irq_map,
+			},
+			.stage = &inband_stage,
+			.status = (1 << STAGE_STALL_BIT),
+		},
+	},
+};
+
+#else /* !CONFIG_SMP */
+
+static struct irq_event_map inband_irq_map;
+
+static struct irq_event_map oob_irq_map;
+
+DEFINE_PER_CPU(struct irq_pipeline_data, irq_pipeline) = {
+	.stages = {
+		[0] = {
+			.log = {
+				.map = &inband_irq_map,
+			},
+			.stage = &inband_stage,
+			.status = (1 << STAGE_STALL_BIT),
+		},
+		[1] = {
+			.log = {
+				.map = &oob_irq_map,
+			},
+		},
+	},
+};
+
+#endif /* !CONFIG_SMP */
+
+EXPORT_PER_CPU_SYMBOL(irq_pipeline);
+
+static void sirq_noop(struct irq_data *data) { }
+
+/* Virtual interrupt controller for synthetic IRQs. */
+static struct irq_chip sirq_chip = {
+	.name		= "SIRQC",
+	.irq_enable	= sirq_noop,
+	.irq_disable	= sirq_noop,
+	.flags		= IRQCHIP_PIPELINE_SAFE | IRQCHIP_SKIP_SET_WAKE,
+};
+
+static int sirq_map(struct irq_domain *d, unsigned int irq,
+		    irq_hw_number_t hwirq)
+{
+	irq_set_percpu_devid(irq);
+	irq_set_chip_and_handler(irq, &sirq_chip, handle_synthetic_irq);
+
+	return 0;
+}
+
+static struct irq_domain_ops sirq_domain_ops = {
+	.map	= sirq_map,
+};
+
+#ifdef CONFIG_SPARSE_IRQ
+/*
+ * The performances of the radix tree in sparse mode are really ugly
+ * under mm stress on some hw, use a local descriptor cache to ease
+ * the pain.
+ */
+#define DESC_CACHE_SZ  128
+
+static struct irq_desc *desc_cache[DESC_CACHE_SZ] __cacheline_aligned;
+
+static inline u32 hash_irq(unsigned int irq)
+{
+	return jhash(&irq, sizeof(irq), irq) % DESC_CACHE_SZ;
+}
+
+static __always_inline
+struct irq_desc *cached_irq_to_desc(unsigned int irq)
+{
+	int hval = hash_irq(irq);
+	struct irq_desc *desc = desc_cache[hval];
+
+	if (unlikely(desc == NULL || irq_desc_get_irq(desc) != irq)) {
+		desc = irq_to_desc(irq);
+		desc_cache[hval] = desc;
+	}
+
+	return desc;
+}
+
+void uncache_irq_desc(unsigned int irq)
+{
+	int hval = hash_irq(irq);
+
+	desc_cache[hval] = NULL;
+}
+
+#else
+
+static struct irq_desc *cached_irq_to_desc(unsigned int irq)
+{
+	return irq_to_desc(irq);
+}
+
+#endif
+
+/**
+ *	handle_synthetic_irq -  synthetic irq handler
+ *	@desc:	the interrupt description structure for this irq
+ *
+ *	Handles synthetic interrupts flowing down the IRQ pipeline
+ *	with per-CPU semantics.
+ *
+ *      CAUTION: synthetic IRQs may be used to map hardware-generated
+ *      events (e.g. IPIs or traps), we must start handling them as
+ *      common interrupts.
+ */
+void handle_synthetic_irq(struct irq_desc *desc)
+{
+	unsigned int irq = irq_desc_get_irq(desc);
+	struct irqaction *action;
+	irqreturn_t ret;
+
+	if (on_pipeline_entry()) {
+		handle_oob_irq(desc);
+		return;
+	}
+
+	action = desc->action;
+	if (action == NULL) {
+		if (printk_ratelimit())
+			printk(KERN_WARNING
+			       "CPU%d: WARNING: synthetic IRQ%d has no action.\n",
+			       smp_processor_id(), irq);
+		return;
+	}
+
+	__kstat_incr_irqs_this_cpu(desc);
+	trace_irq_handler_entry(irq, action);
+	ret = action->handler(irq, action->dev_id);
+	trace_irq_handler_exit(irq, action, ret);
+}
+
+void sync_irq_stage(struct irq_stage *top)
+{
+	struct irq_stage_data *p;
+	struct irq_stage *stage;
+
+	/* We must enter over the inband stage with hardirqs off. */
+	if (irq_pipeline_debug()) {
+		WARN_ON_ONCE(!hard_irqs_disabled());
+		WARN_ON_ONCE(current_irq_stage != &inband_stage);
+	}
+
+	stage = top;
+
+	for (;;) {
+		p = this_staged(stage);
+		if (test_stage_bit(STAGE_STALL_BIT, p))
+			break;
+
+		if (stage_irqs_pending(p)) {
+			if (stage == &inband_stage)
+				sync_current_irq_stage();
+			else {
+				/* Switch to oob before synchronizing. */
+				switch_oob(p);
+				sync_current_irq_stage();
+				/* Then back to the inband stage. */
+				switch_inband(this_inband_staged());
+			}
+		}
+
+		if (stage == &inband_stage)
+			break;
+
+		stage = &inband_stage;
+	}
+}
+
+void synchronize_pipeline(void) /* hardirqs off */
+{
+	struct irq_stage *top = &oob_stage;
+
+	if (unlikely(!oob_stage_present()))
+		top = &inband_stage;
+
+	if (current_irq_stage != top)
+		sync_irq_stage(top);
+	else if (!test_stage_bit(STAGE_STALL_BIT, this_staged(top)))
+		sync_current_irq_stage();
+}
+
+trace_on_debug void __inband_irq_enable(void)
+{
+	struct irq_stage_data *p;
+	unsigned long flags;
+
+	/* This helps catching bad usage from assembly call sites. */
+	check_inband_stage();
+
+	flags = hard_local_irq_save();
+
+	p = this_inband_staged();
+	trace_hardirqs_on();
+	clear_stage_bit(STAGE_STALL_BIT, p);
+	if (unlikely(stage_irqs_pending(p))) {
+		sync_current_irq_stage();
+		hard_local_irq_restore(flags);
+		preempt_check_resched();
+	} else
+		hard_local_irq_restore(flags);
+}
+EXPORT_SYMBOL(__inband_irq_enable);
+
+/**
+ *	inband_irq_enable - enable interrupts for the inband stage
+ *
+ *	Enable interrupts for the inband stage, allowing interrupts to
+ *	preempt the in-band code. If in-band IRQs are pending for the
+ *	inband stage in the per-CPU log at the time of this call, they
+ *	are played back.
+ */
+notrace void inband_irq_enable(void)
+{
+	/*
+	 * We are NOT supposed to enter this code with hard IRQs off.
+	 * If we do, then the caller might be wrongly assuming that
+	 * invoking local_irq_enable() implies enabling hard
+	 * interrupts like the legacy I-pipe did, which is not the
+	 * case anymore. Relax this requirement when oopsing, since
+	 * the kernel may be in a weird state.
+	 */
+	WARN_ON_ONCE(irq_pipeline_debug() && hard_irqs_disabled());
+	__inband_irq_enable();
+}
+EXPORT_SYMBOL(inband_irq_enable);
+
+/**
+ *	inband_irq_disable - disable interrupts for the inband stage
+ *
+ *	Disable interrupts for the inband stage, disabling in-band
+ *	interrupts. Out-of-band interrupts can still be taken and
+ *	delivered to their respective handlers though.
+ */
+trace_on_debug void inband_irq_disable(void)
+{
+	unsigned long flags;
+
+	check_inband_stage();
+	flags = hard_local_irq_save();
+	set_stage_bit(STAGE_STALL_BIT, this_inband_staged());
+	trace_hardirqs_off();
+	hard_local_irq_restore(flags);
+}
+EXPORT_SYMBOL(inband_irq_disable);
+
+/**
+ *	inband_irqs_disabled - test the virtual interrupt state
+ *
+ *	Returns non-zero if interrupts are currently disabled for the
+ *	inband stage, zero otherwise.
+ *
+ *	May be used from the oob stage too (e.g. for tracing
+ *	purpose).
+ */
+notrace unsigned long inband_irqs_disabled(void)
+{
+	unsigned long flags;
+	int ret;
+
+	flags = hard_smp_local_irq_save();
+	ret =  __test_stage_bit(STAGE_STALL_BIT, this_inband_staged());
+	hard_smp_local_irq_restore(flags);
+
+	return ret;
+}
+EXPORT_SYMBOL(inband_irqs_disabled);
+
+/**
+ *	inband_irq_save - test and disable (virtual) interrupts
+ *
+ *	Save the virtual interrupt state then disables interrupts for
+ *	the inband stage.
+ *
+ *      Returns the original interrupt state.
+ */
+trace_on_debug unsigned long inband_irq_save(void)
+{
+	unsigned long flags, x;
+
+	check_inband_stage();
+	flags = hard_local_irq_save();
+	x = test_and_set_stage_bit(STAGE_STALL_BIT, this_inband_staged());
+	trace_hardirqs_off();
+	hard_local_irq_restore(flags);
+
+	return x;
+}
+EXPORT_SYMBOL(inband_irq_save);
+
+/**
+ *	inband_irq_restore - restore the (virtual) interrupt state
+ *      @x:	Interrupt state to restore
+ *
+ *	Restore the virtual interrupt state from x. If the inband
+ *	stage is unstalled as a consequence of this operation, any
+ *	interrupt pending for the inband stage in the per-CPU log is
+ *	played back.
+ */
+trace_on_debug void inband_irq_restore(unsigned long flags)
+{
+	if (flags)
+		inband_irq_disable();
+	else
+		__inband_irq_enable();
+}
+EXPORT_SYMBOL(inband_irq_restore);
+
+/**
+ *	inband_irq_restore_nosync - restore the (virtual) interrupt state
+ *      @x:	Interrupt state to restore
+ *
+ *	Restore the virtual interrupt state from x. Unlike
+ *	inband_irq_restore(), pending interrupts are not played back.
+ *
+ *	Hard irqs must be disabled on entry.
+ */
+trace_on_debug void inband_irq_restore_nosync(unsigned long flags)
+{
+	struct irq_stage_data *p = this_inband_staged();
+
+	check_hard_irqs_disabled();
+
+	if (raw_irqs_disabled_flags(flags)) {
+		set_stage_bit(STAGE_STALL_BIT, p);
+		trace_hardirqs_off();
+	} else {
+		trace_hardirqs_on();
+		clear_stage_bit(STAGE_STALL_BIT, p);
+	}
+}
+
+/**
+ *	oob_irq_enable - enable interrupts in the CPU
+ *
+ *	Enable interrupts in the CPU, allowing out-of-band interrupts
+ *	to preempt any code. If out-of-band IRQs are pending in the
+ *	per-CPU log for the oob stage at the time of this call, they
+ *	are played back.
+ */
+trace_on_debug void oob_irq_enable(void)
+{
+	struct irq_stage_data *p;
+
+	hard_local_irq_disable();
+
+	p = this_oob_staged();
+	clear_stage_bit(STAGE_STALL_BIT, p);
+
+	if (unlikely(stage_irqs_pending(p)))
+		synchronize_pipeline();
+
+	hard_local_irq_enable();
+}
+EXPORT_SYMBOL(oob_irq_enable);
+
+/**
+ *	oob_irq_restore - restore the hardware interrupt state
+ *      @x:	Interrupt state to restore
+ *
+ *	Restore the harware interrupt state from x. If the oob stage
+ *	is unstalled as a consequence of this operation, any interrupt
+ *	pending for the oob stage in the per-CPU log is played back
+ *	prior to turning IRQs on.
+ *
+ *      NOTE: Stalling the oob stage must always be paired with
+ *      disabling hard irqs and conversely when calling
+ *      oob_irq_restore(), otherwise the latter would badly misbehave
+ *      in unbalanced conditions.
+ */
+trace_on_debug void __oob_irq_restore(unsigned long flags) /* hw interrupt off */
+{
+	struct irq_stage_data *p = this_oob_staged();
+
+	check_hard_irqs_disabled();
+
+	if (!flags) {
+		clear_stage_bit(STAGE_STALL_BIT, p);
+		if (unlikely(stage_irqs_pending(p)))
+			synchronize_pipeline();
+		hard_local_irq_enable();
+	}
+}
+EXPORT_SYMBOL(__oob_irq_restore);
+
+/**
+ *	stage_disabled - test the interrupt state of the current stage
+ *
+ *	Returns non-zero if interrupts are currently disabled for the
+ *	current interrupt stage, zero otherwise.
+ *      In other words, returns non-zero either if:
+ *      - interrupts are disabled for the OOB context (i.e. hard disabled),
+ *      - the inband stage is current and inband interrupts are disabled.
+ */
+notrace bool stage_disabled(void)
+{
+	bool ret = true;
+
+	if (!hard_irqs_disabled()) {
+		ret = false;
+		if (running_inband())
+			ret = inband_irqs_disabled();
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(stage_disabled);
+
+/**
+ *	test_and_disable_stage - test and disable interrupts for
+ *                                   the current stage
+ *	@irqsoff:	Pointer to boolean denoting stage_disabled()
+ *                      on entry
+ *
+ *	Fully disables interrupts for the current stage. When the
+ *	inband stage is current, the stall bit is raised and hardware
+ *	IRQs are masked as well. Only the latter operation is
+ *	performed when the oob stage is current.
+ *
+ *      Returns the combined interrupt state on entry including the
+ *      real/hardware (in CPU) and virtual (inband stage) states. For
+ *      this reason, irq_stage_[test_and_]disable() must be paired
+ *      with restore_stage() exclusively. The combo state returned by
+ *      the former may NOT be passed to hard_local_irq_restore().
+ *
+ *      The interrupt state of the current stage in the return value
+ *      (i.e. stall bit for the inband stage, hardware interrupt bit
+ *      for the oob stage) must be testable using
+ *      arch_irqs_disabled_flags().
+ */
+trace_on_debug unsigned long test_and_disable_stage(int *irqsoff)
+{
+	unsigned long flags;
+	int stalled, dummy;
+
+	if (irqsoff == NULL)
+		irqsoff = &dummy;
+
+	/*
+	 * Forge flags combining the hardware and virtual IRQ
+	 * states. We need to fill in the virtual state only if the
+	 * inband stage is current, otherwise it is not relevant.
+	 */
+	flags = hard_local_irq_save();
+	*irqsoff = hard_irqs_disabled_flags(flags);
+	if (running_inband()) {
+		stalled = test_and_set_stage_bit(STAGE_STALL_BIT,
+						 this_inband_staged());
+		if (!irq_pipeline_debug_locking()) {
+			trace_hardirqs_off();
+			if (!*irqsoff)
+				hard_local_irq_enable();
+		}
+		flags = irqs_merge_flags(flags, stalled);
+		if (stalled)
+			*irqsoff = 1;
+	}
+
+	/*
+	 * CAUTION: don't ever pass this verbatim to
+	 * hard_local_irq_restore(). Only restore_stage() knows how to
+	 * decode and use a combo state word.
+	 */
+	return flags;
+}
+EXPORT_SYMBOL_GPL(test_and_disable_stage);
+
+/**
+ *	restore_stage - restore interrupts for the current stage
+ *	@flags: 	Combined interrupt state to restore as received from
+ *              	test_and_disable_stage()
+ *
+ *	Restore the virtual interrupt state if the inband stage is
+ *      current, and the hardware interrupt state unconditionally.
+ *      The per-CPU log is not played for any stage.
+ */
+trace_on_debug void restore_stage(unsigned long combo)
+{
+	unsigned long flags = combo;
+	int stalled;
+
+	WARN_ON_ONCE(irq_pipeline_debug_locking() && !hard_irqs_disabled());
+
+	if (running_inband()) {
+		flags = irqs_split_flags(combo, &stalled);
+		if (!stalled) {
+			if (!irq_pipeline_debug_locking()) {
+				hard_local_irq_disable();
+				trace_hardirqs_on();
+			}
+			clear_stage_bit(STAGE_STALL_BIT,
+					this_inband_staged());
+		}
+	}
+
+	/*
+	 * The hardware interrupt bit is the only flag which may be
+	 * present in the combo state at this point, all other status
+	 * bits have been cleared by irqs_merge_flags(), so don't ever
+	 * try to reload the hardware status register with such value
+	 * directly!
+	 */
+	if (!hard_irqs_disabled_flags(flags))
+		hard_local_irq_enable();
+}
+EXPORT_SYMBOL_GPL(restore_stage);
+
+#if __IRQ_STAGE_MAP_LEVELS == 3
+
+/* Must be called hw IRQs off. */
+void irq_post_stage(struct irq_stage *stage, unsigned int irq)
+{
+	struct irq_stage_data *p = this_staged(stage);
+	int l0b, l1b;
+
+	if (WARN_ON_ONCE(irq_pipeline_debug() &&
+			 (!hard_irqs_disabled() || irq >= IRQ_BITMAP_BITS)))
+		return;
+
+	l0b = irq / (BITS_PER_LONG * BITS_PER_LONG);
+	l1b = irq / BITS_PER_LONG;
+
+	__set_bit(irq, p->log.map->lomap);
+	__set_bit(l1b, p->log.map->mdmap);
+	__set_bit(l0b, &p->log.himap);
+}
+EXPORT_SYMBOL_GPL(irq_post_stage);
+
+static void __clear_pending_irq(struct irq_stage_data *p, unsigned int irq)
+{
+	int l0b, l1b;
+
+	l0b = irq / (BITS_PER_LONG * BITS_PER_LONG);
+	l1b = irq / BITS_PER_LONG;
+
+	__clear_bit(irq, p->log.map->lomap);
+	__clear_bit(l1b, p->log.map->mdmap);
+	__clear_bit(l0b, &p->log.himap);
+}
+
+static void clear_pending_irq(struct irq_stage *stage, unsigned int irq)
+{
+	struct irq_stage_data *p = this_staged(stage);
+	__clear_pending_irq(p, irq);
+}
+
+static inline int pull_next_irq(struct irq_stage_data *p)
+{
+	unsigned long l0m, l1m, l2m;
+	int l0b, l1b, l2b, irq;
+
+	l0m = p->log.himap;
+	if (unlikely(l0m == 0))
+		return -1;
+
+	l0b = __ffs(l0m);
+	l1m = p->log.map->mdmap[l0b];
+	if (unlikely(l1m == 0))
+		return -1;
+
+	l1b = __ffs(l1m) + l0b * BITS_PER_LONG;
+	l2m = p->log.map->lomap[l1b];
+	if (unlikely(l2m == 0))
+		return -1;
+
+	l2b = __ffs(l2m);
+	irq = l1b * BITS_PER_LONG + l2b;
+
+	__clear_bit(irq, p->log.map->lomap);
+	if (p->log.map->lomap[l1b] == 0) {
+		__clear_bit(l1b, p->log.map->mdmap);
+		if (p->log.map->mdmap[l0b] == 0)
+			__clear_bit(l0b, &p->log.himap);
+	}
+
+	return irq;
+}
+
+#else /* __IRQ_STAGE_MAP_LEVELS == 2 */
+
+static void __clear_pending_irq(struct irq_stage_data *p, unsigned int irq)
+{
+	int l0b = irq / BITS_PER_LONG;
+
+	__clear_bit(irq, p->log.map->lomap);
+	__clear_bit(l0b, &p->log.himap);
+}
+
+static void clear_pending_irq(struct irq_stage *stage, unsigned int irq)
+{
+	struct irq_stage_data *p = this_staged(stage);
+	__clear_pending_irq(p, irq);
+}
+
+/* Must be called hw IRQs off. */
+void irq_post_stage(struct irq_stage *stage, unsigned int irq)
+{
+	struct irq_stage_data *p = this_staged(stage);
+	int l0b = irq / BITS_PER_LONG;
+
+	if (WARN_ON_ONCE(irq_pipeline_debug() &&
+			 (!hard_irqs_disabled() || irq >= IRQ_BITMAP_BITS)))
+		return;
+
+	__set_bit(irq, p->log.map->lomap);
+	__set_bit(l0b, &p->log.himap);
+}
+EXPORT_SYMBOL_GPL(irq_post_stage);
+
+static inline int pull_next_irq(struct irq_stage_data *p)
+{
+	unsigned long l0m, l1m;
+	int l0b, l1b;
+
+	l0m = p->log.himap;
+	if (unlikely(l0m == 0))
+		return -1;
+
+	l0b = __ffs(l0m);
+	l1m = p->log.map->lomap[l0b];
+	if (unlikely(l1m == 0))
+		return -1;
+
+	l1b = __ffs(l1m);
+	__clear_bit(l1b, &p->log.map->lomap[l0b]);
+	if (p->log.map->lomap[l0b] == 0)
+		__clear_bit(l0b, &p->log.himap);
+
+	return l0b * BITS_PER_LONG + l1b;
+}
+
+#endif  /* __IRQ_STAGE_MAP_LEVELS == 2 */
+
+/**
+ *	irq_pipeline_clear - clear IRQ event from all per-CPU logs
+ *	@desc: IRQ descriptor
+ *
+ *      Clear any event of the specified IRQ pending from the relevant
+ *      interrupt logs, for both the inband and oob stages.
+ *
+ *      All per-CPU logs are considered for device IRQs, per-CPU IRQ
+ *      events are only looked up into the log of the current CPU.
+ *
+ *      Genirq should be the exclusive user of that code. The only
+ *      safe context for running this code is when the corresponding
+ *      IRQ line is masked, and the matching IRQ descriptor locked.
+ *
+ *      Hard irqs must be off on entry (which has to be the case since
+ *      the IRQ descriptor lock is a mutable beast when pipelining).
+ */
+void irq_pipeline_clear(struct irq_desc *desc)
+{
+	unsigned int irq = irq_desc_get_irq(desc);
+	struct irq_stage_data *p;
+	int cpu;
+
+	check_hard_irqs_disabled();
+
+	if (irq_settings_is_per_cpu_devid(desc)) {
+		clear_pending_irq(&inband_stage, irq);
+		if (oob_stage_present())
+			clear_pending_irq(&oob_stage, irq);
+	} else {
+		for_each_online_cpu(cpu) {
+			p = percpu_inband_staged(&inband_stage, cpu);
+			__clear_pending_irq(p, irq);
+			if (oob_stage_present()) {
+				p = percpu_inband_staged(&oob_stage, cpu);
+				__clear_pending_irq(p, irq);
+			}
+		}
+	}
+}
+
+/**
+ *	hard_preempt_disable - Disable preemption the hard way
+ *
+ *      Disable hardware interrupts in the CPU, and disable preemption
+ *      if currently running in-band code on the inband stage.
+ *
+ *      Return the hardware interrupt state.
+ */
+unsigned long hard_preempt_disable(void)
+{
+	unsigned long flags = hard_local_irq_save();
+
+	if (running_inband())
+		preempt_disable();
+
+	return flags;
+}
+EXPORT_SYMBOL_GPL(hard_preempt_disable);
+
+/**
+ *	hard_preempt_enable - Enable preemption the hard way
+ *
+ *      Enable preemption if currently running in-band code on the
+ *      inband stage, restoring the hardware interrupt state in the CPU.
+ *      The per-CPU log is not played for the oob stage.
+ */
+void hard_preempt_enable(unsigned long flags)
+{
+	if (running_inband()) {
+		preempt_enable_no_resched();
+		hard_local_irq_restore(flags);
+		if (!hard_irqs_disabled_flags(flags))
+			preempt_check_resched();
+	} else
+		hard_local_irq_restore(flags);
+}
+EXPORT_SYMBOL_GPL(hard_preempt_enable);
+
+static void handle_unexpected_irq(struct irq_desc *desc, irqreturn_t ret)
+{
+	unsigned int irq = irq_desc_get_irq(desc);
+	struct irqaction *action;
+
+	/*
+	 * Since IRQ_HANDLED was not received from any handler, we may
+	 * have a problem dealing with an OOB interrupt. The error
+	 * detection logic is as follows:
+	 *
+	 * - check and complain about any bogus return value from a
+	 * out-of-band IRQ handler: we only allow IRQ_HANDLED and
+	 * IRQ_NONE from those routines.
+	 *
+	 * - filter out spurious IRQs which may have been due to bus
+	 * asynchronicity, those tend to happen infrequently and
+	 * should not cause us to pull the break (see
+	 * note_interrupt()).
+	 *
+	 * - otherwise, stop pipelining the IRQ line after a thousand
+	 * consecutive unhandled events.
+	 *
+	 * NOTE: we should already be holding desc->lock for non
+	 * per-cpu IRQs, since we should only get there from the
+	 * pipeline entry context.
+	 */
+
+	WARN_ON_ONCE(irq_pipeline_debug() &&
+		     !irq_settings_is_per_cpu(desc) &&
+		     !raw_spin_is_locked(&desc->lock));
+
+	if (ret != IRQ_NONE) {
+		printk(KERN_ERR "out-of-band irq event %d: bogus return value %x\n",
+		       irq, ret);
+		for_each_action_of_desc(desc, action)
+			printk(KERN_ERR "[<%p>] %pf",
+			       action->handler, action->handler);
+		printk(KERN_CONT "\n");
+		return;
+	}
+
+	if (time_after(jiffies, desc->last_unhandled + HZ/10))
+		desc->irqs_unhandled = 0;
+	else
+		desc->irqs_unhandled++;
+
+	desc->last_unhandled = jiffies;
+
+	if (unlikely(desc->irqs_unhandled > 1000)) {
+		printk(KERN_ERR "out-of-band irq %d: stuck or unexpected\n", irq);
+		irq_settings_clr_oob(desc);
+		desc->istate |= IRQS_SPURIOUS_DISABLED;
+		irq_disable(desc);
+	}
+}
+
+/*
+ * do_oob_irq() - Handles interrupts over the oob stage. Hard irqs
+ * off.
+ */
+static void do_oob_irq(struct irq_desc *desc)
+{
+	bool percpu_devid = irq_settings_is_per_cpu_devid(desc);
+	unsigned int irq = irq_desc_get_irq(desc);
+	irqreturn_t ret = IRQ_NONE, res;
+	struct irqaction *action;
+	void *dev_id;
+
+	action = desc->action;
+	if (unlikely(action == NULL))
+		goto done;
+
+	if (percpu_devid) {
+		trace_irq_handler_entry(irq, action);
+		dev_id = raw_cpu_ptr(action->percpu_dev_id);
+		ret = action->handler(irq, dev_id);
+		trace_irq_handler_exit(irq, action, ret);
+	} else {
+		desc->istate &= ~IRQS_PENDING;
+		irqd_set(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+		raw_spin_unlock(&desc->lock);
+		for_each_action_of_desc(desc, action) {
+			trace_irq_handler_entry(irq, action);
+			dev_id = action->dev_id;
+			res = action->handler(irq, dev_id);
+			trace_irq_handler_exit(irq, action, res);
+			ret |= res;
+		}
+		raw_spin_lock(&desc->lock);
+		irqd_clear(&desc->irq_data, IRQD_IRQ_INPROGRESS);
+	}
+done:
+	if (likely(ret & IRQ_HANDLED)) {
+		desc->irqs_unhandled = 0;
+		return;
+	}
+
+	handle_unexpected_irq(desc, ret);
+}
+
+/*
+ * Over the inband stage, IRQs must be dispatched by the arch-specific
+ * arch_do_IRQ_pipelined() routine.
+ *
+ * Entered with hardirqs on, inband stalled.
+ */
+static inline
+void do_inband_irq(struct irq_desc *desc)
+{
+	arch_do_IRQ_pipelined(desc);
+	WARN_ON_ONCE(irq_pipeline_debug() && !irqs_disabled());
+}
+
+static inline void incr_irq_kstat(struct irq_desc *desc)
+{
+	if (irq_settings_is_per_cpu_devid(desc))
+		__kstat_incr_irqs_this_cpu(desc);
+	else
+		kstat_incr_irqs_this_cpu(desc);
+}
+
+static inline bool is_active_edge_event(struct irq_desc *desc)
+{
+	return (desc->istate & IRQS_PENDING) &&
+		!irqd_irq_disabled(&desc->irq_data);
+}
+
+bool handle_oob_irq(struct irq_desc *desc) /* hardirqs off, oob */
+{
+	struct irq_stage_data *oobd = this_oob_staged();
+	unsigned int irq = irq_desc_get_irq(desc);
+
+	/*
+	 * Flow handlers of chained interrupts have no business
+	 * running here: they should decode the event, invoking
+	 * generic_handle_irq() for each cascaded IRQ.
+	 */
+	if (WARN_ON_ONCE(irq_pipeline_debug() &&
+			 irq_settings_is_chained(desc)))
+		return false;
+
+	/*
+	 * If no oob stage is present, all interrupts must go to the
+	 * inband stage through the interrupt log.
+	 *
+	 * Otherwise, out-of-band IRQs are immediately delivered
+	 * (dispatch_oob_irq()) to the oob stage, while in-band IRQs
+	 * still go through the inband stage log.
+	 *
+	 * This routine returns a boolean status telling the caller
+	 * whether an out-of-band interrupt was delivered.
+	 */
+	if (!oob_stage_present() || !irq_settings_is_oob(desc)) {
+		irq_post_stage(&inband_stage, irq);
+		return false;
+	}
+
+	if (WARN_ON_ONCE(irq_pipeline_debug() && running_inband()))
+		return false;
+	/*
+	 * Running with the oob stage stalled implies hardirqs off, so
+	 * we should have never gotten here for handling an external
+	 * IRQ in the first place: something is badly broken in our
+	 * interrupt state. Pretend the event has been handled, which
+	 * may end up with the device hammering us with more
+	 * interrupts, but there is no safe option at this point.
+	 */
+	if (WARN_ON_ONCE(irq_pipeline_debug() &&
+				on_pipeline_entry() &&
+				test_stage_bit(STAGE_STALL_BIT, oobd)))
+		return true;
+
+	set_stage_bit(STAGE_STALL_BIT, oobd);
+
+	if (unlikely(desc->istate & IRQS_EDGE)) {
+		do {
+			if (is_active_edge_event(desc))  {
+				if (irqd_irq_masked(&desc->irq_data))
+					unmask_irq(desc);
+			}
+			do_oob_irq(desc);
+		} while (is_active_edge_event(desc));
+	} else
+		do_oob_irq(desc);
+
+	clear_stage_bit(STAGE_STALL_BIT, oobd);
+
+	/*
+	 * CPU migration and/or stage switching over the handler are
+	 * NOT allowed. These should take place over
+	 * irq_exit_pipeline().
+	 */
+	if (irq_pipeline_debug()) {
+		/* No CPU migration allowed. */
+		WARN_ON_ONCE(this_oob_staged() != oobd);
+		/* No stage migration allowed. */
+		WARN_ON_ONCE(current_irq_staged != oobd);
+	}
+
+	return true;
+}
+
+static inline
+void copy_timer_regs(struct irq_desc *desc, struct pt_regs *regs)
+{
+	struct irq_pipeline_data *p;
+
+	if (desc->action == NULL || !(desc->action->flags & __IRQF_TIMER))
+		return;
+	/*
+	 * Given our deferred dispatching model for regular IRQs, we
+	 * record the preempted context registers only for the latest
+	 * timer interrupt, so that the regular tick handler charges
+	 * CPU times properly. It is assumed that no other interrupt
+	 * handler cares for such information.
+	 */
+	p = raw_cpu_ptr(&irq_pipeline);
+	arch_save_timer_regs(&p->tick_regs, regs, running_oob());
+}
+
+static __always_inline
+struct irq_stage_data *switch_stage_on_irq(void)
+{
+	struct irq_stage_data *prevd = current_irq_staged, *nextd;
+
+	if (oob_stage_present()) {
+		nextd = this_oob_staged();
+		if (prevd != nextd)
+			switch_oob(nextd);
+	}
+
+	return prevd;
+}
+
+static __always_inline
+void restore_stage_on_irq(struct irq_stage_data *prevd)
+{
+	/*
+	 * CPU migration and/or stage switching over
+	 * irq_exit_pipeline() are allowed.  Our exit logic is as
+	 * follows:
+	 *
+	 *    ENTRY      EXIT      EPILOGUE
+	 *
+	 *    oob        oob       nop
+	 *    inband     oob       switch inband
+	 *    oob        inband    nop
+	 *    inband     inband    nop
+	 */
+	if (prevd->stage == &inband_stage &&
+		current_irq_staged == this_oob_staged())
+		switch_inband(this_inband_staged());
+}
+
+/**
+ *	generic_pipeline_irq - Pass an IRQ to the pipeline
+ *	@irq:	IRQ to pass
+ *	@regs:	Register file coming from the low-level handling code
+ *
+ *	Inject an IRQ into the pipeline from a CPU interrupt or trap
+ *	context.  A flow handler runs for this IRQ.
+ *
+ *      Hard irqs must be off on entry.
+ */
+int generic_pipeline_irq(unsigned int irq, struct pt_regs *regs)
+{
+	struct pt_regs *old_regs;
+	struct irq_desc *desc;
+	int ret = 0;
+
+	trace_irq_pipeline_entry(irq);
+
+	old_regs = set_irq_regs(regs);
+	desc = cached_irq_to_desc(irq);
+
+	if (irq_pipeline_debug()) {
+		if (!hard_irqs_disabled()) {
+			hard_local_irq_disable();
+			pr_err("IRQ pipeline: interrupts enabled on entry (IRQ%u)\n",
+			       irq);
+		}
+		if (unlikely(desc == NULL)) {
+			pr_err("IRQ pipeline: received unhandled IRQ%u\n",
+			       irq);
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	/*
+	 * We may re-enter this routine either legitimately due to
+	 * stacked IRQ domains, or because some chained IRQ handler is
+	 * abusing the API, and should have called
+	 * generic_handle_irq() instead of us. In any case, deal with
+	 * re-entry gracefully.
+	 */
+	if (unlikely(on_pipeline_entry())) {
+		if (WARN_ON_ONCE(irq_pipeline_debug() &&
+				 irq_settings_is_chained(desc)))
+			generic_handle_irq_desc(desc);
+		goto out;
+	}
+
+	/*
+	 * We switch eagerly to the oob stage if present, so that a
+	 * companion kernel readily runs on the right stage when we
+	 * call its out-of-band IRQ handler from handle_oob_irq(),
+	 * then irq_exit_pipeline() to unwind the interrupt context.
+	 */
+	copy_timer_regs(desc, regs);
+	preempt_count_add(PIPELINE_OFFSET);
+	generic_handle_irq_desc(desc);
+	preempt_count_sub(PIPELINE_OFFSET);
+	incr_irq_kstat(desc);
+out:
+	set_irq_regs(old_regs);
+	trace_irq_pipeline_exit(irq);
+
+	return ret;
+}
+
+int handle_irq_pipelined(struct pt_regs *regs)
+{
+	struct irq_stage_data *prevd;
+
+	prevd = switch_stage_on_irq();
+	irq_enter_pipeline();
+	handle_arch_irq(regs);
+	irq_exit_pipeline();
+	restore_stage_on_irq(prevd);
+	/*
+	 * We have to synchronize the logs because interrupts might
+	 * have been logged while we were busy handling an OOB event
+	 * coming from the hardware:
+	 *
+	 * - as a result of calling an OOB handler which in turned
+	 * posted them.
+	 *
+	 * - because we posted them directly for scheduling the
+	 * interrupt to happen from the inband stage.
+	 *
+	 * This also means that hardware-originated OOB events have
+	 * higher precedence when received than software-originated
+	 * ones, which are synced once all IRQ flow handlers involved
+	 * in the interrupt have run.
+	 */
+	synchronize_pipeline_on_irq();
+
+#ifdef CONFIG_DOVETAIL
+	/*
+	 * Sending MAYDAY is in essence a rare case, so prefer test
+	 * then maybe clear over test_and_clear.
+	 */
+	if (user_mode(regs) && test_thread_flag(TIF_MAYDAY))
+		dovetail_call_mayday(current_thread_info(), regs);
+#endif
+
+	return running_inband() && !irqs_disabled();
+}
+
+/**
+ *	irq_inject_pipeline - Inject a software-generated IRQ into the
+ *	pipeline @irq: IRQ to inject
+ *
+ *	Inject an IRQ into the pipeline by software as if such
+ *	hardware event had happened on the current CPU.
+ */
+int irq_inject_pipeline(unsigned int irq)
+{
+	struct irq_stage_data *oobd, *prevd;
+	struct irq_desc *desc;
+	unsigned long flags;
+
+	desc = cached_irq_to_desc(irq);
+	if (desc == NULL)
+		return -EINVAL;
+
+	flags = hard_local_irq_save();
+
+	/*
+	 * Handle the case of an IRQ sent to a stalled oob stage here,
+	 * which allows to trap the same condition in handle_oob_irq()
+	 * in a debug check (see comment there).
+	 */
+	oobd = this_oob_staged();
+	if (oob_stage_present() &&
+		irq_settings_is_oob(desc) &&
+		test_stage_bit(STAGE_STALL_BIT, oobd)) {
+		irq_post_stage(&oob_stage, irq);
+	} else {
+		prevd = switch_stage_on_irq();
+		irq_enter_pipeline();
+		handle_oob_irq(desc);
+		irq_exit_pipeline();
+		incr_irq_kstat(desc);
+		restore_stage_on_irq(prevd);
+		synchronize_pipeline_on_irq();
+	}
+
+	hard_local_irq_restore(flags);
+
+	return 0;
+
+}
+EXPORT_SYMBOL_GPL(irq_inject_pipeline);
+
+/*
+ * sync_current_irq_stage() -- Flush the pending IRQs for the current
+ * stage (and processor). This routine flushes the interrupt log (see
+ * "Optimistic interrupt protection" from D. Stodolsky et al. for more
+ * on the deferred interrupt scheme). Every interrupt that occurred
+ * while the pipeline was stalled gets played.
+ *
+ * CAUTION: CPU migration may occur over this routine if running over
+ * the inband stage.
+ */
+void sync_current_irq_stage(void) /* hw IRQs off */
+{
+	struct irq_stage_data *p;
+	struct irq_stage *stage;
+	struct irq_desc *desc;
+	int irq;
+
+	WARN_ON_ONCE(irq_pipeline_debug() && on_pipeline_entry());
+	check_hard_irqs_disabled();
+
+	p = current_irq_staged;
+respin:
+	stage = p->stage;
+	set_stage_bit(STAGE_STALL_BIT, p);
+	smp_wmb();
+
+	if (stage == &inband_stage)
+		trace_hardirqs_off();
+
+	for (;;) {
+		irq = pull_next_irq(p);
+		if (irq < 0)
+			break;
+		/*
+		 * Make sure the compiler does not reorder wrongly, so
+		 * that all updates to maps are done before the
+		 * handler gets called.
+		 */
+		barrier();
+
+		desc = cached_irq_to_desc(irq);
+
+		if (stage == &inband_stage) {
+			hard_local_irq_enable();
+			do_inband_irq(desc);
+			hard_local_irq_disable();
+		} else {
+			do_oob_irq(desc);
+			incr_irq_kstat(desc);
+		}
+
+		/*
+		 * We may have migrated to a different CPU (1) upon
+		 * return from the handler, or downgraded from the oob
+		 * stage to the inband one (2), the opposite way is
+		 * NOT allowed though.
+		 *
+		 * (1) reload the current per-cpu context pointer, so
+		 * that we further pull pending interrupts from the
+		 * proper per-cpu log.
+		 *
+		 * (2) check the stall bit to know whether we may
+		 * dispatch any interrupt pending for the inband
+		 * stage, and respin the entire dispatch loop if
+		 * so. Otherwise, immediately return to the caller,
+		 * _without_ affecting the stall state for the inband
+		 * stage, since we do not own it at this stage.  This
+		 * case is basically reflecting what may happen in
+		 * dispatch_oob_irq() for the fast path.
+		 */
+		p = current_irq_staged;
+		if (p->stage != stage) {
+			WARN_ON_ONCE(irq_pipeline_debug() &&
+				     stage == &inband_stage);
+			if (test_stage_bit(STAGE_STALL_BIT, p))
+				return;
+			goto respin;
+		}
+	}
+
+	if (stage == &inband_stage)
+		trace_hardirqs_on();
+
+	clear_stage_bit(STAGE_STALL_BIT, p);
+}
+
+/**
+ *      run_oob_call - escalate function call to the oob stage
+ *      @fn:    address of routine
+ *      @arg:   routine argument
+ *
+ *      Make the specified function run on the oob stage, switching
+ *      the current stage accordingly if needed. The escalated call is
+ *      allowed to perform a stage migration in the process.
+ */
+int notrace run_oob_call(int (*fn)(void *arg), void *arg)
+{
+	struct irq_stage_data *p, *old;
+	struct irq_stage *oob;
+	unsigned long flags;
+	int ret, s;
+
+	flags = hard_local_irq_save();
+
+	/* Switch to the oob stage if not current. */
+	p = this_oob_staged();
+	oob = p->stage;
+	old = current_irq_staged;
+	if (old != p)
+		switch_oob(p);
+
+	s = test_and_set_stage_bit(STAGE_STALL_BIT, p);
+	barrier();
+	ret = fn(arg);
+	hard_local_irq_disable();
+	p = this_oob_staged();
+	if (!s)
+		clear_stage_bit(STAGE_STALL_BIT, p);
+
+	/*
+	 * The exit logic is as follows:
+	 *
+	 *    ON-ENTRY  AFTER-CALL  EPILOGUE
+	 *
+	 *    oob       oob         sync current stage if !stalled
+	 *    inband    oob         switch to inband + sync all stages
+	 *    oob       inband      sync all stages
+	 *    inband    inband      sync all stages
+	 *
+	 * Each path which has stalled the oob stage while running on
+	 * the inband stage at some point during the escalation
+	 * process must synchronize all stages of the pipeline on
+	 * exit. Otherwise, we may restrict the synchronization scope
+	 * to the current stage when the whole process runs on the oob
+	 * stage.
+	 */
+	if (likely(current_irq_staged == p)) {
+		if (old->stage == oob) {
+			if (!s && stage_irqs_pending(p))
+				sync_current_irq_stage();
+			goto out;
+		}
+		switch_inband(this_inband_staged());
+	}
+
+	sync_irq_stage(oob);
+out:
+	hard_local_irq_restore(flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(run_oob_call);
+
+int enable_oob_stage(const char *name)
+{
+	struct irq_event_map *map;
+	struct irq_stage_data *p;
+	int cpu, ret;
+
+	if (oob_stage_present())
+		return -EBUSY;
+
+	/* Set up the out-of-band interrupt stage on all CPUs. */
+
+	for_each_possible_cpu(cpu) {
+		p = &per_cpu(irq_pipeline.stages, cpu)[1];
+		map = p->log.map; /* save/restore after memset(). */
+		memset(p, 0, sizeof(*p));
+		p->stage = &oob_stage;
+		memset(map, 0, sizeof(struct irq_event_map));
+		p->log.map = map;
+#ifdef CONFIG_DEBUG_IRQ_PIPELINE
+		p->cpu = cpu;
+#endif
+	}
+
+	ret = arch_enable_oob_stage();
+	if (ret)
+		return ret;
+
+	oob_stage.name = name;
+	smp_wmb();
+	oob_stage.index = 1;
+
+	pr_info("IRQ pipeline: high-priority %s stage added.\n", name);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(enable_oob_stage);
+
+void disable_oob_stage(void)
+{
+	const char *name = oob_stage.name;
+
+	WARN_ON(!running_inband() || !oob_stage_present());
+
+	oob_stage.index = 0;
+	smp_wmb();
+
+	pr_info("IRQ pipeline: %s stage removed.\n", name);
+}
+EXPORT_SYMBOL_GPL(disable_oob_stage);
+
+void irq_pipeline_oops(void)
+{
+	irq_pipeline_oopsing = true;
+	inband_irq_disable();
+	hard_local_irq_disable();
+}
+
+/*
+ * Used to save/restore the status bits of the inband stage across runs
+ * of NMI-triggered code, so that we can restore the original pipeline
+ * state before leaving NMI context.
+ */
+static DEFINE_PER_CPU(unsigned long, nmi_saved_status);
+
+void irq_pipeline_nmi_enter(void)
+{
+	struct irq_stage_data *p = this_inband_staged();
+	raw_cpu_write(nmi_saved_status, p->status);
+
+}
+EXPORT_SYMBOL(irq_pipeline_nmi_enter);
+
+void irq_pipeline_nmi_exit(void)
+{
+	struct irq_stage_data *p = this_inband_staged();
+	p->status = raw_cpu_read(nmi_saved_status);
+}
+EXPORT_SYMBOL(irq_pipeline_nmi_exit);
+
+bool irq_pipeline_steal_tick(void) /* Preemption disabled. */
+{
+	struct irq_pipeline_data *p;
+
+	p = raw_cpu_ptr(&irq_pipeline);
+
+	return arch_steal_pipelined_tick(&p->tick_regs);
+}
+
+bool __weak irq_cpuidle_control(struct cpuidle_device *dev,
+				struct cpuidle_state *state)
+{
+	/*
+	 * Allow entering the idle state by default, matching the
+	 * original behavior when CPU_IDLE is turned
+	 * on. irq_cpuidle_control() may be overriden by an
+	 * out-of-band code for determining whether the CPU may
+	 * actually enter the idle state.
+	 */
+	return true;
+}
+
+bool irq_cpuidle_enter(struct cpuidle_device *dev,
+		       struct cpuidle_state *state)
+{
+	struct irq_stage_data *p;
+
+	WARN_ON_ONCE(irq_pipeline_debug() && !irqs_disabled());
+
+	hard_local_irq_disable();
+	p = this_inband_staged();
+
+	/*
+	 * Pending IRQ(s) waiting for delivery to the inband stage, or
+	 * the arbitrary decision of a co-kernel may deny the
+	 * transition to a deeper C-state. Note that we return from
+	 * this call with hard irqs off, so that we won't allow any
+	 * interrupt to sneak into the IRQ log until we reach the
+	 * processor idling code, or leave the CPU idle framework
+	 * without sleeping.
+	 */
+	return !stage_irqs_pending(p) && irq_cpuidle_control(dev, state);
+}
+
+static unsigned int inband_work_sirq;
+
+static irqreturn_t inband_work_interrupt(int sirq, void *dev_id)
+{
+	irq_work_run();
+
+	return IRQ_HANDLED;
+}
+
+static struct irqaction inband_work = {
+	.handler = inband_work_interrupt,
+	.name = "in-band work",
+	.flags = IRQF_NO_THREAD,
+};
+
+void irq_local_work_raise(void)
+{
+	unsigned long flags;
+
+	/*
+	 * irq_work_queue() may be called from the in-band stage too
+	 * in case we want to delay a work until the hard irqs are on
+	 * again, so we may only sync the in-band log when unstalled,
+	 * with hard irqs on.
+	 */
+	flags = hard_local_irq_save();
+	irq_post_inband(inband_work_sirq);
+	if (running_inband() &&
+	    !hard_irqs_disabled_flags(flags) && !irqs_disabled())
+		sync_current_irq_stage();
+	hard_local_irq_restore(flags);
+}
+
+#ifdef CONFIG_DEBUG_IRQ_PIPELINE
+
+#ifdef CONFIG_LOCKDEP
+static inline bool lockdep_on_error(void)
+{
+	return !debug_locks;
+}
+#else
+static inline bool lockdep_on_error(void)
+{
+	return false;
+}
+#endif
+
+notrace void check_inband_stage(void)
+{
+	struct irq_stage *this_stage;
+	unsigned long flags;
+
+	flags = hard_smp_local_irq_save();
+
+	this_stage = current_irq_stage;
+	if (likely(this_stage == &inband_stage &&
+		   !test_stage_bit(STAGE_STALL_BIT, this_oob_staged()))) {
+		hard_smp_local_irq_restore(flags);
+		return;
+	}
+
+	if (in_nmi() || irq_pipeline_oopsing || lockdep_on_error()) {
+		hard_smp_local_irq_restore(flags);
+		return;
+	}
+
+	/*
+	 * This will disable all further pipeline debug checks, since
+	 * a wrecked interrupt state is likely to trigger many of
+	 * them, ending up in a terrible mess. IOW, the current
+	 * situation must be fixed prior to investigating any
+	 * subsequent issue that might still exist.
+	 */
+	irq_pipeline_oopsing = true;
+
+	hard_smp_local_irq_restore(flags);
+
+	if (this_stage != &inband_stage)
+		pr_err("IRQ pipeline: some code running in oob context '%s'\n"
+		       "              called an in-band only routine\n",
+		       this_stage->name);
+	else
+		pr_err("IRQ pipeline: oob stage found stalled while modifying in-band\n"
+		       "              interrupt state and/or running sleeping code\n");
+
+	dump_stack();
+}
+EXPORT_SYMBOL(check_inband_stage);
+
+#endif /* CONFIG_DEBUG_IRQ_PIPELINE */
+
+static inline void fixup_percpu_data(void)
+{
+#ifdef CONFIG_SMP
+	struct irq_pipeline_data *p;
+	int cpu;
+
+	/*
+	 * A temporary event log is used by the inband stage during the
+	 * early boot up (bootup_irq_map), until the per-cpu areas
+	 * have been set up.
+	 *
+	 * Obviously, this code must run over the boot CPU, before SMP
+	 * operations start, with hard IRQs off so that nothing can
+	 * change under our feet.
+	 */
+	WARN_ON(smp_processor_id() || !hard_irqs_disabled());
+
+	memcpy(&per_cpu(irq_map_array, 0)[0], &bootup_irq_map,
+	       sizeof(struct irq_event_map));
+
+	for_each_possible_cpu(cpu) {
+		p = &per_cpu(irq_pipeline, cpu);
+		p->stages[0].stage = &inband_stage;
+		p->stages[0].log.map = &per_cpu(irq_map_array, cpu)[0];
+		p->stages[1].log.map = &per_cpu(irq_map_array, cpu)[1];
+#ifdef CONFIG_DEBUG_IRQ_PIPELINE
+		p->stages[0].cpu = cpu;
+		p->stages[1].cpu = cpu;
+#endif
+	}
+#endif
+}
+
+void __init irq_pipeline_init_early(void)
+{
+	/*
+	 * This is called early from start_kernel(), even before the
+	 * actual number of IRQs is known. We are running on the boot
+	 * CPU, hw interrupts are off, and secondary CPUs are still
+	 * lost in space. Careful.
+	 */
+	fixup_percpu_data();
+}
+
+/**
+ *	irq_pipeline_init - Main pipeline core inits
+ *
+ *	This is step #2 of the 3-step pipeline initialization, which
+ *	should happen right after init_IRQ() has run. The internal
+ *	service interrupts are created along with the synthetic IRQ
+ *	domain, and the arch-specific init chores are performed too.
+ *
+ *	Interrupt pipelining should be fully functional when this
+ *	routine returns.
+ */
+void __init irq_pipeline_init(void)
+{
+	WARN_ON(!hard_irqs_disabled());
+
+	synthetic_irq_domain = irq_domain_add_nomap(NULL, ~0,
+						    &sirq_domain_ops,
+						    NULL);
+	inband_work_sirq = irq_create_direct_mapping(synthetic_irq_domain);
+	setup_percpu_irq(inband_work_sirq, &inband_work);
+
+	/*
+	 * We are running on the boot CPU, hw interrupts are off, and
+	 * secondary CPUs are still lost in space. Now we may run
+	 * arch-specific code for enabling the pipeline.
+	 */
+	arch_irq_pipeline_init();
+
+	irq_pipeline_active = true;
+
+	pr_info("IRQ pipeline enabled\n");
+}
+
+#ifndef CONFIG_SPARSE_IRQ
+EXPORT_SYMBOL_GPL(irq_desc);
+#endif
diff --git a/kernel/irq/resend.c b/kernel/irq/resend.c
index 98c04ca5fa43..3c608e540d7a 100644
--- a/kernel/irq/resend.c
+++ b/kernel/irq/resend.c
@@ -16,10 +16,11 @@
 #include <linux/module.h>
 #include <linux/random.h>
 #include <linux/interrupt.h>
+#include <linux/irq_pipeline.h>
 
 #include "internals.h"
 
-#ifdef CONFIG_HARDIRQS_SW_RESEND
+#if defined(CONFIG_HARDIRQS_SW_RESEND) && !defined(CONFIG_IRQ_PIPELINE)
 
 /* Bitmap to handle software resend of interrupts: */
 static DECLARE_BITMAP(irqs_resend, IRQ_BITMAP_BITS);
@@ -70,6 +71,9 @@ void check_irq_resend(struct irq_desc *desc)
 		return;
 	if (desc->istate & IRQS_PENDING) {
 		desc->istate &= ~IRQS_PENDING;
+#ifdef CONFIG_IRQ_PIPELINE
+		irq_inject_pipeline(irq_desc_get_irq(desc));
+#else
 		desc->istate |= IRQS_REPLAY;
 
 		if (!desc->irq_data.chip->irq_retrigger ||
@@ -98,5 +102,6 @@ void check_irq_resend(struct irq_desc *desc)
 			tasklet_schedule(&resend_tasklet);
 #endif
 		}
+#endif	/* CONFIG_IRQ_PIPELINE */
 	}
 }
diff --git a/kernel/irq/settings.h b/kernel/irq/settings.h
index e43795cd2ccf..adc2fd0ed47b 100644
--- a/kernel/irq/settings.h
+++ b/kernel/irq/settings.h
@@ -17,6 +17,8 @@ enum {
 	_IRQ_PER_CPU_DEVID	= IRQ_PER_CPU_DEVID,
 	_IRQ_IS_POLLED		= IRQ_IS_POLLED,
 	_IRQ_DISABLE_UNLAZY	= IRQ_DISABLE_UNLAZY,
+	_IRQ_OOB		= IRQ_OOB,
+	_IRQ_CHAINED		= IRQ_CHAINED,
 	_IRQF_MODIFY_MASK	= IRQF_MODIFY_MASK,
 };
 
@@ -31,6 +33,8 @@ enum {
 #define IRQ_PER_CPU_DEVID	GOT_YOU_MORON
 #define IRQ_IS_POLLED		GOT_YOU_MORON
 #define IRQ_DISABLE_UNLAZY	GOT_YOU_MORON
+#define IRQ_OOB			GOT_YOU_MORON
+#define IRQ_CHAINED		GOT_YOU_MORON
 #undef IRQF_MODIFY_MASK
 #define IRQF_MODIFY_MASK	GOT_YOU_MORON
 
@@ -167,3 +171,33 @@ static inline void irq_settings_clr_disable_unlazy(struct irq_desc *desc)
 {
 	desc->status_use_accessors &= ~_IRQ_DISABLE_UNLAZY;
 }
+
+static inline bool irq_settings_is_oob(struct irq_desc *desc)
+{
+	return desc->status_use_accessors & _IRQ_OOB;
+}
+
+static inline void irq_settings_clr_oob(struct irq_desc *desc)
+{
+	desc->status_use_accessors &= ~_IRQ_OOB;
+}
+
+static inline void irq_settings_set_oob(struct irq_desc *desc)
+{
+	desc->status_use_accessors |= _IRQ_OOB;
+}
+
+static inline bool irq_settings_is_chained(struct irq_desc *desc)
+{
+	return desc->status_use_accessors & _IRQ_CHAINED;
+}
+
+static inline void irq_settings_set_chained(struct irq_desc *desc)
+{
+	desc->status_use_accessors |= _IRQ_CHAINED;
+}
+
+static inline void irq_settings_clr_chained(struct irq_desc *desc)
+{
+	desc->status_use_accessors &= ~_IRQ_CHAINED;
+}
diff --git a/kernel/irq_work.c b/kernel/irq_work.c
index d42acaf81886..332994d90d9f 100644
--- a/kernel/irq_work.c
+++ b/kernel/irq_work.c
@@ -57,6 +57,11 @@ void __weak arch_irq_work_raise(void)
 	 */
 }
 
+void __weak irq_local_work_raise(void)
+{
+	arch_irq_work_raise();
+}
+
 /* Enqueue on current CPU, work must already be claimed and preempt disabled */
 static void __irq_work_queue_local(struct irq_work *work)
 {
@@ -64,10 +69,10 @@ static void __irq_work_queue_local(struct irq_work *work)
 	if (work->flags & IRQ_WORK_LAZY) {
 		if (llist_add(&work->llnode, this_cpu_ptr(&lazy_list)) &&
 		    tick_nohz_tick_stopped())
-			arch_irq_work_raise();
+			irq_local_work_raise();
 	} else {
 		if (llist_add(&work->llnode, this_cpu_ptr(&raised_list)))
-			arch_irq_work_raise();
+			irq_local_work_raise();
 	}
 }
 
diff --git a/kernel/locking/Makefile b/kernel/locking/Makefile
index 45452facff3b..b875192f9774 100644
--- a/kernel/locking/Makefile
+++ b/kernel/locking/Makefile
@@ -25,6 +25,7 @@ obj-$(CONFIG_RT_MUTEXES) += rtmutex.o
 obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
 obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock.o
 obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock_debug.o
+obj-$(CONFIG_IRQ_PIPELINE) += pipeline.o
 obj-$(CONFIG_QUEUED_RWLOCKS) += qrwlock.o
 obj-$(CONFIG_LOCK_TORTURE_TEST) += locktorture.o
 obj-$(CONFIG_WW_MUTEX_SELFTEST) += test-ww_mutex.o
diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c
index 233459c03b5a..7bb244f618f9 100644
--- a/kernel/locking/lockdep.c
+++ b/kernel/locking/lockdep.c
@@ -42,6 +42,7 @@
 #include <linux/stacktrace.h>
 #include <linux/debug_locks.h>
 #include <linux/irqflags.h>
+#include <linux/irqstage.h>
 #include <linux/utsname.h>
 #include <linux/hash.h>
 #include <linux/ftrace.h>
@@ -765,6 +766,53 @@ static int static_obj(const void *obj)
 }
 #endif
 
+static inline bool lockdep_stage_disabled(void)
+{
+	return stage_disabled();
+}
+
+#ifdef CONFIG_IRQ_PIPELINE
+/*
+ * If LOCKDEP is enabled, we want irqs to be disabled for both stages
+ * when traversing the lockdep code for hard and mutable locks (at the
+ * expense of massive latency overhead though).
+ */
+static inline unsigned long lockdep_stage_test_and_disable(int *irqsoff)
+{
+	return test_and_disable_stage(irqsoff);
+}
+
+static inline unsigned long lockdep_stage_disable(void)
+{
+	return lockdep_stage_test_and_disable(NULL);
+}
+
+static inline void lockdep_stage_restore(unsigned long flags)
+{
+	restore_stage(flags);
+}
+
+#else
+
+#define lockdep_stage_test_and_disable(__irqsoff)		\
+	({							\
+		unsigned long __flags;				\
+		raw_local_irq_save(__flags);			\
+		*(__irqsoff) = irqs_disabled_flags(__flags);	\
+		__flags;					\
+	})
+
+#define lockdep_stage_disable()					\
+	({							\
+		unsigned long __flags;				\
+		raw_local_irq_save(__flags);			\
+		__flags;					\
+	})
+
+#define lockdep_stage_restore(__flags)		raw_local_irq_restore(__flags)
+
+#endif /* !CONFIG_IRQ_PIPELINE */
+
 /*
  * To make lock name printouts unique, we calculate a unique
  * class->name_version generation counter. The caller must hold the graph
@@ -828,7 +876,7 @@ look_up_lock_class(const struct lockdep_map *lock, unsigned int subclass)
 	/*
 	 * We do an RCU walk of the hash, see lockdep_free_key_range().
 	 */
-	if (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
+	if (DEBUG_LOCKS_WARN_ON(!lockdep_stage_disabled()))
 		return NULL;
 
 	hlist_for_each_entry_rcu(class, hash_head, hash_entry) {
@@ -1180,7 +1228,7 @@ register_lock_class(struct lockdep_map *lock, unsigned int subclass, int force)
 	struct hlist_head *hash_head;
 	struct lock_class *class;
 
-	DEBUG_LOCKS_WARN_ON(!irqs_disabled());
+	DEBUG_LOCKS_WARN_ON(!lockdep_stage_disabled());
 
 	class = look_up_lock_class(lock, subclass);
 	if (likely(class))
@@ -1492,7 +1540,7 @@ static int __bfs(struct lock_list *source_entry,
 
 		head = get_dep_list(lock, offset);
 
-		DEBUG_LOCKS_WARN_ON(!irqs_disabled());
+		DEBUG_LOCKS_WARN_ON(!lockdep_stage_disabled());
 
 		list_for_each_entry_rcu(entry, head, entry) {
 			if (!lock_accessed(entry)) {
@@ -2804,7 +2852,7 @@ static inline int add_chain_cache(struct task_struct *curr,
 	 * disabled to make this an IRQ-safe lock.. for recursion reasons
 	 * lockdep won't complain about its own locking errors.
 	 */
-	if (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
+	if (DEBUG_LOCKS_WARN_ON(!lockdep_stage_disabled()))
 		return 0;
 
 	chain = alloc_lock_chain();
@@ -3414,7 +3462,7 @@ void lockdep_hardirqs_on(unsigned long ip)
 	 * already enabled, yet we find the hardware thinks they are in fact
 	 * enabled.. someone messed up their IRQ state tracing.
 	 */
-	if (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
+	if (DEBUG_LOCKS_WARN_ON(!lockdep_stage_disabled()))
 		return;
 
 	/*
@@ -3450,7 +3498,7 @@ void lockdep_hardirqs_off(unsigned long ip)
 	 * So we're supposed to get called after you mask local IRQs, but for
 	 * some reason the hardware doesn't quite think you did a proper job.
 	 */
-	if (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
+	if (DEBUG_LOCKS_WARN_ON(!lockdep_stage_disabled()))
 		return;
 
 	if (curr->hardirqs_enabled) {
@@ -3480,7 +3528,7 @@ void trace_softirqs_on(unsigned long ip)
 	 * We fancy IRQs being disabled here, see softirq.c, avoids
 	 * funny state and nesting things.
 	 */
-	if (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
+	if (DEBUG_LOCKS_WARN_ON(!lockdep_stage_disabled()))
 		return;
 
 	if (curr->softirqs_enabled) {
@@ -3519,7 +3567,7 @@ void trace_softirqs_off(unsigned long ip)
 	/*
 	 * We fancy IRQs being disabled here, see softirq.c
 	 */
-	if (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
+	if (DEBUG_LOCKS_WARN_ON(!lockdep_stage_disabled()))
 		return;
 
 	if (curr->softirqs_enabled) {
@@ -4086,7 +4134,7 @@ static int reacquire_held_locks(struct task_struct *curr, unsigned int depth,
 	struct held_lock *hlock;
 	int first_idx = idx;
 
-	if (DEBUG_LOCKS_WARN_ON(!irqs_disabled()))
+	if (DEBUG_LOCKS_WARN_ON(!lockdep_stage_disabled()))
 		return 0;
 
 	for (hlock = curr->held_locks + idx; idx < depth; idx++, hlock++) {
@@ -4397,7 +4445,13 @@ static void __lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie cookie
 static void check_flags(unsigned long flags)
 {
 #if defined(CONFIG_PROVE_LOCKING) && defined(CONFIG_DEBUG_LOCKDEP)
-	if (!debug_locks)
+	/*
+	 * irq_pipeline: we can't and don't want to check the
+	 * consistency of the irq tracer when running over the
+	 * pipeline entry or oob stage contexts, since the inband
+	 * stall bit does not reflect the current irq state there.
+	 */
+	if (on_pipeline_entry() || running_oob() || !debug_locks)
 		return;
 
 	if (irqs_disabled_flags(flags)) {
@@ -4475,19 +4529,20 @@ void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
 			  struct lockdep_map *nest_lock, unsigned long ip)
 {
 	unsigned long flags;
+	int irqsoff;
 
 	if (unlikely(current->lockdep_recursion))
 		return;
 
-	raw_local_irq_save(flags);
+	flags = lockdep_stage_test_and_disable(&irqsoff);
 	check_flags(flags);
 
 	current->lockdep_recursion = 1;
 	trace_lock_acquire(lock, subclass, trylock, read, check, nest_lock, ip);
 	__lock_acquire(lock, subclass, trylock, read, check,
-		       irqs_disabled_flags(flags), nest_lock, ip, 0, 0);
+		       irqsoff, nest_lock, ip, 0, 0);
 	current->lockdep_recursion = 0;
-	raw_local_irq_restore(flags);
+	lockdep_stage_restore(flags);
 }
 EXPORT_SYMBOL_GPL(lock_acquire);
 
@@ -4499,14 +4554,14 @@ void lock_release(struct lockdep_map *lock, int nested,
 	if (unlikely(current->lockdep_recursion))
 		return;
 
-	raw_local_irq_save(flags);
+	flags = lockdep_stage_disable();
 	check_flags(flags);
 	current->lockdep_recursion = 1;
 	trace_lock_release(lock, ip);
 	if (__lock_release(lock, ip))
 		check_chain_key(current);
 	current->lockdep_recursion = 0;
-	raw_local_irq_restore(flags);
+	lockdep_stage_restore(flags);
 }
 EXPORT_SYMBOL_GPL(lock_release);
 
@@ -4518,13 +4573,13 @@ int lock_is_held_type(const struct lockdep_map *lock, int read)
 	if (unlikely(current->lockdep_recursion))
 		return 1; /* avoid false negative lockdep_assert_held() */
 
-	raw_local_irq_save(flags);
+	flags = lockdep_stage_disable();
 	check_flags(flags);
 
 	current->lockdep_recursion = 1;
 	ret = __lock_is_held(lock, read);
 	current->lockdep_recursion = 0;
-	raw_local_irq_restore(flags);
+	lockdep_stage_restore(flags);
 
 	return ret;
 }
@@ -4714,13 +4769,13 @@ void lock_contended(struct lockdep_map *lock, unsigned long ip)
 	if (unlikely(current->lockdep_recursion))
 		return;
 
-	raw_local_irq_save(flags);
+	flags = lockdep_stage_disable();
 	check_flags(flags);
 	current->lockdep_recursion = 1;
 	trace_lock_contended(lock, ip);
 	__lock_contended(lock, ip);
 	current->lockdep_recursion = 0;
-	raw_local_irq_restore(flags);
+	lockdep_stage_restore(flags);
 }
 EXPORT_SYMBOL_GPL(lock_contended);
 
@@ -4734,12 +4789,12 @@ void lock_acquired(struct lockdep_map *lock, unsigned long ip)
 	if (unlikely(current->lockdep_recursion))
 		return;
 
-	raw_local_irq_save(flags);
+	flags = lockdep_stage_disable();
 	check_flags(flags);
 	current->lockdep_recursion = 1;
 	__lock_acquired(lock, ip);
 	current->lockdep_recursion = 0;
-	raw_local_irq_restore(flags);
+	lockdep_stage_restore(flags);
 }
 EXPORT_SYMBOL_GPL(lock_acquired);
 #endif
@@ -5287,7 +5342,7 @@ void debug_check_no_locks_freed(const void *mem_from, unsigned long mem_len)
 	if (unlikely(!debug_locks))
 		return;
 
-	raw_local_irq_save(flags);
+	flags = lockdep_stage_disable();
 	for (i = 0; i < curr->lockdep_depth; i++) {
 		hlock = curr->held_locks + i;
 
@@ -5298,7 +5353,7 @@ void debug_check_no_locks_freed(const void *mem_from, unsigned long mem_len)
 		print_freed_lock_bug(curr, mem_from, mem_from + mem_len, hlock);
 		break;
 	}
-	raw_local_irq_restore(flags);
+	lockdep_stage_restore(flags);
 }
 EXPORT_SYMBOL_GPL(debug_check_no_locks_freed);
 
diff --git a/kernel/locking/lockdep_internals.h b/kernel/locking/lockdep_internals.h
index 18d85aebbb57..accaa7f35cdd 100644
--- a/kernel/locking/lockdep_internals.h
+++ b/kernel/locking/lockdep_internals.h
@@ -196,12 +196,12 @@ extern struct lock_class lock_classes[MAX_LOCKDEP_KEYS];
 	this_cpu_inc(lockdep_stats.ptr);
 
 #define debug_atomic_inc(ptr)			{		\
-	WARN_ON_ONCE(!irqs_disabled());				\
+	WARN_ON_ONCE(!hard_irqs_disabled() && !irqs_disabled());\
 	__this_cpu_inc(lockdep_stats.ptr);			\
 }
 
 #define debug_atomic_dec(ptr)			{		\
-	WARN_ON_ONCE(!irqs_disabled());				\
+	WARN_ON_ONCE(!hard_irqs_disabled() && !irqs_disabled());\
 	__this_cpu_dec(lockdep_stats.ptr);			\
 }
 
diff --git a/kernel/locking/pipeline.c b/kernel/locking/pipeline.c
new file mode 100644
index 000000000000..61190ab5e0e4
--- /dev/null
+++ b/kernel/locking/pipeline.c
@@ -0,0 +1,230 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2016 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#include <linux/linkage.h>
+#include <linux/preempt.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/irq_pipeline.h>
+#include <linux/kconfig.h>
+
+/*
+ * A mutable spinlock behaves in different ways depending on the
+ * current interrupt stage on entry.
+ *
+ * Such spinlock always leaves hard IRQs disabled once locked. In
+ * addition, it stalls the in-band stage when protecting a critical
+ * section there, disabling preemption like regular spinlocks do as
+ * well. This combination preserves the regular locking logic when
+ * called from the in-band stage, while fully disabling preemption by
+ * other interrupt stages.
+ *
+ * When taken from the pipeline entry context, a mutable lock behaves
+ * like a hard spinlock, assuming that hard IRQs are already disabled.
+ *
+ * The irq descriptor lock (struct irq_desc) is a typical example of
+ * such lock, which properly serializes accesses regardless of the
+ * calling context.
+ */
+void __mutable_spin_lock(struct raw_spinlock *rlock)
+{
+	struct mutable_spinlock *lock;
+	unsigned long __flags;
+
+	if (running_inband())
+		preempt_disable();
+
+	__flags = hard_local_irq_save();
+	hard_lock_acquire(rlock, 0, _RET_IP_);
+	lock = container_of(rlock, struct mutable_spinlock, rlock);
+	lock->hwflags = __flags;
+}
+EXPORT_SYMBOL(__mutable_spin_lock);
+
+void __mutable_spin_lock_nested(struct raw_spinlock *rlock, int subclass)
+{
+	struct mutable_spinlock *lock;
+	unsigned long __flags;
+
+	if (running_inband())
+		preempt_disable();
+
+	__flags = hard_local_irq_save();
+	hard_lock_acquire_nested(rlock, subclass, _RET_IP_);
+	lock = container_of(rlock, struct mutable_spinlock, rlock);
+	lock->hwflags = __flags;
+}
+EXPORT_SYMBOL(__mutable_spin_lock_nested);
+
+void __mutable_spin_unlock(struct raw_spinlock *rlock)
+{
+	struct mutable_spinlock *lock;
+	unsigned long __flags;
+
+	/* Pick the flags before releasing the lock. */
+	lock = container_of(rlock, struct mutable_spinlock, rlock);
+	__flags = lock->hwflags;
+	hard_lock_release(rlock, _RET_IP_);
+	hard_local_irq_restore(__flags);
+
+	if (running_inband())
+		preempt_enable();
+}
+EXPORT_SYMBOL(__mutable_spin_unlock);
+
+void __mutable_spin_lock_irq(struct raw_spinlock *rlock)
+{
+	struct mutable_spinlock *lock;
+	unsigned long __flags;
+
+	__flags = hard_local_irq_save();
+
+	if (running_inband()) {
+		set_stage_bit(STAGE_STALL_BIT, this_inband_staged());
+		trace_hardirqs_off();
+		preempt_disable();
+	}
+
+	hard_lock_acquire(rlock, 0, _RET_IP_);
+	lock = container_of(rlock, struct mutable_spinlock, rlock);
+	lock->hwflags = __flags;
+}
+EXPORT_SYMBOL(__mutable_spin_lock_irq);
+
+void __mutable_spin_unlock_irq(struct raw_spinlock *rlock)
+{
+	struct mutable_spinlock *lock;
+	unsigned long __flags;
+
+	/* Pick the flags before releasing the lock. */
+	lock = container_of(rlock, struct mutable_spinlock, rlock);
+	__flags = lock->hwflags;
+	hard_lock_release(rlock, _RET_IP_);
+
+	if (running_inband()) {
+		trace_hardirqs_on();
+		clear_stage_bit(STAGE_STALL_BIT, this_inband_staged());
+		hard_local_irq_restore(__flags);
+		preempt_enable();
+		return;
+	}
+
+	hard_local_irq_restore(__flags);
+}
+EXPORT_SYMBOL(__mutable_spin_unlock_irq);
+
+unsigned long __mutable_spin_lock_irqsave(struct raw_spinlock *rlock)
+{
+	struct mutable_spinlock *lock;
+	unsigned long __flags, flags;
+
+	__flags = flags = hard_local_irq_save();
+
+	if (running_inband()) {
+		flags = test_and_set_stage_bit(STAGE_STALL_BIT,
+				       this_inband_staged());
+		trace_hardirqs_off();
+		preempt_disable();
+	}
+
+	hard_lock_acquire(rlock, 0, _RET_IP_);
+	lock = container_of(rlock, struct mutable_spinlock, rlock);
+	lock->hwflags = __flags;
+
+	return flags;
+}
+EXPORT_SYMBOL(__mutable_spin_lock_irqsave);
+
+void __mutable_spin_unlock_irqrestore(struct raw_spinlock *rlock,
+				      unsigned long flags)
+{
+	struct mutable_spinlock *lock;
+	unsigned long __flags;
+
+	/* Pick the flags before releasing the lock. */
+	lock = container_of(rlock, struct mutable_spinlock, rlock);
+	__flags = lock->hwflags;
+	hard_lock_release(rlock, _RET_IP_);
+
+	if (running_inband()) {
+		if (!flags) {
+			trace_hardirqs_on();
+			clear_stage_bit(STAGE_STALL_BIT,
+					this_inband_staged());
+		}
+		hard_local_irq_restore(__flags);
+		preempt_enable();
+		return;
+	}
+
+	hard_local_irq_restore(__flags);
+}
+EXPORT_SYMBOL(__mutable_spin_unlock_irqrestore);
+
+int __mutable_spin_trylock(struct raw_spinlock *rlock)
+{
+	struct mutable_spinlock *lock;
+	unsigned long __flags;
+
+	if (running_inband())
+		preempt_disable();
+
+	lock = container_of(rlock, struct mutable_spinlock, rlock);
+	__flags = hard_local_irq_save();
+
+	if (do_raw_spin_trylock(rlock)) {
+		lock->hwflags = __flags;
+		hard_trylock_acquire(rlock, 1, _RET_IP_);
+		return 1;
+	}
+
+	hard_local_irq_restore(__flags);
+
+	if (running_inband())
+		preempt_enable();
+
+	return 0;
+}
+EXPORT_SYMBOL(__mutable_spin_trylock);
+
+int __mutable_spin_trylock_irqsave(struct raw_spinlock *rlock,
+				   unsigned long *flags)
+{
+	struct mutable_spinlock *lock;
+	struct irq_stage_data *p;
+	unsigned long __flags;
+	bool inband;
+
+	inband = running_inband();
+
+	__flags = *flags = hard_local_irq_save();
+
+	p = this_inband_staged();
+	lock = container_of(rlock, struct mutable_spinlock, rlock);
+	if (inband) {
+		*flags = test_and_set_stage_bit(STAGE_STALL_BIT, p);
+		trace_hardirqs_off();
+		preempt_disable();
+	}
+
+	if (do_raw_spin_trylock(rlock)) {
+		hard_trylock_acquire(rlock, 1, _RET_IP_);
+		lock->hwflags = __flags;
+		return 1;
+	}
+
+	if (inband && !*flags) {
+		trace_hardirqs_on();
+		clear_stage_bit(STAGE_STALL_BIT, p);
+	}
+
+	hard_local_irq_restore(__flags);
+
+	if (inband)
+		preempt_enable();
+
+	return 0;
+}
+EXPORT_SYMBOL(__mutable_spin_trylock_irqsave);
diff --git a/kernel/locking/spinlock_debug.c b/kernel/locking/spinlock_debug.c
index 399669f7eba8..a6b3a703f93c 100644
--- a/kernel/locking/spinlock_debug.c
+++ b/kernel/locking/spinlock_debug.c
@@ -114,6 +114,7 @@ void do_raw_spin_lock(raw_spinlock_t *lock)
 	mmiowb_spin_lock();
 	debug_spin_lock_after(lock);
 }
+EXPORT_SYMBOL_GPL(do_raw_spin_lock);
 
 int do_raw_spin_trylock(raw_spinlock_t *lock)
 {
@@ -131,6 +132,7 @@ int do_raw_spin_trylock(raw_spinlock_t *lock)
 #endif
 	return ret;
 }
+EXPORT_SYMBOL_GPL(do_raw_spin_trylock);
 
 void do_raw_spin_unlock(raw_spinlock_t *lock)
 {
@@ -138,6 +140,7 @@ void do_raw_spin_unlock(raw_spinlock_t *lock)
 	debug_spin_unlock(lock);
 	arch_spin_unlock(&lock->raw_lock);
 }
+EXPORT_SYMBOL_GPL(do_raw_spin_unlock);
 
 static void rwlock_bug(rwlock_t *lock, const char *msg)
 {
diff --git a/kernel/notifier.c b/kernel/notifier.c
index d9f5081d578d..1da0eb3e2102 100644
--- a/kernel/notifier.c
+++ b/kernel/notifier.c
@@ -192,6 +192,9 @@ NOKPROBE_SYMBOL(__atomic_notifier_call_chain);
 int atomic_notifier_call_chain(struct atomic_notifier_head *nh,
 			       unsigned long val, void *v)
 {
+	if (!running_inband())
+		return notifier_call_chain(&nh->head, val, v, -1, NULL);
+
 	return __atomic_notifier_call_chain(nh, val, v, -1, NULL);
 }
 EXPORT_SYMBOL_GPL(atomic_notifier_call_chain);
diff --git a/kernel/panic.c b/kernel/panic.c
index f470a038b05b..6a73816b9903 100644
--- a/kernel/panic.c
+++ b/kernel/panic.c
@@ -27,6 +27,7 @@
 #include <linux/sysrq.h>
 #include <linux/init.h>
 #include <linux/nmi.h>
+#include <linux/irq_pipeline.h>
 #include <linux/console.h>
 #include <linux/bug.h>
 #include <linux/ratelimit.h>
@@ -41,7 +42,7 @@ static unsigned long tainted_mask =
 	IS_ENABLED(CONFIG_GCC_PLUGIN_RANDSTRUCT) ? (1 << TAINT_RANDSTRUCT) : 0;
 static int pause_on_oops;
 static int pause_on_oops_flag;
-static DEFINE_SPINLOCK(pause_on_oops_lock);
+static DEFINE_HARD_SPINLOCK(pause_on_oops_lock);
 bool crash_kexec_post_notifiers;
 int panic_on_warn __read_mostly;
 
@@ -179,8 +180,9 @@ void panic(const char *fmt, ...)
 	 * there is nothing to prevent an interrupt handler (that runs
 	 * after setting panic_cpu) from invoking panic() again.
 	 */
-	local_irq_disable();
+	hard_local_irq_disable();
 	preempt_disable_notrace();
+	irq_pipeline_oops();
 
 	/*
 	 * It's possible to come here directly from a panic-assertion and
@@ -257,9 +259,12 @@ void panic(const char *fmt, ...)
 
 	/*
 	 * Run any panic handlers, including those that might need to
-	 * add information to the kmsg dump output.
+	 * add information to the kmsg dump output. Skip panic
+	 * handlers if running over the oob stage, as they would most
+	 * likely break.
 	 */
-	atomic_notifier_call_chain(&panic_notifier_list, 0, buf);
+	if (running_inband())
+		atomic_notifier_call_chain(&panic_notifier_list, 0, buf);
 
 	/* Call flush even twice. It tries harder with a single online CPU */
 	printk_safe_flush_on_panic();
@@ -459,7 +464,7 @@ static void do_oops_enter_exit(void)
 	if (!pause_on_oops)
 		return;
 
-	spin_lock_irqsave(&pause_on_oops_lock, flags);
+	raw_spin_lock_irqsave(&pause_on_oops_lock, flags);
 	if (pause_on_oops_flag == 0) {
 		/* This CPU may now print the oops message */
 		pause_on_oops_flag = 1;
@@ -469,21 +474,21 @@ static void do_oops_enter_exit(void)
 			/* This CPU gets to do the counting */
 			spin_counter = pause_on_oops;
 			do {
-				spin_unlock(&pause_on_oops_lock);
+				raw_spin_unlock(&pause_on_oops_lock);
 				spin_msec(MSEC_PER_SEC);
-				spin_lock(&pause_on_oops_lock);
+				raw_spin_lock(&pause_on_oops_lock);
 			} while (--spin_counter);
 			pause_on_oops_flag = 0;
 		} else {
 			/* This CPU waits for a different one */
 			while (spin_counter) {
-				spin_unlock(&pause_on_oops_lock);
+				raw_spin_unlock(&pause_on_oops_lock);
 				spin_msec(1);
-				spin_lock(&pause_on_oops_lock);
+				raw_spin_lock(&pause_on_oops_lock);
 			}
 		}
 	}
-	spin_unlock_irqrestore(&pause_on_oops_lock, flags);
+	raw_spin_unlock_irqrestore(&pause_on_oops_lock, flags);
 }
 
 /*
@@ -513,6 +518,7 @@ void oops_enter(void)
 {
 	tracing_off();
 	/* can't trust the integrity of the kernel anymore: */
+	irq_pipeline_oops();
 	debug_locks_off();
 	do_oops_enter_exit();
 }
diff --git a/kernel/power/hibernate.c b/kernel/power/hibernate.c
index 3c0a5a8170b0..463f9b785da0 100644
--- a/kernel/power/hibernate.c
+++ b/kernel/power/hibernate.c
@@ -290,6 +290,7 @@ static int create_image(int platform_mode)
 		goto Enable_cpus;
 
 	local_irq_disable();
+	hard_cond_local_irq_disable();
 
 	system_state = SYSTEM_SUSPEND;
 
@@ -457,6 +458,7 @@ static int resume_target_kernel(bool platform_mode)
 
 	local_irq_disable();
 	system_state = SYSTEM_SUSPEND;
+	hard_cond_local_irq_disable();
 
 	error = syscore_suspend();
 	if (error)
@@ -578,6 +580,7 @@ int hibernation_platform_enter(void)
 
 	local_irq_disable();
 	system_state = SYSTEM_SUSPEND;
+	hard_cond_local_irq_disable();
 	syscore_suspend();
 	if (pm_wakeup_pending()) {
 		error = -EAGAIN;
diff --git a/kernel/printk/printk.c b/kernel/printk/printk.c
index ca65327a6de8..47ddbbf3901d 100644
--- a/kernel/printk/printk.c
+++ b/kernel/printk/printk.c
@@ -48,6 +48,7 @@
 #include <linux/sched/clock.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
+#include <linux/irqstage.h>
 
 #include <linux/uaccess.h>
 #include <asm/sections.h>
@@ -2114,6 +2115,73 @@ asmlinkage __visible void early_printk(const char *fmt, ...)
 }
 #endif
 
+#ifdef CONFIG_RAW_PRINTK
+static struct console *raw_console;
+static DEFINE_HARD_SPINLOCK(raw_console_lock);
+
+void raw_puts(const char *s, size_t len)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&raw_console_lock, flags);
+	if (raw_console)
+		raw_console->write_raw(raw_console, s, len);
+	raw_spin_unlock_irqrestore(&raw_console_lock, flags);
+}
+EXPORT_SYMBOL(raw_puts);
+
+void raw_vprintk(const char *fmt, va_list ap)
+{
+	char buf[256];
+	size_t n;
+
+	if (raw_console == NULL || console_suspended)
+		return;
+
+        touch_nmi_watchdog();
+	n = vscnprintf(buf, sizeof(buf), fmt, ap);
+	raw_puts(buf, n);
+}
+EXPORT_SYMBOL(raw_vprintk);
+
+asmlinkage __visible void raw_printk(const char *fmt, ...)
+{
+	va_list ap;
+
+	va_start(ap, fmt);
+	raw_vprintk(fmt, ap);
+	va_end(ap);
+}
+EXPORT_SYMBOL(raw_printk);
+
+static inline void register_raw_console(struct console *newcon)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&raw_console_lock, flags);
+	if (newcon->write_raw)
+		raw_console = newcon;
+	raw_spin_unlock_irqrestore(&raw_console_lock, flags);
+}
+
+static inline void unregister_raw_console(struct console *oldcon)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&raw_console_lock, flags);
+	if (oldcon == raw_console)
+		raw_console = NULL;
+	raw_spin_unlock_irqrestore(&raw_console_lock, flags);
+}
+
+#else
+
+static inline void register_raw_console(struct console *newcon) { }
+
+static inline void unregister_raw_console(struct console *oldcon) { }
+
+#endif
+
 static int __add_preferred_console(char *name, int idx, char *options,
 				   char *brl_options)
 {
@@ -2737,6 +2805,9 @@ void register_console(struct console *newcon)
 	if (!(newcon->flags & CON_ENABLED))
 		return;
 
+	/* The latest raw console to register is current. */
+	register_raw_console(newcon);
+
 	/*
 	 * If we have a bootconsole, and are switching to a real console,
 	 * don't print everything out again, since when the boot console, and
@@ -2820,6 +2891,8 @@ int unregister_console(struct console *console)
 		(console->flags & CON_BOOT) ? "boot" : "" ,
 		console->name, console->index);
 
+	unregister_raw_console(console);
+
 	res = _braille_unregister_console(console);
 	if (res)
 		return res;
diff --git a/kernel/printk/printk_safe.c b/kernel/printk/printk_safe.c
index b4045e782743..ef231ed589d3 100644
--- a/kernel/printk/printk_safe.c
+++ b/kernel/printk/printk_safe.c
@@ -10,6 +10,7 @@
 #include <linux/cpumask.h>
 #include <linux/irq_work.h>
 #include <linux/printk.h>
+#include <linux/irq_pipeline.h>
 
 #include "internal.h"
 
@@ -360,6 +361,9 @@ void __printk_safe_exit(void)
 
 __printf(1, 0) int vprintk_func(const char *fmt, va_list args)
 {
+	if (inband_unsafe())
+		return vprintk_nmi(fmt, args);
+
 	/*
 	 * Try to use the main logbuf even in NMI. But avoid calling console
 	 * drivers that might have their own locks.
diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index 81105141b6a8..956763448678 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -897,6 +897,9 @@ bool notrace rcu_is_watching(void)
 {
 	bool ret;
 
+	if (running_oob())
+		return true;
+
 	preempt_disable_notrace();
 	ret = !rcu_dynticks_curr_cpu_in_eqs();
 	preempt_enable_notrace();
diff --git a/kernel/rcu/update.c b/kernel/rcu/update.c
index 1861103662db..74e09509a7ff 100644
--- a/kernel/rcu/update.c
+++ b/kernel/rcu/update.c
@@ -117,6 +117,9 @@ int rcu_read_lock_sched_held(void)
 {
 	bool ret;
 
+	if (irqs_pipelined() &&
+	    (hard_irqs_disabled() || !running_inband()))
+		return true;
 	if (rcu_read_lock_held_common(&ret))
 		return ret;
 	return lock_is_held(&rcu_sched_lock_map) || !preemptible();
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 44123b4d14e8..f9349d459348 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1678,6 +1678,7 @@ static int __set_cpus_allowed_ptr(struct task_struct *p,
 	if (cpumask_test_cpu(task_cpu(p), new_mask))
 		goto out;
 
+	inband_migration_notify(p, dest_cpu);
 	if (task_running(rq, p) || p->state == TASK_WAKING) {
 		struct migration_arg arg = { p, dest_cpu };
 		/* Need help from migration thread: drop lock and wait. */
@@ -2531,7 +2532,7 @@ try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 		 *  - we're serialized against set_special_state() by virtue of
 		 *    it disabling IRQs (this allows not taking ->pi_lock).
 		 */
-		if (!(p->state & state))
+		if (!(p->state & state) || task_is_off_stage(p))
 			goto out;
 
 		success = 1;
@@ -2550,7 +2551,7 @@ try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 	 */
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	smp_mb__after_spinlock();
-	if (!(p->state & state))
+	if (!(p->state & state) || task_is_off_stage(p))
 		goto unlock;
 
 	trace_sched_waking(p);
@@ -3158,6 +3159,7 @@ prepare_task_switch(struct rq *rq, struct task_struct *prev,
 	rseq_preempt(prev);
 	fire_sched_out_preempt_notifiers(prev, next);
 	prepare_task(next);
+	prepare_inband_switch(next);
 	prepare_arch_switch(next);
 }
 
@@ -3314,8 +3316,15 @@ asmlinkage __visible void schedule_tail(struct task_struct *prev)
 	 * finish_task_switch() will drop rq->lock() and lower preempt_count
 	 * and the preempt_enable() will end up enabling preemption (on
 	 * PREEMPT_COUNT kernels).
+	 *
+	 * When dovetailing is enabled, schedule_tail() is the place
+	 * where transitions of tasks from the in-band to the oob
+	 * stage completes. The co-kernel is notified that 'prev' is
+	 * now suspended in the in-band stage, and can be safely
+	 * resumed in the oob stage.
 	 */
 
+	oob_trampoline();
 	rq = finish_task_switch(prev);
 	balance_callback(rq);
 	preempt_enable();
@@ -3369,6 +3378,20 @@ context_switch(struct rq *rq, struct task_struct *prev,
 		 */
 		switch_mm_irqs_off(prev->active_mm, next->mm, next);
 
+		/*
+		 * If dovetail is enabled, insert a short window of
+		 * opportunity for preemption by out-of-band IRQs
+		 * before finalizing the context switch.
+		 * dovetail_context_switch() can deal with preempting
+		 * partially switched in-band contexts.
+		 */
+		if (dovetailing()) {
+			struct mm_struct *oldmm = prev->active_mm;
+			prev->active_mm = next->mm;
+			hard_local_irq_sync();
+			prev->active_mm = oldmm;
+		}
+
 		if (!prev->mm) {                        // from kernel
 			/* will mmdrop() in finish_task_switch(). */
 			rq->prev_mm = prev->active_mm;
@@ -3384,6 +3407,15 @@ context_switch(struct rq *rq, struct task_struct *prev,
 	switch_to(prev, next, prev);
 	barrier();
 
+	/*
+	 * If 'next' is on its way to the oob stage, don't run the
+	 * context switch epilogue just yet. We will do that at some
+	 * point later, when the task switches back to the in-band
+	 * stage.
+	 */
+	if (unlikely(inband_switch_tail()))
+		return NULL;
+
 	return finish_task_switch(prev);
 }
 
@@ -3878,6 +3910,8 @@ static inline void schedule_debug(struct task_struct *prev, bool preempt)
 		panic("corrupted stack end detected inside scheduler\n");
 #endif
 
+	check_inband_stage();
+
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 	if (!preempt && prev->state && prev->non_block_count) {
 		printk(KERN_ERR "BUG: scheduling in a non-blocking section: %s/%d/%i\n",
@@ -3995,7 +4029,7 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
  *
  * WARNING: must be called with preemption disabled!
  */
-static void __sched notrace __schedule(bool preempt)
+static int __sched notrace __schedule(bool preempt)
 {
 	struct task_struct *prev, *next;
 	unsigned long *switch_count;
@@ -4076,12 +4110,17 @@ static void __sched notrace __schedule(bool preempt)
 
 		/* Also unlocks the rq: */
 		rq = context_switch(rq, prev, next, &rf);
+		if (dovetailing() && rq == NULL)
+			/* Task moved to the oob stage. */
+			return 1;
 	} else {
 		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
 		rq_unlock_irq(rq, &rf);
 	}
 
 	balance_callback(rq);
+
+	return 0;
 }
 
 void __noreturn do_task_dead(void)
@@ -4142,7 +4181,8 @@ asmlinkage __visible void __sched schedule(void)
 	sched_submit_work(tsk);
 	do {
 		preempt_disable();
-		__schedule(false);
+		if (__schedule(false))
+			return;
 		sched_preempt_enable_no_resched();
 	} while (need_resched());
 	sched_update_worker(tsk);
@@ -4223,7 +4263,8 @@ static void __sched notrace preempt_schedule_common(void)
 		 */
 		preempt_disable_notrace();
 		preempt_latency_start(1);
-		__schedule(true);
+		if (__schedule(true))
+			return;
 		preempt_latency_stop(1);
 		preempt_enable_no_resched_notrace();
 
@@ -4245,7 +4286,7 @@ asmlinkage __visible void __sched notrace preempt_schedule(void)
 	 * If there is a non-zero preempt_count or interrupts are disabled,
 	 * we do not want to preempt the current task. Just return..
 	 */
-	if (likely(!preemptible()))
+	if (likely(!running_inband() || !preemptible()))
 		return;
 
 	preempt_schedule_common();
@@ -4271,7 +4312,7 @@ asmlinkage __visible void __sched notrace preempt_schedule_notrace(void)
 {
 	enum ctx_state prev_ctx;
 
-	if (likely(!preemptible()))
+	if (likely(!running_inband() || !preemptible()))
 		return;
 
 	do {
@@ -4307,6 +4348,28 @@ EXPORT_SYMBOL_GPL(preempt_schedule_notrace);
 
 #endif /* CONFIG_PREEMPTION */
 
+#ifdef CONFIG_IRQ_PIPELINE
+static inline void preempt_sync_inband_irqs(void)
+{
+	struct irq_stage_data *p;
+
+	hard_local_irq_disable();
+	p = this_inband_staged();
+	if (unlikely(stage_irqs_pending(p))) {
+		preempt_disable();
+		trace_hardirqs_on();
+		clear_stage_bit(STAGE_STALL_BIT, p);
+		sync_current_irq_stage();
+		preempt_enable_no_resched_notrace();
+	} else
+		clear_stage_bit(STAGE_STALL_BIT, p);
+
+	/* hard IRQs are left disabled. */
+}
+#else
+static inline void preempt_sync_inband_irqs(void) { }
+#endif
+
 /*
  * This is the entry point to schedule() from kernel preemption
  * off of irq context.
@@ -4317,6 +4380,16 @@ asmlinkage __visible void __sched preempt_schedule_irq(void)
 {
 	enum ctx_state prev_state;
 
+	if (irqs_pipelined()) {
+		if (irq_pipeline_debug()) {
+			WARN_ON_ONCE(!running_inband());
+			WARN_ON_ONCE(!hard_irqs_disabled());
+			WARN_ON_ONCE(irqs_disabled());
+		}
+		local_irq_disable();
+		hard_local_irq_enable();
+	}
+
 	/* Catch callers which need to be fixed */
 	BUG_ON(preempt_count() || !irqs_disabled());
 
@@ -4330,6 +4403,15 @@ asmlinkage __visible void __sched preempt_schedule_irq(void)
 		sched_preempt_enable_no_resched();
 	} while (need_resched());
 
+	/*
+	 * If pipelining interrupts, flush any pending IRQ that might
+	 * have been logged since the last time we stalled the in-band
+	 * stage. The caller is expected to call us back again until
+	 * need_resched is clear, so we just need to synchronize the
+	 * in-band stage log.
+	 */
+	preempt_sync_inband_irqs();
+
 	exception_exit(prev_state);
 }
 
@@ -7899,6 +7981,183 @@ struct cgroup_subsys cpu_cgrp_subsys = {
 
 #endif	/* CONFIG_CGROUP_SCHED */
 
+#ifdef CONFIG_DOVETAIL
+
+int dovetail_leave_inband(void)
+{
+	struct task_struct *p = current;
+	struct irq_pipeline_data *pd;
+	unsigned long flags;
+
+	preempt_disable();
+
+	pd = raw_cpu_ptr(&irq_pipeline);
+
+	if (WARN_ON_ONCE(dovetail_debug() && pd->task_inflight))
+		goto out;	/* Paranoid. */
+
+	raw_spin_lock_irqsave(&p->pi_lock, flags);
+	pd->task_inflight = p;
+	/*
+	 * The scope of the off-stage state is broader than _TLF_OOB,
+	 * in that it includes the transition path from the in-band
+	 * context to the oob stage.
+	 */
+	set_thread_local_flags(_TLF_OFFSTAGE);
+	set_current_state(TASK_INTERRUPTIBLE);
+	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+	sched_submit_work(p);
+	/*
+	 * The current task is scheduled out from the inband stage,
+	 * before resuming on the oob stage. Since this code stands
+	 * for the scheduling tail of the oob scheduler,
+	 * arch_dovetail_switch_finish() is called to perform
+	 * architecture-specific fixups (e.g. fpu context reload).
+	 */
+	if (likely(__schedule(false))) {
+		arch_dovetail_switch_finish(false);
+		return 0;
+	}
+
+	clear_thread_local_flags(_TLF_OFFSTAGE);
+	pd->task_inflight = NULL;
+out:
+	preempt_enable();
+
+	return -ERESTARTSYS;
+}
+EXPORT_SYMBOL_GPL(dovetail_leave_inband);
+
+void dovetail_resume_inband(void)
+{
+	struct task_struct *p;
+	struct rq *rq;
+
+	p = __this_cpu_read(irq_pipeline.rqlock_owner);
+	if (WARN_ON_ONCE(dovetail_debug() && p == NULL))
+		return;
+
+	if (WARN_ON_ONCE(dovetail_debug() && (preempt_count() & STAGE_MASK)))
+		return;
+
+	rq = finish_task_switch(p);
+	balance_callback(rq);
+	preempt_enable();
+	oob_trampoline();
+}
+EXPORT_SYMBOL_GPL(dovetail_resume_inband);
+
+bool dovetail_context_switch(struct dovetail_altsched_context *out,
+			struct dovetail_altsched_context *in,
+			bool leave_inband)
+{
+	struct task_struct *next, *prev, *last;
+	struct mm_struct *prev_mm, *next_mm;
+	unsigned long pc __maybe_unused;
+	bool inband = false;
+
+	if (leave_inband) {
+		struct task_struct *tsk = current;
+		/*
+		 * We are about to leave the current inband context
+		 * for switching to an out-of-band task, save the
+		 * preempted context information.
+		 */
+		out->task = tsk;
+		out->active_mm = tsk->active_mm;
+
+		if (IS_ENABLED(CONFIG_KVM))
+			oob_notify_kvm();
+	}
+
+	arch_dovetail_switch_prepare(leave_inband);
+
+	next = in->task;
+	prev = out->task;
+	prev_mm = out->active_mm;
+	next_mm = in->active_mm;
+
+	if (next_mm == NULL) {
+		in->active_mm = prev_mm;
+		in->borrowed_mm = true;
+		enter_lazy_tlb(prev_mm, next);
+	} else {
+		switch_oob_mm(prev_mm, next_mm, next);
+		/*
+		 * We might be switching back to the inband context
+		 * which we preempted earlier, shortly after "current"
+		 * dropped its mm context in the do_exit() path
+		 * (next->mm == NULL). In such a case, a lazy TLB
+		 * state is expected when leaving the mm.
+		 */
+		if (next->mm == NULL)
+			enter_lazy_tlb(prev_mm, next);
+	}
+
+	if (out->borrowed_mm) {
+		out->borrowed_mm = false;
+		out->active_mm = NULL;
+	}
+
+	/*
+	 * Tasks running out-of-band may alter the (in-band)
+	 * preemption count as long as they don't trigger an in-band
+	 * rescheduling, which Dovetail properly blocks.
+	 *
+	 * If the preemption count is not stack-based but a global
+	 * per-cpu variable instead, changing it has a globally
+	 * visible side-effect though, which is a problem if the
+	 * out-of-band task is preempted and schedules away before the
+	 * change is rolled back: this may cause the in-band context
+	 * to later resume with a broken preemption count.
+	 *
+	 * For this reason, the preemption count of any context which
+	 * blocks from the out-of-band stage is carried over and
+	 * restored across switches, emulating a stack-based
+	 * storage.
+	 *
+	 * Eventually, the count is reset to FORK_PREEMPT_COUNT upon
+	 * transition from out-of-band to in-band stage, reinstating
+	 * the value in effect when the converse transition happened
+	 * at some point before.
+	 */
+	if (IS_ENABLED(CONFIG_HAVE_PERCPU_PREEMPT_COUNT))
+		pc = preempt_count();
+
+	switch_to(prev, next, last);
+	barrier();
+
+	if (check_hard_irqs_disabled())
+		hard_irqs_disabled();
+
+	/*
+	 * If we entered this routine for switching to an out-of-band
+	 * task but don't have _TLF_OOB set for the current context
+	 * when resuming, this portion of code is the switch tail of
+	 * the inband schedule() routine, finalizing a transition to
+	 * the inband stage for the current task. Update the stage
+	 * level as/if required.
+	 */
+	if (unlikely(!leave_inband && !test_thread_local_flags(_TLF_OOB))) {
+		if (IS_ENABLED(CONFIG_HAVE_PERCPU_PREEMPT_COUNT))
+			preempt_count_set(FORK_PREEMPT_COUNT);
+		else if (unlikely(dovetail_debug() &&
+					!(preempt_count() & STAGE_MASK)))
+			WARN_ON_ONCE(1);
+		else
+			preempt_count_sub(STAGE_OFFSET);
+		inband = true;
+	} else if (IS_ENABLED(CONFIG_HAVE_PERCPU_PREEMPT_COUNT))
+		preempt_count_set(pc);
+
+	arch_dovetail_switch_finish(leave_inband);
+
+	return inband;
+}
+EXPORT_SYMBOL_GPL(dovetail_context_switch);
+
+#endif /* CONFIG_DOVETAIL */
+
 void dump_cpu_task(int cpu)
 {
 	pr_info("Task dump for CPU %d:\n", cpu);
diff --git a/kernel/sched/cputime.c b/kernel/sched/cputime.c
index 46ed4e1383e2..9905f5192bfa 100644
--- a/kernel/sched/cputime.c
+++ b/kernel/sched/cputime.c
@@ -245,6 +245,10 @@ static __always_inline u64 steal_account_process_time(u64 maxtime)
 
 		return steal;
 	}
+#endif
+#ifdef CONFIG_IRQ_PIPELINE
+	if (irq_pipeline_steal_tick())
+		return maxtime;
 #endif
 	return 0;
 }
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index f65ef1e2f204..ddeaf8e96596 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -77,22 +77,29 @@ void __weak arch_cpu_idle_dead(void) { }
 void __weak arch_cpu_idle(void)
 {
 	cpu_idle_force_poll = 1;
-	local_irq_enable();
+	local_irq_enable_full();
 }
 
 /**
  * default_idle_call - Default CPU idle routine.
  *
  * To use when the cpuidle framework cannot be used.
+ *
+ * When interrupts are pipelined, this call is entered with hard irqs
+ * on and the in-band stage stalled, returns with hard irqs on, and
+ * the in-band stage unstalled.
  */
 void __cpuidle default_idle_call(void)
 {
 	if (current_clr_polling_and_test()) {
-		local_irq_enable();
+		local_irq_enable_full();
 	} else {
-		stop_critical_timings();
-		arch_cpu_idle();
-		start_critical_timings();
+		if (irq_cpuidle_enter(NULL, NULL)) {
+			stop_critical_timings();
+			arch_cpu_idle();
+			start_critical_timings();
+		} else
+			local_irq_enable_full();
 	}
 }
 
@@ -208,6 +215,13 @@ static void cpuidle_idle_call(void)
 exit_idle:
 	__current_set_polling();
 
+	/*
+	 *  Catch mishandling of the CPU's interrupt disable flag when
+	 *  pipelining IRQs.
+	 */
+	if (WARN_ON_ONCE(irq_pipeline_debug() && hard_irqs_disabled()))
+		hard_local_irq_enable();
+
 	/*
 	 * It is up to the idle functions to reenable local interrupts
 	 */
@@ -261,6 +275,7 @@ static void do_idle(void)
 			cpu_idle_poll();
 		} else {
 			cpuidle_idle_call();
+			WARN_ON_ONCE(irq_pipeline_debug() && hard_irqs_disabled());
 		}
 		arch_cpu_idle_exit();
 	}
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c8870c5bd7df..f0b76db02cf8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -52,6 +52,8 @@
 #include <linux/membarrier.h>
 #include <linux/migrate.h>
 #include <linux/mmu_context.h>
+#include <linux/irq_pipeline.h>
+#include <linux/dovetail.h>
 #include <linux/nmi.h>
 #include <linux/proc_fs.h>
 #include <linux/prefetch.h>
diff --git a/kernel/sched/wait.c b/kernel/sched/wait.c
index c1e566a114ca..c0b0b6e17f69 100644
--- a/kernel/sched/wait.c
+++ b/kernel/sched/wait.c
@@ -70,6 +70,8 @@ static int __wake_up_common(struct wait_queue_head *wq_head, unsigned int mode,
 	wait_queue_entry_t *curr, *next;
 	int cnt = 0;
 
+	check_inband_stage();
+
 	lockdep_assert_held(&wq_head->lock);
 
 	if (bookmark && (bookmark->flags & WQ_FLAG_BOOKMARK)) {
diff --git a/kernel/signal.c b/kernel/signal.c
index bcd46f547db3..560efa95e0e0 100644
--- a/kernel/signal.c
+++ b/kernel/signal.c
@@ -754,6 +754,10 @@ static int dequeue_synchronous_signal(kernel_siginfo_t *info)
 void signal_wake_up_state(struct task_struct *t, unsigned int state)
 {
 	set_tsk_thread_flag(t, TIF_SIGPENDING);
+
+	/* TIF_SIGPENDING must be set prior to notifying. */
+	inband_signal_notify(t);
+
 	/*
 	 * TASK_WAKEKILL also means wake it up in the stopped/traced/killable
 	 * case. We don't check t->state here because there is a race with it
@@ -975,8 +979,11 @@ static inline bool wants_signal(int sig, struct task_struct *p)
 	if (sig == SIGKILL)
 		return true;
 
-	if (task_is_stopped_or_traced(p))
+	if (task_is_stopped_or_traced(p)) {
+		if (!signal_pending(p))
+			inband_signal_notify(p);
 		return false;
+	}
 
 	return task_curr(p) || !signal_pending(p);
 }
diff --git a/kernel/softirq.c b/kernel/softirq.c
index 0427a86743a4..8027a6d5c464 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -417,6 +417,18 @@ void irq_exit(void)
 	trace_hardirq_exit(); /* must be last! */
 }
 
+void irq_enter_if_inband(void)
+{
+	if (running_inband())
+		irq_enter();
+}
+
+void irq_exit_if_inband(void)
+{
+	if (running_inband())
+		irq_exit();
+}
+
 /*
  * This function must run with irqs disabled!
  */
diff --git a/kernel/stop_machine.c b/kernel/stop_machine.c
index 998d50ee2d9b..a665d13d67d7 100644
--- a/kernel/stop_machine.c
+++ b/kernel/stop_machine.c
@@ -216,8 +216,8 @@ static int multi_cpu_stop(void *data)
 			curstate = newstate;
 			switch (curstate) {
 			case MULTI_STOP_DISABLE_IRQ:
-				local_irq_disable();
 				hard_irq_disable();
+				local_irq_disable();
 				break;
 			case MULTI_STOP_RUN:
 				if (is_active)
@@ -237,6 +237,7 @@ static int multi_cpu_stop(void *data)
 		}
 	} while (curstate != MULTI_STOP_EXIT);
 
+	hard_irq_enable();
 	local_irq_restore(flags);
 	return err;
 }
@@ -618,6 +619,7 @@ int stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,
 		local_irq_save(flags);
 		hard_irq_disable();
 		ret = (*fn)(data);
+		hard_irq_enable();
 		local_irq_restore(flags);
 
 		return ret;
diff --git a/kernel/time/Makefile b/kernel/time/Makefile
index 1867044800bb..4e336980bd2d 100644
--- a/kernel/time/Makefile
+++ b/kernel/time/Makefile
@@ -19,3 +19,4 @@ obj-$(CONFIG_TICK_ONESHOT)			+= tick-oneshot.o tick-sched.o
 obj-$(CONFIG_HAVE_GENERIC_VDSO)			+= vsyscall.o
 obj-$(CONFIG_DEBUG_FS)				+= timekeeping_debug.o
 obj-$(CONFIG_TEST_UDELAY)			+= test_udelay.o
+obj-$(CONFIG_IRQ_PIPELINE)			+= tick-proxy.o
diff --git a/kernel/time/clockevents.c b/kernel/time/clockevents.c
index f5490222e134..fbe9ef4c6930 100644
--- a/kernel/time/clockevents.c
+++ b/kernel/time/clockevents.c
@@ -97,6 +97,7 @@ static int __clockevents_switch_state(struct clock_event_device *dev,
 	/* Transition with new state-specific callbacks */
 	switch (state) {
 	case CLOCK_EVT_STATE_DETACHED:
+	case CLOCK_EVT_STATE_RESERVED:
 		/* The clockevent device is getting replaced. Shut it down. */
 
 	case CLOCK_EVT_STATE_SHUTDOWN:
@@ -437,6 +438,69 @@ int clockevents_unbind_device(struct clock_event_device *ced, int cpu)
 }
 EXPORT_SYMBOL_GPL(clockevents_unbind_device);
 
+#ifdef CONFIG_IRQ_PIPELINE
+
+/**
+ * clockevents_register_proxy - register a proxy device on the current CPU
+ * @dev:	proxy to register
+ */
+int clockevents_register_proxy(struct clock_proxy_device *dev)
+{
+	struct clock_event_device *proxy_dev, *real_dev;
+	unsigned long flags;
+	u32 freq;
+	int ret;
+
+	raw_spin_lock_irqsave(&clockevents_lock, flags);
+
+	ret = tick_setup_proxy(dev);
+	if (ret)  {
+		raw_spin_unlock_irqrestore(&clockevents_lock, flags);
+		return ret;
+	}
+
+	proxy_dev = &dev->proxy_device;
+	clockevent_set_state(proxy_dev, CLOCK_EVT_STATE_DETACHED);
+
+	list_add(&proxy_dev->list, &clockevent_devices);
+	tick_check_new_device(proxy_dev);
+	clockevents_notify_released();
+
+	raw_spin_unlock_irqrestore(&clockevents_lock, flags);
+
+	real_dev = dev->real_device;
+	freq = (1000000000ULL * real_dev->mult) >> real_dev->shift;
+	printk(KERN_INFO "CPU%d: proxy tick device registered (%u.%02uMHz)\n",
+		 smp_processor_id(), freq / 1000000, (freq / 10000) % 100);
+
+	return ret;
+}
+
+void clockevents_unregister_proxy(struct clock_proxy_device *dev)
+{
+	unsigned long flags;
+	int ret;
+
+	clockevents_register_device(dev->real_device);
+	clockevents_switch_state(dev->real_device, CLOCK_EVT_STATE_DETACHED);
+
+	/*
+	 *  Pop the proxy device, do not give it back to the
+	 *  framework.
+	 */
+	raw_spin_lock_irqsave(&clockevents_lock, flags);
+	ret = clockevents_replace(&dev->proxy_device);
+	raw_spin_unlock_irqrestore(&clockevents_lock, flags);
+
+	if (WARN_ON(ret))
+		return;
+
+	printk(KERN_INFO "CPU%d: proxy tick device unregistered\n",
+		smp_processor_id());
+}
+
+#endif
+
 /**
  * clockevents_register_device - register a clock event device
  * @dev:	device to register
@@ -575,9 +639,13 @@ void clockevents_exchange_device(struct clock_event_device *old,
 	 */
 	if (old) {
 		module_put(old->owner);
-		clockevents_switch_state(old, CLOCK_EVT_STATE_DETACHED);
 		list_del(&old->list);
-		list_add(&old->list, &clockevents_released);
+		if (new && new->features & CLOCK_EVT_FEAT_PROXY) {
+			clockevents_switch_state(old, CLOCK_EVT_STATE_RESERVED);
+		} else {
+			clockevents_switch_state(old, CLOCK_EVT_STATE_DETACHED);
+			list_add(&old->list, &clockevents_released);
+		}
 	}
 
 	if (new) {
diff --git a/kernel/time/clocksource.c b/kernel/time/clocksource.c
index fff5f64981c6..49b55a38fea1 100644
--- a/kernel/time/clocksource.c
+++ b/kernel/time/clocksource.c
@@ -899,8 +899,8 @@ void __clocksource_update_freq_scale(struct clocksource *cs, u32 scale, u32 freq
 
 	clocksource_update_max_deferment(cs);
 
-	pr_info("%s: mask: 0x%llx max_cycles: 0x%llx, max_idle_ns: %lld ns\n",
-		cs->name, cs->mask, cs->max_cycles, cs->max_idle_ns);
+	pr_info("%s: freq: %Lu Hz, mask: 0x%llx max_cycles: 0x%llx, max_idle_ns: %lld ns\n",
+		cs->name, (u64)freq * scale, cs->mask, cs->max_cycles, cs->max_idle_ns);
 }
 EXPORT_SYMBOL_GPL(__clocksource_update_freq_scale);
 
diff --git a/kernel/time/hrtimer.c b/kernel/time/hrtimer.c
index 65605530ee34..bdb96ce57a70 100644
--- a/kernel/time/hrtimer.c
+++ b/kernel/time/hrtimer.c
@@ -872,6 +872,7 @@ void clock_was_set(void)
 	on_each_cpu(retrigger_next_event, NULL, 1);
 #endif
 	timerfd_clock_was_set();
+	inband_clock_was_set();
 }
 
 /*
diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 59225b484e4e..660e77431503 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -243,7 +243,8 @@ static void tick_setup_device(struct tick_device *td,
 	} else {
 		handler = td->evtdev->event_handler;
 		next_event = td->evtdev->next_event;
-		td->evtdev->event_handler = clockevents_handle_noop;
+		if (!clockevent_state_reserved(td->evtdev))
+			td->evtdev->event_handler = clockevents_handle_noop;
 	}
 
 	td->evtdev = newdev;
@@ -318,6 +319,17 @@ static bool tick_check_preferred(struct clock_event_device *curdev,
 	       !cpumask_equal(curdev->cpumask, newdev->cpumask);
 }
 
+static bool tick_check_is_proxy(struct clock_event_device *curdev)
+{
+	if (!irqs_pipelined())
+		return false;
+
+	/*
+	 * Never replace an active proxy except when unregistering it.
+	 */
+	return curdev && curdev->features & CLOCK_EVT_FEAT_PROXY;
+}
+
 /*
  * Check whether the new device is a better fit than curdev. curdev
  * can be NULL !
@@ -325,6 +337,9 @@ static bool tick_check_preferred(struct clock_event_device *curdev,
 bool tick_check_replacement(struct clock_event_device *curdev,
 			    struct clock_event_device *newdev)
 {
+	if (tick_check_is_proxy(curdev))
+		return false;
+
 	if (!tick_check_percpu(curdev, newdev, smp_processor_id()))
 		return false;
 
@@ -345,6 +360,9 @@ void tick_check_new_device(struct clock_event_device *newdev)
 	td = &per_cpu(tick_cpu_device, cpu);
 	curdev = td->evtdev;
 
+	if (tick_check_is_proxy(curdev))
+		goto out_bc;
+
 	/* cpu local device ? */
 	if (!tick_check_percpu(curdev, newdev, cpu))
 		goto out_bc;
diff --git a/kernel/time/tick-internal.h b/kernel/time/tick-internal.h
index 7b2496136729..2c6c0f4ab199 100644
--- a/kernel/time/tick-internal.h
+++ b/kernel/time/tick-internal.h
@@ -51,12 +51,15 @@ static inline void clockevent_set_state(struct clock_event_device *dev,
 extern void clockevents_shutdown(struct clock_event_device *dev);
 extern void clockevents_exchange_device(struct clock_event_device *old,
 					struct clock_event_device *new);
-extern void clockevents_switch_state(struct clock_event_device *dev,
-				     enum clock_event_state state);
 extern int clockevents_program_event(struct clock_event_device *dev,
 				     ktime_t expires, bool force);
 extern void clockevents_handle_noop(struct clock_event_device *dev);
 extern int __clockevents_update_freq(struct clock_event_device *dev, u32 freq);
+#ifdef CONFIG_IRQ_PIPELINE
+int clockevents_register_proxy(struct clock_proxy_device *dev);
+extern void clockevents_unregister_proxy(struct clock_proxy_device *dev);
+int tick_setup_proxy(struct clock_proxy_device *dev);
+#endif
 extern ssize_t sysfs_get_uname(const char *buf, char *dst, size_t cnt);
 
 /* Broadcasting support */
diff --git a/kernel/time/tick-proxy.c b/kernel/time/tick-proxy.c
new file mode 100644
index 000000000000..ee7799a38d98
--- /dev/null
+++ b/kernel/time/tick-proxy.c
@@ -0,0 +1,455 @@
+/*
+ * SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (C) 2017 Philippe Gerum  <rpm@xenomai.org>.
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/printk.h>
+#include <linux/delay.h>
+#include <linux/smp.h>
+#include <linux/err.h>
+#include <linux/cpumask.h>
+#include <linux/clockchips.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/irq_pipeline.h>
+#include <linux/stop_machine.h>
+#include <linux/slab.h>
+#include "tick-internal.h"
+
+static unsigned int proxy_tick_irq;
+
+static DEFINE_MUTEX(proxy_mutex);
+
+static DEFINE_PER_CPU(struct clock_proxy_device, proxy_tick_device);
+
+static inline struct clock_event_device *
+get_real_tick_device(struct clock_event_device *proxy_dev)
+{
+	return container_of(proxy_dev, struct clock_proxy_device, proxy_device)->real_device;
+}
+
+static void proxy_event_handler(struct clock_event_device *real_dev)
+{
+	struct clock_proxy_device *dev = raw_cpu_ptr(&proxy_tick_device);
+	struct clock_event_device *proxy_dev = &dev->proxy_device;
+
+	proxy_dev->event_handler(proxy_dev);
+}
+
+static int proxy_set_state_oneshot(struct clock_event_device *dev)
+{
+	struct clock_event_device *real_dev = get_real_tick_device(dev);
+	unsigned long flags;
+	int ret;
+
+	flags = hard_local_irq_save();
+	ret = real_dev->set_state_oneshot(real_dev);
+	hard_local_irq_restore(flags);
+
+	return ret;
+}
+
+static int proxy_set_state_periodic(struct clock_event_device *dev)
+{
+	struct clock_event_device *real_dev = get_real_tick_device(dev);
+	unsigned long flags;
+	int ret;
+
+	flags = hard_local_irq_save();
+	ret = real_dev->set_state_periodic(real_dev);
+	hard_local_irq_restore(flags);
+
+	return ret;
+}
+
+static int proxy_set_state_oneshot_stopped(struct clock_event_device *dev)
+{
+        struct clock_event_device *real_dev = get_real_tick_device(dev);
+	unsigned long flags;
+	int ret;
+
+	flags = hard_local_irq_save();
+	ret = real_dev->set_state_oneshot_stopped(real_dev);
+	hard_local_irq_restore(flags);
+
+	return ret;
+}
+
+static int proxy_set_state_shutdown(struct clock_event_device *dev)
+{
+        struct clock_event_device *real_dev = get_real_tick_device(dev);
+	unsigned long flags;
+	int ret;
+
+	flags = hard_local_irq_save();
+	ret = real_dev->set_state_shutdown(real_dev);
+	hard_local_irq_restore(flags);
+
+	return ret;
+}
+
+static void proxy_suspend(struct clock_event_device *dev)
+{
+        struct clock_event_device *real_dev = get_real_tick_device(dev);
+	unsigned long flags;
+
+	flags = hard_local_irq_save();
+	real_dev->suspend(real_dev);
+	hard_local_irq_restore(flags);
+}
+
+static void proxy_resume(struct clock_event_device *dev)
+{
+        struct clock_event_device *real_dev = get_real_tick_device(dev);
+	unsigned long flags;
+
+	flags = hard_local_irq_save();
+	real_dev->resume(real_dev);
+	hard_local_irq_restore(flags);
+}
+
+static int proxy_tick_resume(struct clock_event_device *dev)
+{
+        struct clock_event_device *real_dev = get_real_tick_device(dev);
+	unsigned long flags;
+	int ret;
+
+	flags = hard_local_irq_save();
+	ret = real_dev->tick_resume(real_dev);
+	hard_local_irq_restore(flags);
+
+	return ret;
+}
+
+static int proxy_set_next_event(unsigned long delay,
+				struct clock_event_device *dev)
+{
+	struct clock_event_device *real_dev = get_real_tick_device(dev);
+	unsigned long flags;
+	int ret;
+
+	flags = hard_local_irq_save();
+	ret = real_dev->set_next_event(delay, real_dev);
+	hard_local_irq_restore(flags);
+
+	return ret;
+}
+
+static int proxy_set_next_ktime(ktime_t expires,
+				struct clock_event_device *dev)
+{
+	struct clock_event_device *real_dev = get_real_tick_device(dev);
+	unsigned long flags;
+	int ret;
+
+	flags = hard_local_irq_save();
+	ret = real_dev->set_next_ktime(expires, real_dev);
+	hard_local_irq_restore(flags);
+
+	return ret;
+}
+
+static irqreturn_t proxy_irq_handler(int sirq, void *dev_id)
+{
+	struct clock_event_device *evt;
+
+	/*
+	 * Tricky: we may end up running this in-band IRQ handler
+	 * because tick_notify_proxy() was posted either:
+	 *
+	 * - from the out-of-band stage via ->handle_oob_event() for
+	 * emulating an in-band tick.  In this case, the active tick
+	 * device for the in-band timing core is the proxy device,
+	 * whose event handler is still the same than the real tick
+	 * device's.
+	 *
+	 * - directly by the clock chip driver on the local CPU via
+	 * clockevents_handle_event(), for propagating a tick to the
+	 * in-band stage nobody from the out-of-band stage is
+	 * interested on i.e. no proxy device was registered on the
+	 * receiving CPU, which was excluded from @cpumask in the call
+	 * to tick_install_proxy(). In this case, the active tick
+	 * device for the in-band timing core is a real clock event
+	 * device.
+	 *
+	 * In both cases, we are running on the in-band stage, and we
+	 * should fire the event handler of the currently active tick
+	 * device for the in-band timing core.
+	 */
+	evt = raw_cpu_ptr(&tick_cpu_device)->evtdev;
+	evt->event_handler(evt);
+
+	return IRQ_HANDLED;
+}
+
+#define interpose_proxy_handler(__proxy, __real, __h)		\
+	do {							\
+		if ((__real)->__h)				\
+			(__proxy)->__h = proxy_ ## __h;		\
+	} while (0)
+
+/*
+ * Setup a proxy which is about to override the tick device on the
+ * current CPU. Called with clockevents_lock held and irqs off so that
+ * the tick device does not change under our feet.
+ */
+int tick_setup_proxy(struct clock_proxy_device *dev)
+{
+	struct clock_event_device *proxy_dev, *real_dev;
+
+	real_dev = raw_cpu_ptr(&tick_cpu_device)->evtdev;
+	if ((real_dev->features &
+			(CLOCK_EVT_FEAT_PIPELINE|CLOCK_EVT_FEAT_ONESHOT))
+		!= (CLOCK_EVT_FEAT_PIPELINE|CLOCK_EVT_FEAT_ONESHOT)) {
+		WARN(1, "cannot use clockevent device %s in proxy mode!",
+			real_dev->name);
+		return -ENODEV;
+	}
+
+ 	/*
+ 	 * The assumption is that neither us nor clockevents_register_proxy()
+	 * can fail afterwards, so this is ok to advertise the new proxy as
+	 * built by setting dev->real_device early.
+ 	 */
+	dev->real_device = real_dev;
+	dev->__original_handler = real_dev->event_handler;
+
+	/*
+	 * Inherit the feature bits since the proxy device has the
+	 * same capabilities than the real one we are overriding
+	 * (including CLOCK_EVT_FEAT_C3STOP if present).
+	 */
+	proxy_dev = &dev->proxy_device;
+	memset(proxy_dev, 0, sizeof(*proxy_dev));
+	proxy_dev->features = real_dev->features |
+		CLOCK_EVT_FEAT_PERCPU | CLOCK_EVT_FEAT_PROXY;
+	proxy_dev->name = "proxy";
+	proxy_dev->irq = real_dev->irq;
+	proxy_dev->bound_on = -1;
+	proxy_dev->cpumask = cpumask_of(smp_processor_id());
+	proxy_dev->rating = real_dev->rating + 1;
+	proxy_dev->mult = real_dev->mult;
+	proxy_dev->shift = real_dev->shift;
+	proxy_dev->max_delta_ticks = real_dev->max_delta_ticks;
+	proxy_dev->min_delta_ticks = real_dev->min_delta_ticks;
+	proxy_dev->max_delta_ns = real_dev->max_delta_ns;
+	proxy_dev->min_delta_ns = real_dev->min_delta_ns;
+	/*
+	 * Interpose default handlers which are safe wrt preemption by
+	 * the out-of-band stage.
+	 */
+	interpose_proxy_handler(proxy_dev, real_dev, set_state_oneshot);
+	interpose_proxy_handler(proxy_dev, real_dev, set_state_oneshot_stopped);
+	interpose_proxy_handler(proxy_dev, real_dev, set_state_periodic);
+	interpose_proxy_handler(proxy_dev, real_dev, set_state_shutdown);
+	interpose_proxy_handler(proxy_dev, real_dev, suspend);
+	interpose_proxy_handler(proxy_dev, real_dev, resume);
+	interpose_proxy_handler(proxy_dev, real_dev, tick_resume);
+	interpose_proxy_handler(proxy_dev, real_dev, set_next_event);
+	interpose_proxy_handler(proxy_dev, real_dev, set_next_ktime);
+
+	dev->__setup_handler(dev);
+
+	return 0;
+}
+
+static int enable_oob_timer(void *arg) /* hard_irqs_disabled() */
+{
+	struct clock_proxy_device *dev = raw_cpu_ptr(&proxy_tick_device);
+	struct clock_event_device *real_dev;
+
+	/*
+	 * Install the out-of-band handler on this CPU's real clock
+	 * device, then turn on out-of-band mode for the associated
+	 * IRQ (duplicates are silently ignored if the IRQ is common
+	 * to multiple CPUs).
+	 */
+	real_dev = dev->real_device;
+	real_dev->event_handler = dev->handle_oob_event;
+	real_dev->features |= CLOCK_EVT_FEAT_OOB;
+	barrier();
+
+	/*
+	 * irq_switch_oob() grabs the IRQ descriptor lock which is
+	 * mutable, so that is fine to invoke this routine with hard
+	 * IRQs off.
+	 */
+	irq_switch_oob(real_dev->irq, true);
+
+	return 0;
+}
+
+struct proxy_install_arg {
+	void (*setup_proxy)(struct clock_proxy_device *dev);
+	int result;
+};
+
+static void register_proxy_device(void *arg) /* irqs_disabled() */
+{
+	struct clock_proxy_device *dev = raw_cpu_ptr(&proxy_tick_device);
+	struct proxy_install_arg *req = arg;
+	int ret;
+
+	dev->__setup_handler = req->setup_proxy;
+	ret = clockevents_register_proxy(dev);
+	if (ret) {
+		if (!req->result)
+			req->result = ret;
+	} else {
+		dev->real_device->event_handler = proxy_event_handler;
+	}
+}
+
+int tick_install_proxy(void (*setup_proxy)(struct clock_proxy_device *dev),
+		const struct cpumask *cpumask)
+{
+	struct proxy_install_arg arg;
+	int ret, sirq;
+
+	mutex_lock(&proxy_mutex);
+
+	ret = -EAGAIN;
+	if (proxy_tick_irq)
+		goto out;
+
+	sirq = irq_create_direct_mapping(synthetic_irq_domain);
+	if (WARN_ON(sirq == 0))
+		goto out;
+
+	ret = __request_percpu_irq(sirq, proxy_irq_handler,
+				   IRQF_NO_THREAD, /* no IRQF_TIMER here. */
+				   "proxy tick",
+				   &proxy_tick_device);
+	if (WARN_ON(ret)) {
+		irq_dispose_mapping(sirq);
+		goto out;
+	}
+
+	proxy_tick_irq = sirq;
+	barrier();
+
+	/*
+	 * Install a proxy tick device on each CPU. As the proxy
+	 * device is picked, the previous (real) tick device is
+	 * switched to reserved state by the clockevent core.
+	 * Immediately after, the proxy device starts controlling the
+	 * real device under the hood to carry out the timing requests
+	 * it receives.
+	 *
+	 * For a short period of time, after the proxy device is
+	 * installed, and until the real device IRQ is switched to
+	 * out-of-band mode, the flow is as follows:
+	 *
+	 *    [inband timing request]
+	 *        proxy_dev->set_next_event(proxy_dev)
+	 *            oob_program_event(proxy_dev)
+	 *                real_dev->set_next_event(real_dev)
+	 *        ...
+	 *        <tick event>
+	 *        original_timer_handler() [in-band stage]
+	 *            clockevents_handle_event(real_dev)
+	 *               proxy_event_handler(real_dev)
+	 *                  inband_event_handler(proxy_dev)
+	 *
+	 * Eventually, we substitute the original (in-band) clock
+	 * event handler with the out-of-band handler for the real
+	 * clock event device, then turn on out-of-band mode for the
+	 * timer IRQ associated to the latter. These two steps are
+	 * performed over a stop_machine() context, so that no tick
+	 * can race with this code while we swap handlers.
+	 *
+	 * Once the hand over is complete, the flow is as follows:
+	 *
+	 *    [inband timing request]
+	 *        proxy_dev->set_next_event(proxy_dev)
+	 *            oob_program_event(proxy_dev)
+	 *                real_dev->set_next_event(real_dev)
+	 *        ...
+	 *        <tick event>
+	 *        inband_event_handler() [out-of-band stage]
+	 *            clockevents_handle_event(real_dev)
+	 *                handle_oob_event(proxy_dev)
+	 *                    ...(inband tick emulation)...
+	 *                         tick_notify_proxy()
+	 *        ...
+	 *        proxy_irq_handler(proxy_dev) [in-band stage]
+	 *            clockevents_handle_event(proxy_dev)
+	 *                inband_event_handler(proxy_dev)
+	 */
+	arg.setup_proxy = setup_proxy;
+	arg.result = 0;
+	on_each_cpu_mask(cpumask, register_proxy_device, &arg, true);
+	if (arg.result) {
+		tick_uninstall_proxy(cpumask);
+		return arg.result;
+	}
+
+	/*
+	 * Start ticking from the out-of-band interrupt stage upon
+	 * receipt of out-of-band timer events.
+	 */
+	stop_machine(enable_oob_timer, NULL, cpumask);
+out:
+	mutex_unlock(&proxy_mutex);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(tick_install_proxy);
+
+static int disable_oob_timer(void *arg) /* hard_irqs_disabled() */
+{
+	struct clock_proxy_device *dev = raw_cpu_ptr(&proxy_tick_device);
+	struct clock_event_device *real_dev;
+
+	dev = raw_cpu_ptr(&proxy_tick_device);
+	real_dev = dev->real_device;
+	real_dev->event_handler = dev->__original_handler;
+	real_dev->features &= ~CLOCK_EVT_FEAT_OOB;
+	barrier();
+
+	dev->proxy_device.set_next_event = real_dev->set_next_event;
+	irq_switch_oob(real_dev->irq, false);
+
+	return 0;
+}
+
+static void unregister_proxy_device(void *arg) /* irqs_disabled() */
+{
+	struct clock_proxy_device *dev = raw_cpu_ptr(&proxy_tick_device);
+
+	if (dev->real_device) {
+		clockevents_unregister_proxy(dev);
+		dev->real_device = NULL;
+	}
+}
+
+void tick_uninstall_proxy(const struct cpumask *cpumask)
+{
+	/*
+	 * Undo all we did in tick_install_proxy(), handing over
+	 * control of the tick device back to the inband code.
+	 */
+	mutex_lock(&proxy_mutex);
+	stop_machine(disable_oob_timer, NULL, cpu_online_mask);
+	on_each_cpu_mask(cpumask, unregister_proxy_device, NULL, true);
+	free_percpu_irq(proxy_tick_irq, &proxy_tick_device);
+	irq_dispose_mapping(proxy_tick_irq);
+	proxy_tick_irq = 0;
+	mutex_unlock(&proxy_mutex);
+}
+EXPORT_SYMBOL_GPL(tick_uninstall_proxy);
+
+void tick_notify_proxy(void)
+{
+	/*
+	 * Schedule a tick on the proxy device to occur from the
+	 * in-band stage, which will trigger proxy_irq_handler() at
+	 * some point (i.e. when the in-band stage is back in control
+	 * and not stalled). Note that we might be called from the
+	 * in-band stage in some cases (see proxy_irq_handler()).
+	 */
+	irq_post_inband(proxy_tick_irq);
+}
+EXPORT_SYMBOL_GPL(tick_notify_proxy);
diff --git a/kernel/time/timekeeping.c b/kernel/time/timekeeping.c
index ca69290bee2a..c13489d2e890 100644
--- a/kernel/time/timekeeping.c
+++ b/kernel/time/timekeeping.c
@@ -89,6 +89,8 @@ static struct tk_fast tk_fast_raw  ____cacheline_aligned = {
 	.base[1] = { .clock = &dummy_clock, },
 };
 
+static struct timespec64 fast_wall_to_monotonic ____cacheline_aligned;
+
 /* flag for if timekeeping is suspended */
 int __read_mostly timekeeping_suspended;
 
@@ -510,6 +512,16 @@ u64 notrace ktime_get_boot_fast_ns(void)
 }
 EXPORT_SYMBOL_GPL(ktime_get_boot_fast_ns);
 
+ktime_t ktime_get_real_fast(void)
+{
+	ktime_t mono, wtm;
+
+	mono = ns_to_ktime(ktime_get_mono_fast_ns());
+	wtm = timespec64_to_ktime(fast_wall_to_monotonic);
+
+	return ktime_sub(mono, wtm);
+}
+EXPORT_SYMBOL_GPL(ktime_get_real_fast);
 
 /*
  * See comment for __ktime_get_fast_ns() vs. timestamp ordering
@@ -569,6 +581,8 @@ static void halt_fast_timekeeper(const struct timekeeper *tk)
 	memcpy(&tkr_dummy, tkr, sizeof(tkr_dummy));
 	tkr_dummy.clock = &dummy_clock;
 	update_fast_timekeeper(&tkr_dummy, &tk_fast_raw);
+
+	fast_wall_to_monotonic = tk->wall_to_monotonic;
 }
 
 static RAW_NOTIFIER_HEAD(pvclock_gtod_chain);
@@ -674,6 +688,7 @@ static void timekeeping_update(struct timekeeper *tk, unsigned int action)
 	tk->tkr_mono.base_real = tk->tkr_mono.base + tk->offs_real;
 	update_fast_timekeeper(&tk->tkr_mono, &tk_fast_mono);
 	update_fast_timekeeper(&tk->tkr_raw,  &tk_fast_raw);
+	fast_wall_to_monotonic = tk->wall_to_monotonic;
 
 	if (action & TK_CLOCK_WAS_SET)
 		tk->clock_was_set_seq++;
diff --git a/kernel/trace/ftrace.c b/kernel/trace/ftrace.c
index f296d89be757..d539a44821b6 100644
--- a/kernel/trace/ftrace.c
+++ b/kernel/trace/ftrace.c
@@ -5652,10 +5652,10 @@ static int ftrace_process_locs(struct module *mod,
 	 * reason to cause large interrupt latencies while we do it.
 	 */
 	if (!mod)
-		local_irq_save(flags);
+		flags = hard_local_irq_save();
 	ftrace_update_code(mod, start_pg);
 	if (!mod)
-		local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 	ret = 0;
  out:
 	mutex_unlock(&ftrace_lock);
@@ -6193,9 +6193,9 @@ void __init ftrace_init(void)
 	unsigned long count, flags;
 	int ret;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	ret = ftrace_dyn_arch_init();
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 	if (ret)
 		goto failed;
 
@@ -6330,7 +6330,15 @@ __ftrace_ops_list_func(unsigned long ip, unsigned long parent_ip,
 		}
 	} while_for_each_ftrace_op(op);
 out:
-	preempt_enable_notrace();
+	if (irqs_pipelined() && (hard_irqs_disabled() || !running_inband()))
+		/*
+		 * Nothing urgent to schedule here. At latest the
+		 * timer tick will pick up whatever the tracing
+		 * functions kicked off.
+		 */
+		preempt_enable_no_resched_notrace();
+	else
+		preempt_enable_notrace();
 	trace_clear_recursion(bit);
 }
 
diff --git a/kernel/trace/ring_buffer.c b/kernel/trace/ring_buffer.c
index 66358d66c933..3af2d6661a54 100644
--- a/kernel/trace/ring_buffer.c
+++ b/kernel/trace/ring_buffer.c
@@ -2690,8 +2690,8 @@ rb_wakeups(struct ring_buffer *buffer, struct ring_buffer_per_cpu *cpu_buffer)
 static __always_inline int
 trace_recursive_lock(struct ring_buffer_per_cpu *cpu_buffer)
 {
-	unsigned int val = cpu_buffer->current_context;
-	unsigned long pc = preempt_count();
+	unsigned int val;
+	unsigned long pc = preempt_count(), flags;
 	int bit;
 
 	if (!(pc & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET)))
@@ -2700,20 +2700,31 @@ trace_recursive_lock(struct ring_buffer_per_cpu *cpu_buffer)
 		bit = pc & NMI_MASK ? RB_CTX_NMI :
 			pc & HARDIRQ_MASK ? RB_CTX_IRQ : RB_CTX_SOFTIRQ;
 
-	if (unlikely(val & (1 << (bit + cpu_buffer->nest))))
+	flags = hard_local_irq_save();
+
+	val = cpu_buffer->current_context;
+	if (unlikely(val & (1 << (bit + cpu_buffer->nest)))) {
+		hard_local_irq_restore(flags);
 		return 1;
+	}
 
 	val |= (1 << (bit + cpu_buffer->nest));
 	cpu_buffer->current_context = val;
 
+	hard_local_irq_restore(flags);
+
 	return 0;
 }
 
 static __always_inline void
 trace_recursive_unlock(struct ring_buffer_per_cpu *cpu_buffer)
 {
+	unsigned long flags;
+
+	flags = hard_local_irq_save();
 	cpu_buffer->current_context &=
 		cpu_buffer->current_context - (1 << cpu_buffer->nest);
+	hard_local_irq_restore(flags);
 }
 
 /* The recursive locking above uses 4 bits */
diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c
index 6a0ee9178365..affcfd50f83c 100644
--- a/kernel/trace/trace.c
+++ b/kernel/trace/trace.c
@@ -941,9 +941,9 @@ void tracing_snapshot_instance_cond(struct trace_array *tr, void *cond_data)
 		return;
 	}
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	update_max_tr(tr, current, smp_processor_id(), cond_data);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 void tracing_snapshot_instance(struct trace_array *tr)
@@ -1555,7 +1555,7 @@ update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu,
 	if (tr->stop_count)
 		return;
 
-	WARN_ON_ONCE(!irqs_disabled());
+	WARN_ON_ONCE(!hard_irqs_disabled());
 
 	if (!tr->allocated_snapshot) {
 		/* Only the nop tracer should hit this when disabling */
@@ -1599,7 +1599,7 @@ update_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)
 	if (tr->stop_count)
 		return;
 
-	WARN_ON_ONCE(!irqs_disabled());
+	WARN_ON_ONCE(!hard_irqs_disabled());
 	if (!tr->allocated_snapshot) {
 		/* Only the nop tracer should hit this when disabling */
 		WARN_ON_ONCE(tr->current_trace != &nop_trace);
@@ -2340,12 +2340,14 @@ tracing_generic_entry_update(struct trace_entry *entry, unsigned short type,
 	entry->flags =
 #ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT
 		(irqs_disabled_flags(flags) ? TRACE_FLAG_IRQS_OFF : 0) |
-#else
+		(hard_irqs_disabled() ? TRACE_FLAG_IRQS_HARDOFF : 0) |
+#elif !defined(CONFIG_IRQ_PIPELINE)
 		TRACE_FLAG_IRQS_NOSUPPORT |
 #endif
 		((pc & NMI_MASK    ) ? TRACE_FLAG_NMI     : 0) |
 		((pc & HARDIRQ_MASK) ? TRACE_FLAG_HARDIRQ : 0) |
 		((pc & SOFTIRQ_OFFSET) ? TRACE_FLAG_SOFTIRQ : 0) |
+		(running_oob() ? TRACE_FLAG_OOB_STAGE : 0) |
 		(tif_need_resched() ? TRACE_FLAG_NEED_RESCHED : 0) |
 		(test_preempt_need_resched() ? TRACE_FLAG_PREEMPT_RESCHED : 0);
 }
@@ -6764,13 +6766,13 @@ tracing_snapshot_write(struct file *filp, const char __user *ubuf, size_t cnt,
 			ret = tracing_alloc_snapshot_instance(tr);
 		if (ret < 0)
 			break;
-		local_irq_disable();
+		hard_local_irq_disable();
 		/* Now, we're going to swap */
 		if (iter->cpu_file == RING_BUFFER_ALL_CPUS)
 			update_max_tr(tr, current, smp_processor_id(), NULL);
 		else
 			update_max_tr_single(tr, current, iter->cpu_file);
-		local_irq_enable();
+		hard_local_irq_enable();
 		break;
 	default:
 		if (tr->allocated_snapshot) {
diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d685c61085c0..95da79982574 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -121,11 +121,14 @@ struct kretprobe_trace_entry_head {
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:
- *  IRQS_OFF		- interrupts were disabled
+ *  IRQS_OFF		- interrupts were off (only virtually if pipelining)
  *  IRQS_NOSUPPORT	- arch does not support irqs_disabled_flags
  *  NEED_RESCHED	- reschedule is requested
  *  HARDIRQ		- inside an interrupt handler
  *  SOFTIRQ		- inside a softirq handler
+ *  IRQS_HARDOFF	- interrupts were hard disabled
+ *  OOB_STAGE		- running over the oob stage (assume IRQ tracing
+ *                        support is always available w/ pipelining).
  */
 enum trace_flag_type {
 	TRACE_FLAG_IRQS_OFF		= 0x01,
@@ -135,6 +138,8 @@ enum trace_flag_type {
 	TRACE_FLAG_SOFTIRQ		= 0x10,
 	TRACE_FLAG_PREEMPT_RESCHED	= 0x20,
 	TRACE_FLAG_NMI			= 0x40,
+	TRACE_FLAG_IRQS_HARDOFF		= 0x80,
+	TRACE_FLAG_OOB_STAGE		= TRACE_FLAG_IRQS_NOSUPPORT,
 };
 
 #define TRACE_BUF_SIZE		1024
diff --git a/kernel/trace/trace_branch.c b/kernel/trace/trace_branch.c
index 3ea65cdff30d..10d680e3239c 100644
--- a/kernel/trace/trace_branch.c
+++ b/kernel/trace/trace_branch.c
@@ -53,7 +53,7 @@ probe_likely_condition(struct ftrace_likely_data *f, int val, int expect)
 	if (unlikely(!tr))
 		return;
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 	current->trace_recursion |= TRACE_BRANCH_BIT;
 	data = this_cpu_ptr(tr->trace_buffer.data);
 	if (atomic_read(&data->disabled))
@@ -87,7 +87,7 @@ probe_likely_condition(struct ftrace_likely_data *f, int val, int expect)
 
  out:
 	current->trace_recursion &= ~TRACE_BRANCH_BIT;
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static inline
diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index aaf6793ededa..9903dc66e708 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -97,7 +97,7 @@ u64 notrace trace_clock_global(void)
 	int this_cpu;
 	u64 now;
 
-	raw_local_irq_save(flags);
+	flags = hard_local_irq_save();
 
 	this_cpu = raw_smp_processor_id();
 	now = sched_clock_cpu(this_cpu);
@@ -123,7 +123,7 @@ u64 notrace trace_clock_global(void)
 	arch_spin_unlock(&trace_clock_struct.lock);
 
  out:
-	raw_local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 
 	return now;
 }
diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index b611cd36e22d..57c1fc375b3b 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -190,7 +190,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	 * Need to use raw, since this must be called before the
 	 * recursive protection is performed.
 	 */
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	cpu = raw_smp_processor_id();
 	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
 	disabled = atomic_inc_return(&data->disabled);
@@ -202,7 +202,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	}
 
 	atomic_dec(&data->disabled);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 static struct tracer_opt func_opts[] = {
diff --git a/kernel/trace/trace_functions_graph.c b/kernel/trace/trace_functions_graph.c
index 78af97163147..4fea7d44070d 100644
--- a/kernel/trace/trace_functions_graph.c
+++ b/kernel/trace/trace_functions_graph.c
@@ -169,7 +169,7 @@ int trace_graph_entry(struct ftrace_graph_ent *trace)
 	if (tracing_thresh)
 		return 1;
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	cpu = raw_smp_processor_id();
 	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
 	disabled = atomic_inc_return(&data->disabled);
@@ -181,7 +181,7 @@ int trace_graph_entry(struct ftrace_graph_ent *trace)
 	}
 
 	atomic_dec(&data->disabled);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 
 	return ret;
 }
@@ -250,7 +250,7 @@ void trace_graph_return(struct ftrace_graph_ret *trace)
 		return;
 	}
 
-	local_irq_save(flags);
+	flags = hard_local_irq_save();
 	cpu = raw_smp_processor_id();
 	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
 	disabled = atomic_inc_return(&data->disabled);
@@ -259,7 +259,7 @@ void trace_graph_return(struct ftrace_graph_ret *trace)
 		__trace_graph_return(tr, trace, flags, pc);
 	}
 	atomic_dec(&data->disabled);
-	local_irq_restore(flags);
+	hard_local_irq_restore(flags);
 }
 
 void set_graph_array(struct trace_array *tr)
diff --git a/kernel/trace/trace_irqsoff.c b/kernel/trace/trace_irqsoff.c
index a745b0cee5d3..780a529f3984 100644
--- a/kernel/trace/trace_irqsoff.c
+++ b/kernel/trace/trace_irqsoff.c
@@ -26,7 +26,7 @@ static int				tracer_enabled __read_mostly;
 
 static DEFINE_PER_CPU(int, tracing_cpu);
 
-static DEFINE_RAW_SPINLOCK(max_trace_lock);
+static DEFINE_HARD_SPINLOCK(max_trace_lock);
 
 enum {
 	TRACER_IRQS_OFF		= (1 << 1),
@@ -44,7 +44,7 @@ static int start_irqsoff_tracer(struct trace_array *tr, int graph);
 static inline int
 preempt_trace(int pc)
 {
-	return ((trace_type & TRACER_PREEMPT_OFF) && pc);
+	return (running_inband() && (trace_type & TRACER_PREEMPT_OFF) && pc);
 }
 #else
 # define preempt_trace(pc) (0)
@@ -55,7 +55,7 @@ static inline int
 irq_trace(void)
 {
 	return ((trace_type & TRACER_IRQS_OFF) &&
-		irqs_disabled());
+		(hard_irqs_disabled() || (running_inband() && irqs_disabled())));
 }
 #else
 # define irq_trace() (0)
@@ -393,7 +393,7 @@ start_critical_timing(unsigned long ip, unsigned long parent_ip, int pc)
 	data->preempt_timestamp = ftrace_now(cpu);
 	data->critical_start = parent_ip ? : ip;
 
-	local_save_flags(flags);
+	stage_save_flags(flags);
 
 	__trace_function(tr, ip, parent_ip, flags, pc);
 
@@ -428,7 +428,7 @@ stop_critical_timing(unsigned long ip, unsigned long parent_ip, int pc)
 
 	atomic_inc(&data->disabled);
 
-	local_save_flags(flags);
+	stage_save_flags(flags);
 	__trace_function(tr, ip, parent_ip, flags, pc);
 	check_critical_timing(tr, data, parent_ip ? : ip, cpu);
 	data->critical_start = 0;
diff --git a/kernel/trace/trace_output.c b/kernel/trace/trace_output.c
index d54ce252b05a..9230fb4478bc 100644
--- a/kernel/trace/trace_output.c
+++ b/kernel/trace/trace_output.c
@@ -430,14 +430,19 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
 	int hardirq;
 	int softirq;
 	int nmi;
+	int oob;
 
 	nmi = entry->flags & TRACE_FLAG_NMI;
 	hardirq = entry->flags & TRACE_FLAG_HARDIRQ;
 	softirq = entry->flags & TRACE_FLAG_SOFTIRQ;
+	oob = irqs_pipelined() && (entry->flags & TRACE_FLAG_OOB_STAGE);
 
 	irqs_off =
+		(entry->flags & (TRACE_FLAG_IRQS_OFF|TRACE_FLAG_IRQS_HARDOFF)) ==
+		(TRACE_FLAG_IRQS_OFF|TRACE_FLAG_IRQS_HARDOFF) ? '*' :
+		(entry->flags & TRACE_FLAG_IRQS_HARDOFF) ? 'D' :
 		(entry->flags & TRACE_FLAG_IRQS_OFF) ? 'd' :
-		(entry->flags & TRACE_FLAG_IRQS_NOSUPPORT) ? 'X' :
+		!irqs_pipelined() && (entry->flags & TRACE_FLAG_IRQS_NOSUPPORT) ? 'X' :
 		'.';
 
 	switch (entry->flags & (TRACE_FLAG_NEED_RESCHED |
@@ -457,6 +462,8 @@ int trace_print_lat_fmt(struct trace_seq *s, struct trace_entry *entry)
 	}
 
 	hardsoft_irq =
+		(nmi && oob)  ? '#' :
+		oob           ? '~' :
 		(nmi && hardirq)     ? 'Z' :
 		nmi                  ? 'z' :
 		(hardirq && softirq) ? 'H' :
diff --git a/kernel/trace/trace_preemptirq.c b/kernel/trace/trace_preemptirq.c
index 4d8e99fdbbbe..0e6537bc3056 100644
--- a/kernel/trace/trace_preemptirq.c
+++ b/kernel/trace/trace_preemptirq.c
@@ -10,6 +10,7 @@
 #include <linux/module.h>
 #include <linux/ftrace.h>
 #include <linux/kprobes.h>
+#include <linux/irq_pipeline.h>
 #include "trace.h"
 
 #define CREATE_TRACE_POINTS
@@ -74,6 +75,23 @@ __visible void trace_hardirqs_off_caller(unsigned long caller_addr)
 }
 EXPORT_SYMBOL(trace_hardirqs_off_caller);
 NOKPROBE_SYMBOL(trace_hardirqs_off_caller);
+
+void trace_hardirqs_on_pipelined(void)
+{
+	/*
+	 * If the IRQ was not delivered to the kernel, keep the
+	 * tracing logic unaware of the receipt, so that no false
+	 * positive is triggered in lockdep (e.g. IN-HARDIRQ-W ->
+	 * HARDIRQ-ON-W).
+	 */
+	if (irqs_pipelined() && (!running_inband() || irqs_disabled())) {
+		WARN_ON(irq_pipeline_debug() && !hard_irqs_disabled());
+		return;
+	}
+
+	trace_hardirqs_on();
+}
+
 #endif /* CONFIG_TRACE_IRQFLAGS */
 
 #ifdef CONFIG_TRACE_PREEMPT_TOGGLE
diff --git a/kernel/trace/trace_sched_wakeup.c b/kernel/trace/trace_sched_wakeup.c
index 5e43b9664eca..c399d762383a 100644
--- a/kernel/trace/trace_sched_wakeup.c
+++ b/kernel/trace/trace_sched_wakeup.c
@@ -486,7 +486,9 @@ probe_wakeup_sched_switch(void *ignore, bool preempt,
 
 	if (likely(!is_tracing_stopped())) {
 		wakeup_trace->max_latency = delta;
+		hard_local_irq_disable();
 		update_max_tr(wakeup_trace, wakeup_task, wakeup_cpu, NULL);
+		hard_local_irq_enable();
 	}
 
 out_unlock:
diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 4df9a209f7ca..62cbd27f4002 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -171,8 +171,9 @@ static void check_stack(unsigned long ip, unsigned long *stack)
 	if (!object_is_on_stack(stack))
 		return;
 
-	/* Can't do this from NMI context (can cause deadlocks) */
-	if (in_nmi())
+	/* Can't do this from NMI or oob stage contexts (can cause
+	   deadlocks) */
+	if (in_nmi() || !running_inband())
 		return;
 
 	local_irq_save(flags);
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 93d97f9b0157..79e57977a001 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -814,6 +814,38 @@ config DEBUG_SHIRQ
 	  Drivers ought to be able to handle interrupts coming in at those
 	  points; some don't and need to be caught.
 
+config DEBUG_IRQ_PIPELINE
+	bool "Debug IRQ pipeline"
+	depends on IRQ_PIPELINE && DEBUG_KERNEL
+	---help---
+	  Turn on this option for enabling debug checks related to
+	  interrupt pipelining, like interrupt state consistency and
+	  proper context isolation between the in-band and oob stages.
+
+	  If unsure, say N.
+
+config IRQ_PIPELINE_TORTURE_TEST
+	bool "Torture tests for IRQ pipeline"
+	depends on DEBUG_IRQ_PIPELINE && DEBUG_KERNEL
+	select TORTURE_TEST
+	default n
+	help
+	  This option provides a kernel module that runs torture tests
+	  on the IRQ pipeline mechanism.
+
+	  Say Y here if you want the IRQ pipeline torture tests to run
+	  when the kernel starts. Say N if you are unsure.
+
+config DEBUG_DOVETAIL
+	bool "Debug Dovetail interface"
+	depends on DOVETAIL
+	select DEBUG_IRQ_PIPELINE
+	---help---
+	  Turn on this option for enabling debug checks related to
+	  running a dual kernel configuration, aka dovetailing. This
+	  option implicitly enables the interrupt pipeline debugging
+	  features.
+
 menu "Debug Lockups and Hangs"
 
 config LOCKUP_DETECTOR
@@ -1199,6 +1231,27 @@ config DEBUG_LOCK_ALLOC
 	 spin_lock_init()/mutex_init()/etc., or whether there is any lock
 	 held during task exit.
 
+config DEBUG_HARD_LOCKS
+	bool "Debug hard spinlocks"
+	depends on DEBUG_IRQ_PIPELINE && LOCKDEP
+	---help---
+	  Turn on this option for enabling LOCKDEP for hard spinlock
+	  types used in interrupt pipelining.
+
+	  Keep in mind that enabling such feature will ruin the
+	  latency figures for any out-of-band code, this is merely
+	  useful for proving the correctness of the locking scheme of
+	  such code without any consideration for real-time
+	  guarantees. You have been warned.
+
+	  If unsure, say N.
+
+if DEBUG_HARD_LOCKS
+comment "WARNING! DEBUG_HARD_LOCKS induces **massive** latency"
+comment "overhead for the code running on the out-of-band"
+comment "interrupt stage."
+endif
+
 config LOCKDEP
 	bool
 	depends on DEBUG_KERNEL && LOCK_DEBUGGING_SUPPORT
diff --git a/lib/atomic64.c b/lib/atomic64.c
index e98c85a99787..bf7d0409e916 100644
--- a/lib/atomic64.c
+++ b/lib/atomic64.c
@@ -25,15 +25,15 @@
  * Ensure each lock is in a separate cacheline.
  */
 static union {
-	raw_spinlock_t lock;
+	hard_spinlock_t lock;
 	char pad[L1_CACHE_BYTES];
 } atomic64_lock[NR_LOCKS] __cacheline_aligned_in_smp = {
 	[0 ... (NR_LOCKS - 1)] = {
-		.lock =  __RAW_SPIN_LOCK_UNLOCKED(atomic64_lock.lock),
+		.lock =  __HARD_SPIN_LOCK_INITIALIZER(atomic64_lock.lock),
 	},
 };
 
-static inline raw_spinlock_t *lock_addr(const atomic64_t *v)
+static inline hard_spinlock_t *lock_addr(const atomic64_t *v)
 {
 	unsigned long addr = (unsigned long) v;
 
@@ -45,7 +45,7 @@ static inline raw_spinlock_t *lock_addr(const atomic64_t *v)
 s64 atomic64_read(const atomic64_t *v)
 {
 	unsigned long flags;
-	raw_spinlock_t *lock = lock_addr(v);
+	hard_spinlock_t *lock = lock_addr(v);
 	s64 val;
 
 	raw_spin_lock_irqsave(lock, flags);
@@ -58,7 +58,7 @@ EXPORT_SYMBOL(atomic64_read);
 void atomic64_set(atomic64_t *v, s64 i)
 {
 	unsigned long flags;
-	raw_spinlock_t *lock = lock_addr(v);
+	hard_spinlock_t *lock = lock_addr(v);
 
 	raw_spin_lock_irqsave(lock, flags);
 	v->counter = i;
@@ -70,7 +70,7 @@ EXPORT_SYMBOL(atomic64_set);
 void atomic64_##op(s64 a, atomic64_t *v)				\
 {									\
 	unsigned long flags;						\
-	raw_spinlock_t *lock = lock_addr(v);				\
+	hard_spinlock_t *lock = lock_addr(v);				\
 									\
 	raw_spin_lock_irqsave(lock, flags);				\
 	v->counter c_op a;						\
@@ -82,7 +82,7 @@ EXPORT_SYMBOL(atomic64_##op);
 s64 atomic64_##op##_return(s64 a, atomic64_t *v)			\
 {									\
 	unsigned long flags;						\
-	raw_spinlock_t *lock = lock_addr(v);				\
+	hard_spinlock_t *lock = lock_addr(v);				\
 	s64 val;							\
 									\
 	raw_spin_lock_irqsave(lock, flags);				\
@@ -96,7 +96,7 @@ EXPORT_SYMBOL(atomic64_##op##_return);
 s64 atomic64_fetch_##op(s64 a, atomic64_t *v)				\
 {									\
 	unsigned long flags;						\
-	raw_spinlock_t *lock = lock_addr(v);				\
+	hard_spinlock_t *lock = lock_addr(v);				\
 	s64 val;							\
 									\
 	raw_spin_lock_irqsave(lock, flags);				\
@@ -133,7 +133,7 @@ ATOMIC64_OPS(xor, ^=)
 s64 atomic64_dec_if_positive(atomic64_t *v)
 {
 	unsigned long flags;
-	raw_spinlock_t *lock = lock_addr(v);
+	hard_spinlock_t *lock = lock_addr(v);
 	s64 val;
 
 	raw_spin_lock_irqsave(lock, flags);
@@ -148,7 +148,7 @@ EXPORT_SYMBOL(atomic64_dec_if_positive);
 s64 atomic64_cmpxchg(atomic64_t *v, s64 o, s64 n)
 {
 	unsigned long flags;
-	raw_spinlock_t *lock = lock_addr(v);
+	hard_spinlock_t *lock = lock_addr(v);
 	s64 val;
 
 	raw_spin_lock_irqsave(lock, flags);
@@ -163,7 +163,7 @@ EXPORT_SYMBOL(atomic64_cmpxchg);
 s64 atomic64_xchg(atomic64_t *v, s64 new)
 {
 	unsigned long flags;
-	raw_spinlock_t *lock = lock_addr(v);
+	hard_spinlock_t *lock = lock_addr(v);
 	s64 val;
 
 	raw_spin_lock_irqsave(lock, flags);
@@ -177,7 +177,7 @@ EXPORT_SYMBOL(atomic64_xchg);
 s64 atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
 {
 	unsigned long flags;
-	raw_spinlock_t *lock = lock_addr(v);
+	hard_spinlock_t *lock = lock_addr(v);
 	s64 val;
 
 	raw_spin_lock_irqsave(lock, flags);
diff --git a/lib/dump_stack.c b/lib/dump_stack.c
index 33ffbf308853..b9e688851148 100644
--- a/lib/dump_stack.c
+++ b/lib/dump_stack.c
@@ -9,6 +9,7 @@
 #include <linux/sched.h>
 #include <linux/sched/debug.h>
 #include <linux/smp.h>
+#include <linux/irqstage.h>
 #include <linux/atomic.h>
 #include <linux/kexec.h>
 #include <linux/utsname.h>
@@ -56,6 +57,11 @@ void dump_stack_print_info(const char *log_lvl)
 		printk("%sHardware name: %s\n",
 		       log_lvl, dump_stack_arch_desc_str);
 
+#ifdef CONFIG_IRQ_PIPELINE
+	printk("%sIRQ stage: %s\n",
+	       log_lvl, current_irq_stage->name);
+#endif
+
 	print_worker_info(log_lvl, current);
 }
 
@@ -85,6 +91,29 @@ static void __dump_stack(void)
 #ifdef CONFIG_SMP
 static atomic_t dump_lock = ATOMIC_INIT(-1);
 
+static unsigned long disable_local_irqs(void)
+{
+	unsigned long flags = 0; /* only to trick the UMR detection */
+
+	/*
+	 * We neither need nor want to disable in-band IRQs over the
+	 * oob stage, where CPU migration can't happen. Conversely, we
+	 * neither need nor want to disable hard IRQs from the oob
+	 * stage, so that latency won't skyrocket as a result of
+	 * dumping the stack backtrace.
+	 */
+	if (running_inband())
+		local_irq_save(flags);
+
+	return flags;
+}
+
+static void restore_local_irqs(unsigned long flags)
+{
+	if (running_inband())
+		local_irq_restore(flags);
+}
+
 asmlinkage __visible void dump_stack(void)
 {
 	unsigned long flags;
@@ -97,7 +126,7 @@ asmlinkage __visible void dump_stack(void)
 	 * against other CPUs
 	 */
 retry:
-	local_irq_save(flags);
+	flags = disable_local_irqs();
 	cpu = smp_processor_id();
 	old = atomic_cmpxchg(&dump_lock, -1, cpu);
 	if (old == -1) {
@@ -105,7 +134,7 @@ asmlinkage __visible void dump_stack(void)
 	} else if (old == cpu) {
 		was_locked = 1;
 	} else {
-		local_irq_restore(flags);
+		restore_local_irqs(flags);
 		/*
 		 * Wait for the lock to release before jumping to
 		 * atomic_cmpxchg() in order to mitigate the thundering herd
@@ -120,7 +149,7 @@ asmlinkage __visible void dump_stack(void)
 	if (!was_locked)
 		atomic_set(&dump_lock, -1);
 
-	local_irq_restore(flags);
+	restore_local_irqs(flags);
 }
 #else
 asmlinkage __visible void dump_stack(void)
diff --git a/lib/ioremap.c b/lib/ioremap.c
index 0a2ffadc6d71..831540bdf5ea 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -227,6 +227,7 @@ int ioremap_page_range(unsigned long addr,
 			break;
 	} while (pgd++, phys_addr += (next - addr), addr = next, addr != end);
 
+	arch_advertise_page_mapping(start, end);
 	flush_cache_vmap(start, end);
 
 	return err;
diff --git a/lib/smp_processor_id.c b/lib/smp_processor_id.c
index 60ba93fc42ce..c95015aaeaa3 100644
--- a/lib/smp_processor_id.c
+++ b/lib/smp_processor_id.c
@@ -7,12 +7,16 @@
 #include <linux/export.h>
 #include <linux/kprobes.h>
 #include <linux/sched.h>
+#include <linux/irqstage.h>
 
 notrace static nokprobe_inline
 unsigned int check_preemption_disabled(const char *what1, const char *what2)
 {
 	int this_cpu = raw_smp_processor_id();
 
+	if (hard_irqs_disabled() || !running_inband())
+		goto out;
+
 	if (likely(preempt_count()))
 		goto out;
 
diff --git a/mm/kasan/report.c b/mm/kasan/report.c
index 621782100eaa..ce23ba7ad68a 100644
--- a/mm/kasan/report.c
+++ b/mm/kasan/report.c
@@ -75,7 +75,7 @@ static void print_error_description(struct kasan_access_info *info)
 		info->access_addr, current->comm, task_pid_nr(current));
 }
 
-static DEFINE_SPINLOCK(report_lock);
+static DEFINE_HARD_SPINLOCK(report_lock);
 
 static void start_report(unsigned long *flags)
 {
@@ -83,7 +83,7 @@ static void start_report(unsigned long *flags)
 	 * Make sure we don't end up in loop.
 	 */
 	kasan_disable_current();
-	spin_lock_irqsave(&report_lock, *flags);
+	raw_spin_lock_irqsave(&report_lock, *flags);
 	pr_err("==================================================================\n");
 }
 
@@ -91,7 +91,7 @@ static void end_report(unsigned long *flags)
 {
 	pr_err("==================================================================\n");
 	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
-	spin_unlock_irqrestore(&report_lock, *flags);
+	raw_spin_unlock_irqrestore(&report_lock, *flags);
 	if (panic_on_warn)
 		panic("panic_on_warn set ...\n");
 	kasan_enable_current();
diff --git a/mm/memory.c b/mm/memory.c
index b1ca51a079f2..298594747eed 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -4434,6 +4434,15 @@ void print_vma_addr(char *prefix, unsigned long ip)
 #if defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_DEBUG_ATOMIC_SLEEP)
 void __might_fault(const char *file, int line)
 {
+	/*
+	 * When running over the oob stage (e.g. some co-kernel's own
+	 * thread), we should only make sure to run with hw IRQs
+	 * enabled before accessing the memory.
+	 */
+	if (running_oob()) {
+		WARN_ON_ONCE(hard_irqs_disabled());
+		return;
+	}
 	/*
 	 * Some code (nfs/sunrpc) uses socket ops on kernel memory while
 	 * holding the mmap_sem, this is safe because kernel memory doesn't
@@ -4628,6 +4637,66 @@ long copy_huge_page_from_user(struct page *dst_page,
 }
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */
 
+#ifdef CONFIG_DOVETAIL
+
+int commit_vma(struct mm_struct *mm, struct vm_area_struct *vma)
+{
+	unsigned int gup_flags;
+	int ret, npages;
+
+	if (vma->vm_flags & (VM_IO | VM_PFNMAP))
+		return 0;
+
+	if (!((vma->vm_flags & VM_DONTEXPAND) ||
+	    is_vm_hugetlb_page(vma) || vma == get_gate_vma(mm))) {
+		ret = populate_vma_page_range(vma, vma->vm_start, vma->vm_end,
+					      NULL);
+		return ret < 0 ? ret : 0;
+	}
+
+	gup_flags = (vma->vm_flags & (VM_WRITE | VM_SHARED)) == VM_WRITE
+		? FOLL_WRITE : 0;
+	npages = DIV_ROUND_UP(vma->vm_end, PAGE_SIZE) - vma->vm_start/PAGE_SIZE;
+	ret = get_user_pages(vma->vm_start, npages, gup_flags, NULL, NULL);
+	if (ret < 0)
+		return ret;
+
+	return ret == npages ? 0 : -EFAULT;
+}
+
+int force_commit_memory(void)
+{
+	struct task_struct *tsk = current;
+	struct vm_area_struct *vma;
+	struct mm_struct *mm;
+	int ret = 0;
+
+	mm = get_task_mm(tsk);
+	if (!mm)
+		return -EPERM;
+
+	down_write(&mm->mmap_sem);
+	if (test_bit(MMF_VM_PINNED, &mm->flags))
+		goto done_mm;
+
+	for (vma = mm->mmap; vma; vma = vma->vm_next) {
+		if (is_cow_mapping(vma->vm_flags) &&
+		    (vma->vm_flags & VM_WRITE)) {
+			ret = commit_vma(mm, vma);
+			if (ret < 0)
+				goto done_mm;
+		}
+	}
+	set_bit(MMF_VM_PINNED, &mm->flags);
+done_mm:
+	up_write(&mm->mmap_sem);
+	mmput(mm);
+
+	return ret;
+}
+
+#endif
+
 #if USE_SPLIT_PTE_PTLOCKS && ALLOC_SPLIT_PTLOCKS
 
 static struct kmem_cache *page_ptl_cachep;
diff --git a/mm/mmu_context.c b/mm/mmu_context.c
index 3e612ae748e9..1a6fa10052c1 100644
--- a/mm/mmu_context.c
+++ b/mm/mmu_context.c
@@ -23,15 +23,18 @@ void use_mm(struct mm_struct *mm)
 {
 	struct mm_struct *active_mm;
 	struct task_struct *tsk = current;
+	unsigned long flags;
 
 	task_lock(tsk);
 	active_mm = tsk->active_mm;
+	protect_inband_mm(flags);
 	if (active_mm != mm) {
 		mmgrab(mm);
 		tsk->active_mm = mm;
 	}
 	tsk->mm = mm;
 	switch_mm(active_mm, mm, tsk);
+	unprotect_inband_mm(flags);
 	task_unlock(tsk);
 #ifdef finish_arch_post_lock_switch
 	finish_arch_post_lock_switch();
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 7967825f6d33..78e39629436c 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -23,6 +23,7 @@
 #include <linux/swapops.h>
 #include <linux/mmu_notifier.h>
 #include <linux/migrate.h>
+#include <linux/dovetail.h>
 #include <linux/perf_event.h>
 #include <linux/pkeys.h>
 #include <linux/ksm.h>
@@ -41,7 +42,7 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 {
 	pte_t *pte, oldpte;
 	spinlock_t *ptl;
-	unsigned long pages = 0;
+	unsigned long pages = 0, flags;
 	int target_node = NUMA_NO_NODE;
 
 	/*
@@ -109,6 +110,7 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 					continue;
 			}
 
+			flags = hard_local_irq_save();
 			oldpte = ptep_modify_prot_start(vma, addr, pte);
 			ptent = pte_modify(oldpte, newprot);
 			if (preserve_write)
@@ -121,6 +123,7 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				ptent = pte_mkwrite(ptent);
 			}
 			ptep_modify_prot_commit(vma, addr, pte, oldpte, ptent);
+			hard_local_irq_restore(flags);
 			pages++;
 		} else if (IS_ENABLED(CONFIG_MIGRATION)) {
 			swp_entry_t entry = pte_to_swp_entry(oldpte);
@@ -305,6 +308,12 @@ unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
 	else
 		pages = change_protection_range(vma, start, end, newprot, dirty_accountable, prot_numa);
 
+	if (dovetailing() && !prot_numa &&
+	    test_bit(MMF_VM_PINNED, &vma->vm_mm->flags) &&
+	    ((vma->vm_flags | vma->vm_mm->def_flags) & VM_LOCKED) &&
+	    (vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC)))
+		commit_vma(vma->vm_mm, vma);
+
 	return pages;
 }
 
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index a3c70e275f4e..4441275f2bf0 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -211,6 +211,10 @@ static int vmap_p4d_range(pgd_t *pgd, unsigned long addr,
 	return 0;
 }
 
+void __weak arch_advertise_page_mapping(unsigned long start, unsigned long end)
+{
+}
+
 /*
  * Set up page tables in kva (addr, end). The ptes shall have prot "prot", and
  * will have pfns corresponding to the "pages" array.
@@ -235,6 +239,8 @@ static int vmap_page_range_noflush(unsigned long start, unsigned long end,
 			return err;
 	} while (pgd++, addr = next, addr != end);
 
+	arch_advertise_page_mapping(start, end);
+
 	return nr;
 }
 
diff --git a/scripts/mkcompile_h b/scripts/mkcompile_h
index d1d757c6edf4..9103227ae82d 100755
--- a/scripts/mkcompile_h
+++ b/scripts/mkcompile_h
@@ -6,7 +6,8 @@ ARCH=$2
 SMP=$3
 PREEMPT=$4
 PREEMPT_RT=$5
-CC=$6
+IRQPIPE=$6
+CC=$7
 
 vecho() { [ "${quiet}" = "silent_" ] || echo "$@" ; }
 
@@ -55,6 +56,7 @@ CONFIG_FLAGS=""
 if [ -n "$SMP" ] ; then CONFIG_FLAGS="SMP"; fi
 if [ -n "$PREEMPT" ] ; then CONFIG_FLAGS="$CONFIG_FLAGS PREEMPT"; fi
 if [ -n "$PREEMPT_RT" ] ; then CONFIG_FLAGS="$CONFIG_FLAGS PREEMPT_RT"; fi
+if [ -n "$IRQPIPE" ] ; then CONFIG_FLAGS="$CONFIG_FLAGS IRQPIPE"; fi
 UTS_VERSION="$UTS_VERSION $CONFIG_FLAGS $TIMESTAMP"
 
 # Truncate to maximum length
